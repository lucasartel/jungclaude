# Plano de Aprimoramento da Mem√≥ria Sem√¢ntica ChromaDB - JungAgent

**Data de Cria√ß√£o:** 2026-01-02
**Vers√£o:** 1.0
**Status:** Planejamento Aprovado

---

## üìã Sum√°rio Executivo

Este documento detalha o plano completo de aprimoramento da consist√™ncia e efic√°cia da mem√≥ria sem√¢ntica do agente Jung, focando na otimiza√ß√£o do sistema ChromaDB + SQLite h√≠brido.

**Objetivo Central:** Garantir que o agente Jung n√£o perca mem√≥ria contextual como um LLM comum, melhorando a organiza√ß√£o, busca e utiliza√ß√£o da mem√≥ria sem√¢ntica armazenada no ChromaDB.

**Escopo:**
- ‚úÖ SQLite est√° bem resolvido (dados estruturados, fatos, padr√µes)
- üéØ **FOCO:** ChromaDB (mem√≥ria sem√¢ntica, busca vetorial, retrieval)

---

## üîç An√°lise da Arquitetura Atual

### Componentes do Sistema

#### **1. SQLite - Dados Estruturados** ‚úÖ BEM RESOLVIDO
**Localiza√ß√£o:** `jung_core.py:595-899`

**Tabelas Principais:**
- `users` - Cadastro de usu√°rios
- `conversations` - Metadados de conversas (IDs, timestamps, m√©tricas)
- `user_facts_v2` - Fatos extra√≠dos com LLM (hier√°rquico: categoria ‚Üí tipo ‚Üí atributo)
- `user_patterns` - Padr√µes comportamentais detectados
- `archetype_conflicts` - Conflitos arquet√≠picos registrados
- `agent_development` - Evolu√ß√£o do agente por usu√°rio

**Extra√ß√£o de Fatos:**
- Sistema LLM-based com fallback regex (`llm_fact_extractor.py`)
- Categorias: RELACIONAMENTO (vida pessoal) e TRABALHO (vida profissional)
- Confian√ßa por fato (0.0 a 1.0)

#### **2. ChromaDB - Mem√≥ria Sem√¢ntica** ‚ö†Ô∏è REQUER APRIMORAMENTO
**Localiza√ß√£o:** `jung_core.py:480-502, 1140-1216, 1369-1472`

**Armazenamento Atual:**
```python
# Documento completo salvo no ChromaDB
doc_content = """
Usu√°rio: {user_name}
Input: {user_input}
Resposta: {ai_response}

=== VOZES INTERNAS ===
{archetype_analyses}

=== CONFLITOS DETECTADOS ===
{detected_conflicts}
"""

# Metadata atual
metadata = {
    "user_id": user_id,
    "user_name": user_name,
    "session_id": session_id,
    "timestamp": datetime.now().isoformat(),
    "conversation_id": conversation_id,
    "tension_level": tension_level,
    "affective_charge": affective_charge,
    "existential_depth": existential_depth,
    "intensity_level": intensity_level,
    "complexity": complexity,
    "keywords": ",".join(keywords),
    "has_conflicts": bool(detected_conflicts)
}
```

**Busca Sem√¢ntica Atual** (`jung_core.py:1369-1472`):
```python
def semantic_search(user_id, query, k=5, chat_history=None):
    # Enriquece query com √∫ltimas 3 mensagens do chat_history
    enriched_query = query + " " + chat_history_snippet

    # Busca vetorial com filtro de user_id
    results = vectorstore.similarity_search_with_score(
        enriched_query,
        k=k * 2,  # Busca mais para filtrar manualmente
        filter={"user_id": user_id}
    )

    # Filtra manualmente para evitar vazamento entre usu√°rios
    # Converte dist√¢ncia em similaridade
    # Retorna top k mem√≥rias
```

**Constru√ß√£o de Contexto** (`jung_core.py:1505-1618`):
```python
def build_rich_context(user_id, current_input, k_memories=5, chat_history=None):
    # Combina:
    # 1. Hist√≥rico da conversa atual (√∫ltimas 6 mensagens)
    # 2. Fatos estruturados do SQLite
    # 3. Mem√≥rias sem√¢nticas do ChromaDB (k=5)
    # 4. Padr√µes detectados
```

### Problemas Identificados

#### **Problema 1: Busca Vetorial Simples**
- Apenas similaridade cosine, sem pondera√ß√£o por rec√™ncia, emo√ß√£o ou contexto relacional
- k=5 fixo pode ser insuficiente para conversas complexas
- N√£o captura nuances temporais (mem√≥rias antigas vs recentes)

#### **Problema 2: Query Enrichment Limitado**
- Enriquece apenas com √∫ltimas 3 mensagens do usu√°rio
- N√£o utiliza fatos estruturados para enriquecer a busca
- N√£o detecta t√≥picos ou pessoas mencionadas

#### **Problema 3: Sem Consolida√ß√£o de Mem√≥ria**
- Mem√≥rias individuais acumulam sem resumos ou agrupamentos
- Redund√¢ncias n√£o s√£o tratadas
- Sem "mem√≥ria epis√≥dica" de longo prazo

#### **Problema 4: Metadata Pobre**
- Falta estratifica√ß√£o temporal (dia, semana, m√™s)
- N√£o rastreia men√ß√µes a pessoas espec√≠ficas
- N√£o categoriza por t√≥picos (trabalho, fam√≠lia, sa√∫de)

#### **Problema 5: Fatos e Mem√≥rias Desconectados**
- SQLite (fatos estruturados) e ChromaDB (mem√≥ria sem√¢ntica) n√£o conversam
- N√£o h√° cross-referencing entre sistemas

---

## üéØ Plano de Aprimoramento - 6 Fases

---

## **FASE 1: Metadata Enriquecido e Organiza√ß√£o Temporal** üè∑Ô∏è

### Objetivo
Adicionar campos temporais, emocionais e relacionais ao metadata do ChromaDB para permitir filtragem e reranking inteligente.

### Implementa√ß√£o

#### **1.1 Expandir Metadata na Fun√ß√£o `save_conversation()`**
**Arquivo:** `jung_core.py:1100-1264`

**Metadata Atual ‚Üí Metadata Enriquecido:**
```python
# ANTES (linha ~1160)
metadata = {
    "user_id": user_id,
    "user_name": user_name,
    "session_id": session_id or "",
    "timestamp": datetime.now().isoformat(),
    "conversation_id": conversation_id,
    "tension_level": tension_level,
    "affective_charge": affective_charge,
    "existential_depth": existential_depth,
    "intensity_level": intensity_level,
    "complexity": complexity,
    "keywords": ",".join(keywords) if keywords else "",
    "has_conflicts": len(detected_conflicts) > 0 if detected_conflicts else False
}

# DEPOIS (NOVO)
now = datetime.now()
metadata = {
    # Existentes (manter)
    "user_id": user_id,
    "user_name": user_name,
    "session_id": session_id or "",
    "timestamp": now.isoformat(),
    "conversation_id": conversation_id,
    "tension_level": tension_level,
    "affective_charge": affective_charge,
    "existential_depth": existential_depth,
    "intensity_level": intensity_level,
    "complexity": complexity,
    "keywords": ",".join(keywords) if keywords else "",
    "has_conflicts": len(detected_conflicts) > 0 if detected_conflicts else False,

    # NOVOS - Temporal Estratificado
    "day_bucket": now.strftime("%Y-%m-%d"),        # Ex: "2026-01-02"
    "week_bucket": now.strftime("%Y-W%W"),         # Ex: "2026-W01"
    "month_bucket": now.strftime("%Y-%m"),         # Ex: "2026-01"
    "recency_tier": self._calculate_recency_tier(now),  # "recent" | "medium" | "old"

    # NOVOS - Emocional/Tem√°tico
    "emotional_intensity": round(affective_charge + tension_level, 2),  # Score combinado
    "dominant_archetype": self._get_dominant_archetype(archetype_analyses) if archetype_analyses else "",

    # NOVOS - Relacional
    "mentions_people": ",".join(self._extract_people_from_conversation(conversation_id)),
    "topics": ",".join(self._extract_topics_from_keywords(keywords)),
}
```

#### **1.2 Implementar Fun√ß√µes Auxiliares**
**Adicionar em `HybridDatabaseManager` (jung_core.py):**

```python
def _calculate_recency_tier(self, timestamp: datetime) -> str:
    """
    Calcula tier de rec√™ncia da conversa

    Args:
        timestamp: Timestamp da conversa

    Returns:
        "recent" (‚â§30 dias) | "medium" (31-90 dias) | "old" (>90 dias)
    """
    days_ago = (datetime.now() - timestamp).days

    if days_ago <= 30:
        return "recent"
    elif days_ago <= 90:
        return "medium"
    else:
        return "old"

def _get_dominant_archetype(self, archetype_analyses: Dict) -> str:
    """
    Retorna arqu√©tipo com maior intensidade

    Args:
        archetype_analyses: Dict com an√°lises arquet√≠picas

    Returns:
        Nome do arqu√©tipo dominante ou ""
    """
    if not archetype_analyses:
        return ""

    dominant = max(
        archetype_analyses.items(),
        key=lambda x: x[1].intensity if hasattr(x[1], 'intensity') else 0
    )

    return dominant[0] if dominant else ""

def _extract_people_from_conversation(self, conversation_id: int) -> List[str]:
    """
    Extrai nomes de pessoas mencionadas nos fatos desta conversa

    Args:
        conversation_id: ID da conversa

    Returns:
        Lista de nomes pr√≥prios
    """
    cursor = self.conn.cursor()

    # Verificar se user_facts_v2 existe
    cursor.execute("""
        SELECT name FROM sqlite_master
        WHERE type='table' AND name='user_facts_v2'
    """)
    use_v2 = cursor.fetchone() is not None

    if use_v2:
        cursor.execute("""
            SELECT fact_value
            FROM user_facts_v2
            WHERE source_conversation_id = ?
            AND fact_attribute = 'nome'
            AND is_current = 1
        """, (conversation_id,))
    else:
        cursor.execute("""
            SELECT fact_value
            FROM user_facts
            WHERE source_conversation_id = ?
            AND fact_key = 'nome'
            AND is_current = 1
        """, (conversation_id,))

    names = [row[0] for row in cursor.fetchall() if row[0]]
    return names

def _extract_topics_from_keywords(self, keywords: List[str]) -> List[str]:
    """
    Classifica keywords em t√≥picos amplos

    Args:
        keywords: Lista de keywords da conversa

    Returns:
        Lista de t√≥picos detectados
    """
    if not keywords:
        return []

    # Mapeamento de keywords para t√≥picos
    topic_mapping = {
        "trabalho": ["trabalho", "emprego", "empresa", "carreira", "chefe", "colega", "projeto"],
        "familia": ["esposa", "marido", "filho", "filha", "pai", "mae", "familia", "casa"],
        "saude": ["saude", "medico", "doen√ßa", "ansiedade", "depressao", "insonia", "terapia"],
        "relacionamento": ["amigo", "amizade", "namoro", "relacionamento", "amor"],
        "lazer": ["viagem", "hobby", "leitura", "esporte", "musica"],
        "dinheiro": ["dinheiro", "financeiro", "salario", "conta", "divida"],
    }

    topics = set()
    keywords_lower = [k.lower() for k in keywords]

    for topic, topic_keywords in topic_mapping.items():
        if any(kw in " ".join(keywords_lower) for kw in topic_keywords):
            topics.add(topic)

    return list(topics)
```

#### **1.3 Implementar Decay Temporal**
**Adicionar em `HybridDatabaseManager`:**

```python
def calculate_temporal_boost(self, memory_timestamp: str, mode: str = "balanced") -> float:
    """
    Calcula boost temporal para reranking de mem√≥rias

    Args:
        memory_timestamp: Timestamp ISO da mem√≥ria
        mode: Modo de decay ("recent_focused" | "balanced" | "archeological")

    Returns:
        Float multiplicador (0.5 a 1.5)
    """
    try:
        mem_time = datetime.fromisoformat(memory_timestamp)
    except:
        return 1.0  # Fallback se timestamp inv√°lido

    days_ago = (datetime.now() - mem_time).days

    if mode == "recent_focused":
        # Valoriza √∫ltimos 7 dias, penaliza antigas
        if days_ago <= 7:
            return 1.5
        elif days_ago <= 30:
            return 1.2
        elif days_ago <= 90:
            return 1.0
        else:
            return 0.7

    elif mode == "balanced":
        # Equil√≠brio entre recente e hist√≥rico
        if days_ago <= 30:
            return 1.2
        elif days_ago <= 90:
            return 1.0
        else:
            return 0.9

    elif mode == "archeological":
        # Valoriza padr√µes de longo prazo
        if days_ago <= 30:
            return 1.0
        elif days_ago <= 90:
            return 1.1
        else:
            return 1.3  # Boost para mem√≥rias antigas

    return 1.0  # Default
```

### Checklist de Implementa√ß√£o Fase 1

- [ ] Adicionar novos campos ao metadata em `save_conversation()`
- [ ] Implementar `_calculate_recency_tier()`
- [ ] Implementar `_get_dominant_archetype()`
- [ ] Implementar `_extract_people_from_conversation()`
- [ ] Implementar `_extract_topics_from_keywords()`
- [ ] Implementar `calculate_temporal_boost()`
- [ ] Testar com 10 conversas e validar metadata no ChromaDB
- [ ] Documentar novos campos no README

---

## **FASE 2: Query Enrichment Avan√ßado** üîç

### Objetivo
Enriquecer a query de busca sem√¢ntica com contexto estruturado (fatos, pessoas, t√≥picos) para melhorar relev√¢ncia dos resultados.

### Implementa√ß√£o

#### **2.1 Multi-Stage Query Enhancement**
**Modificar `semantic_search()` em jung_core.py:1369-1472:**

**ANTES:**
```python
# Query enriquecida com hist√≥rico recente (se dispon√≠vel)
enriched_query = query

if chat_history and len(chat_history) > 0:
    recent_context = " ".join([
        msg["content"][:100]
        for msg in chat_history[-3:]
        if msg["role"] == "user"
    ])
    enriched_query = f"{recent_context} {query}"
```

**DEPOIS:**
```python
# Query enriquecida multi-stage
enriched_query = self._build_enriched_query(
    user_id=user_id,
    user_input=query,
    chat_history=chat_history
)
```

**Adicionar nova fun√ß√£o:**
```python
def _build_enriched_query(self, user_id: str, user_input: str, chat_history: List[Dict] = None) -> str:
    """
    Constr√≥i query enriquecida com m√∫ltiplas fontes

    Args:
        user_id: ID do usu√°rio
        user_input: Input do usu√°rio
        chat_history: Hist√≥rico da conversa atual

    Returns:
        Query enriquecida
    """
    query_parts = [user_input]  # Base

    # CAMADA 1: Contexto conversacional recente (expandir de 3 para 5)
    if chat_history and len(chat_history) > 0:
        recent = " ".join([
            msg["content"][:100]
            for msg in chat_history[-5:]  # Era -3, agora -5
            if msg["role"] == "user"
        ])
        query_parts.append(recent)

    # CAMADA 2: Fatos relevantes do usu√°rio (NOVO)
    # Buscar nomes de pessoas mencionadas no input
    mentioned_names = self._extract_names_from_text(user_input)

    if mentioned_names:
        # Buscar fatos sobre essas pessoas
        cursor = self.conn.cursor()

        # Usar user_facts_v2 se dispon√≠vel
        cursor.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name='user_facts_v2'
        """)
        use_v2 = cursor.fetchone() is not None

        relevant_facts = []
        for name in mentioned_names:
            if use_v2:
                cursor.execute("""
                    SELECT fact_type, fact_attribute, fact_value
                    FROM user_facts_v2
                    WHERE user_id = ? AND fact_value LIKE ? AND is_current = 1
                    LIMIT 3
                """, (user_id, f"%{name}%"))
            else:
                cursor.execute("""
                    SELECT fact_key, fact_value
                    FROM user_facts
                    WHERE user_id = ? AND fact_value LIKE ? AND is_current = 1
                    LIMIT 3
                """, (user_id, f"%{name}%"))

            relevant_facts.extend([
                f"{row[0]}: {row[1]}" if use_v2 else f"{row[0]}: {row[1]}"
                for row in cursor.fetchall()
            ])

        if relevant_facts:
            query_parts.append(" ".join(relevant_facts[:5]))

    # CAMADA 3: T√≥picos impl√≠citos (NOVO)
    topics = self._detect_topics_in_text(user_input)
    if topics:
        query_parts.append(" ".join(topics))

    return " ".join(query_parts)

def _extract_names_from_text(self, text: str) -> List[str]:
    """
    Extrai nomes pr√≥prios do texto (heur√≠stica simples)

    Args:
        text: Texto para an√°lise

    Returns:
        Lista de poss√≠veis nomes pr√≥prios
    """
    import re

    # Padr√£o: Palavras capitalizadas que n√£o s√£o in√≠cio de frase
    # Ex: "Minha esposa Ana" -> captura "Ana"
    pattern = r'\b([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+)\b'

    # Filtrar palavras comuns que n√£o s√£o nomes
    stopwords = {'O', 'A', 'Os', 'As', 'Um', 'Uma', 'De', 'Da', 'Do', 'Em', 'No', 'Na'}

    matches = re.findall(pattern, text)
    names = [m for m in matches if m not in stopwords]

    return list(set(names))  # Remover duplicatas

def _detect_topics_in_text(self, text: str) -> List[str]:
    """
    Detecta t√≥picos mencionados no texto

    Args:
        text: Texto para an√°lise

    Returns:
        Lista de t√≥picos detectados
    """
    text_lower = text.lower()

    topic_keywords = {
        "trabalho": ["trabalho", "emprego", "empresa", "chefe", "colega", "reuni√£o"],
        "fam√≠lia": ["esposa", "marido", "filho", "filha", "pai", "m√£e", "fam√≠lia"],
        "sa√∫de": ["sa√∫de", "doen√ßa", "m√©dico", "ansiedade", "depress√£o", "terapia"],
        "relacionamento": ["amigo", "namoro", "amor", "relacionamento"],
    }

    detected = []
    for topic, keywords in topic_keywords.items():
        if any(kw in text_lower for kw in keywords):
            detected.append(topic)

    return detected
```

#### **2.2 Hypothetical Document Embeddings (HyDE) - Opcional**
**Para queries muito curtas/amb√≠guas:**

```python
def _generate_hypothetical_response(self, user_input: str, chat_history: List[Dict]) -> str:
    """
    Gera resposta hipot√©tica para melhorar busca sem√¢ntica
    T√©cnica HyDE (Hypothetical Document Embeddings)

    Args:
        user_input: Input curto/amb√≠guo do usu√°rio
        chat_history: Hist√≥rico recente

    Returns:
        Query enriquecida com resposta hipot√©tica
    """
    # S√≥ usar HyDE se input for muito curto
    if len(user_input.split()) >= 5:
        return user_input

    # Formatar hist√≥rico recente
    history_text = "\n".join([
        f"{'Usu√°rio' if msg['role'] == 'user' else 'Jung'}: {msg['content'][:100]}"
        for msg in chat_history[-3:]
    ])

    prompt = f"""Hist√≥rico recente:
{history_text}

Usu√°rio perguntou: "{user_input}"

Gere UMA resposta hipot√©tica breve (2-3 frases) que Jung daria.
Essa resposta ser√° usada para buscar mem√≥rias relevantes."""

    try:
        if self.anthropic_client:
            response = self.anthropic_client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=150,
                messages=[{"role": "user", "content": prompt}]
            )
            hypothetical = response.content[0].text.strip()
        else:
            # Fallback: retornar input original
            return user_input

        # Combinar input + resposta hipot√©tica
        return f"{user_input} {hypothetical}"

    except Exception as e:
        logger.warning(f"Erro ao gerar resposta hipot√©tica: {e}")
        return user_input
```

**Integrar no `semantic_search()` (opcional):**
```python
# Usar HyDE se query for muito curta
if len(query.split()) < 5 and chat_history:
    enriched_query = self._generate_hypothetical_response(query, chat_history)
else:
    enriched_query = self._build_enriched_query(user_id, query, chat_history)
```

### Checklist de Implementa√ß√£o Fase 2

- [ ] Implementar `_build_enriched_query()`
- [ ] Implementar `_extract_names_from_text()`
- [ ] Implementar `_detect_topics_in_text()`
- [ ] Integrar no `semantic_search()`
- [ ] (Opcional) Implementar `_generate_hypothetical_response()` para HyDE
- [ ] Testar com queries curtas ("e ela?") e longas ("como est√° minha esposa Ana?")
- [ ] Validar que nomes e t√≥picos s√£o corretamente detectados

---

## **FASE 3: Busca Multi-Stage e Reranking Inteligente** üéØ

### Objetivo
Implementar busca em dois est√°gios (broad ‚Üí narrow) com reranking inteligente baseado em m√∫ltiplos fatores (temporal, emocional, relacional, tem√°tico).

### Implementa√ß√£o

#### **3.1 Refatorar `semantic_search()` para Two-Stage Retrieval**
**Modificar jung_core.py:1369-1472:**

**ESTRUTURA ATUAL:**
```python
def semantic_search(user_id, query, k=5, chat_history=None):
    # Busca √∫nica com k fixo
    results = vectorstore.similarity_search_with_score(query, k=k*2, filter={"user_id": user_id})
    # Processamento b√°sico
    return memories[:k]
```

**NOVA ESTRUTURA:**
```python
def semantic_search(self, user_id: str, query: str, k: int = None,
                   chat_history: List[Dict] = None) -> List[Dict]:
    """
    Busca sem√¢ntica com two-stage retrieval e reranking inteligente

    STAGE 1: Broad retrieval (k=20)
    STAGE 2: Intelligent reranking com m√∫ltiplos fatores

    Args:
        user_id: ID do usu√°rio
        query: Texto da consulta
        k: N√∫mero de resultados (None = adaptativo)
        chat_history: Hist√≥rico da conversa atual

    Returns:
        Lista de mem√≥rias rerankeadas
    """

    if not self.chroma_enabled:
        logger.warning("ChromaDB desabilitado. Retornando conversas recentes do SQLite.")
        return self._fallback_keyword_search(user_id, query, k or 5)

    try:
        # Calcular k adaptativo se n√£o fornecido
        if k is None:
            k = self._calculate_adaptive_k(query, chat_history, user_id)

        logger.info(f"üîç Busca sem√¢ntica two-stage para user_id='{user_id}' (k={k})")

        # Enriquecer query
        enriched_query = self._build_enriched_query(user_id, query, chat_history)

        # STAGE 1: BROAD RETRIEVAL
        broad_k = max(k * 3, 15)  # Buscar pelo menos 3x mais
        logger.info(f"   STAGE 1: Broad retrieval (k={broad_k})")

        results = self.vectorstore.similarity_search_with_score(
            enriched_query,
            k=broad_k,
            filter={"user_id": str(user_id)}
        )

        logger.info(f"   Resultados retornados: {len(results)}")

        # STAGE 2: INTELLIGENT RERANKING
        logger.info(f"   STAGE 2: Reranking inteligente")
        reranked = self._rerank_memories(
            results=results,
            user_id=user_id,
            query=query,
            chat_history=chat_history
        )

        # Retornar top k
        top_memories = reranked[:k]

        logger.info(f"‚úÖ Top {len(top_memories)} mem√≥rias ap√≥s reranking:")
        for i, mem in enumerate(top_memories[:3], 1):
            logger.info(f"   {i}. [score={mem['final_score']:.3f}] {mem['user_input'][:50]}...")

        return top_memories

    except Exception as e:
        logger.error(f"‚ùå Erro na busca sem√¢ntica: {e}")
        return self._fallback_keyword_search(user_id, query, k or 5)
```

#### **3.2 Implementar Adaptive k**
```python
def _calculate_adaptive_k(self, query: str, chat_history: List[Dict], user_id: str) -> int:
    """
    Calcula k adaptativo baseado em complexidade do contexto

    Args:
        query: Query do usu√°rio
        chat_history: Hist√≥rico da conversa
        user_id: ID do usu√°rio

    Returns:
        k din√¢mico entre 3 e 12
    """
    base_k = 5

    # Fator 1: Comprimento do hist√≥rico
    if chat_history and len(chat_history) > 10:
        base_k += 2  # Conversas longas precisam de mais contexto

    # Fator 2: Complexidade da query
    query_words = len(query.split())
    if query_words > 20:
        base_k += 2
    elif query_words < 5:
        base_k -= 1  # Queries curtas precisam de menos

    # Fator 3: M√∫ltiplas pessoas mencionadas
    mentioned_names = self._extract_names_from_text(query)
    if len(mentioned_names) > 1:
        base_k += len(mentioned_names)

    # Fator 4: Hist√≥rico total do usu√°rio
    total_conversations = self.count_conversations(user_id)
    if total_conversations < 20:
        base_k = min(base_k, 3)  # Limitar para usu√°rios novos

    # Limitar entre 3 e 12
    return max(3, min(base_k, 12))
```

#### **3.3 Implementar Reranking Inteligente**
```python
def _rerank_memories(self, results: List[tuple], user_id: str, query: str,
                    chat_history: List[Dict] = None) -> List[Dict]:
    """
    Reranking inteligente com m√∫ltiplos fatores

    Args:
        results: Lista de (Document, score) do ChromaDB
        user_id: ID do usu√°rio
        query: Query original
        chat_history: Hist√≥rico da conversa

    Returns:
        Lista de mem√≥rias rerankeadas com scores combinados
    """
    reranked = []

    # Extrair informa√ß√µes da query para boosting
    query_names = set(self._extract_names_from_text(query))
    query_topics = set(self._detect_topics_in_text(query))

    for doc, base_score in results:
        metadata = doc.metadata

        # Valida√ß√£o extra: filtrar manualmente user_id errado
        doc_user_id = str(metadata.get('user_id', ''))
        if doc_user_id != str(user_id):
            logger.error(f"üö® Removendo doc com user_id='{doc_user_id}' (esperado='{user_id}')")
            continue

        # === C√ÅLCULO DE BOOSTS ===

        # 1. BOOST TEMPORAL
        temporal_boost = self.calculate_temporal_boost(
            metadata.get('timestamp', ''),
            mode="balanced"
        )

        # 2. BOOST EMOCIONAL
        emotional_intensity = metadata.get('emotional_intensity', 0.0)
        emotional_boost = 1.0
        if emotional_intensity > 1.5:
            emotional_boost = 1.3  # Priorizar momentos emocionalmente intensos
        elif emotional_intensity > 2.5:
            emotional_boost = 1.5  # Muito intenso

        # 3. BOOST DE T√ìPICO
        memory_topics = set(metadata.get('topics', '').split(',')) if metadata.get('topics') else set()
        topic_boost = 1.0
        if query_topics & memory_topics:  # Interse√ß√£o
            topic_boost = 1.2

        # 4. BOOST DE PESSOA MENCIONADA (mais forte)
        memory_people = set(metadata.get('mentions_people', '').split(',')) if metadata.get('mentions_people') else set()
        person_boost = 1.0
        if query_names & memory_people:  # Interse√ß√£o
            person_boost = 1.5  # FORTE boost se mesma pessoa mencionada

        # 5. BOOST DE PROFUNDIDADE EXISTENCIAL
        depth = metadata.get('existential_depth', 0.0)
        depth_boost = 1.0
        if depth > 0.7:
            depth_boost = 1.1  # Leve boost para conversas profundas

        # 6. BOOST DE CONFLITO ARQUET√çPICO
        conflict_boost = 1.0
        if metadata.get('has_conflicts', False):
            conflict_boost = 1.1  # Leve boost para momentos de conflito interno

        # === SCORE FINAL COMBINADO ===
        final_score = (
            base_score *
            temporal_boost *
            emotional_boost *
            topic_boost *
            person_boost *
            depth_boost *
            conflict_boost
        )

        # Extrair conte√∫do do documento
        user_input_match = re.search(r"Input:\s*(.+?)(?:\n|Resposta:|$)", doc.page_content, re.DOTALL)
        user_input_text = user_input_match.group(1).strip() if user_input_match else ""

        response_match = re.search(r"Resposta:\s*(.+?)(?:\n|===|$)", doc.page_content, re.DOTALL)
        response_text = response_match.group(1).strip() if response_match else ""

        reranked.append({
            'conversation_id': metadata.get('conversation_id'),
            'user_input': user_input_text,
            'ai_response': response_text,
            'timestamp': metadata.get('timestamp', ''),
            'base_score': base_score,
            'final_score': final_score,
            'boosts': {
                'temporal': round(temporal_boost, 2),
                'emotional': round(emotional_boost, 2),
                'topic': round(topic_boost, 2),
                'person': round(person_boost, 2),
                'depth': round(depth_boost, 2),
                'conflict': round(conflict_boost, 2),
            },
            'metadata': metadata,
            'full_document': doc.page_content,
        })

    # Ordenar por final_score (decrescente)
    reranked.sort(key=lambda x: x['final_score'], reverse=True)

    # Log dos top 3 com detalhes de boosts
    for i, mem in enumerate(reranked[:3], 1):
        logger.info(f"   Mem√≥ria {i}: base={mem['base_score']:.3f}, final={mem['final_score']:.3f}")
        logger.info(f"      Boosts: {mem['boosts']}")

    return reranked
```

### Checklist de Implementa√ß√£o Fase 3

- [ ] Refatorar `semantic_search()` para two-stage
- [ ] Implementar `_calculate_adaptive_k()`
- [ ] Implementar `_rerank_memories()`
- [ ] Validar que boosts est√£o sendo aplicados corretamente (logs)
- [ ] Testar com queries que mencionam pessoas ("como est√° Ana?")
- [ ] Testar com queries sobre t√≥picos ("problemas no trabalho")
- [ ] Comparar resultados antes/depois do reranking

---

## **FASE 4: Consolida√ß√£o e Mem√≥ria de Longo Prazo** üß†

### Objetivo
Implementar sistema de consolida√ß√£o de mem√≥rias para reduzir redund√¢ncia e criar "mem√≥ria epis√≥dica" de longo prazo.

### Implementa√ß√£o

#### **4.1 Memory Summarization (Background Job)**

**Criar novo arquivo:** `jung_memory_consolidation.py`

```python
"""
jung_memory_consolidation.py - Sistema de Consolida√ß√£o de Mem√≥rias

Respons√°vel por:
- Agrupar mem√≥rias similares por per√≠odo
- Gerar resumos tem√°ticos com LLM
- Criar documentos "consolidated" no ChromaDB
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict
import json

logger = logging.getLogger(__name__)


class MemoryConsolidator:
    """
    Consolida mem√≥rias similares em resumos tem√°ticos
    """

    def __init__(self, db_manager):
        """
        Args:
            db_manager: HybridDatabaseManager instance
        """
        self.db = db_manager

    def consolidate_user_memories(self, user_id: str, lookback_days: int = 90):
        """
        Consolida mem√≥rias de um usu√°rio nos √∫ltimos N dias

        Args:
            user_id: ID do usu√°rio
            lookback_days: Per√≠odo de lookback (default: 90 dias)
        """
        logger.info(f"üì¶ Iniciando consolida√ß√£o de mem√≥rias para user_id={user_id} (lookback={lookback_days} dias)")

        # 1. Buscar todas as mem√≥rias do per√≠odo
        start_date = datetime.now() - timedelta(days=lookback_days)

        cursor = self.db.conn.cursor()
        cursor.execute("""
            SELECT id, user_input, ai_response, timestamp, keywords,
                   tension_level, affective_charge, existential_depth
            FROM conversations
            WHERE user_id = ?
            AND timestamp >= ?
            ORDER BY timestamp ASC
        """, (user_id, start_date.isoformat()))

        memories = [dict(row) for row in cursor.fetchall()]

        if len(memories) < 5:
            logger.info(f"   Menos de 5 mem√≥rias encontradas ({len(memories)}), consolida√ß√£o n√£o necess√°ria")
            return

        logger.info(f"   Encontradas {len(memories)} mem√≥rias para consolidar")

        # 2. Agrupar por t√≥pico usando keywords
        clusters = self._cluster_by_topic(memories)

        logger.info(f"   Identificados {len(clusters)} clusters tem√°ticos")

        # 3. Para cada cluster grande (‚â•5 mem√≥rias), gerar resumo
        for topic, cluster_memories in clusters.items():
            if len(cluster_memories) >= 5:
                logger.info(f"   Consolidando cluster '{topic}' ({len(cluster_memories)} mem√≥rias)")
                self._create_consolidated_memory(
                    user_id=user_id,
                    topic=topic,
                    memories=cluster_memories,
                    lookback_days=lookback_days
                )

    def _cluster_by_topic(self, memories: List[Dict]) -> Dict[str, List[Dict]]:
        """
        Agrupa mem√≥rias por t√≥pico baseado em keywords

        Args:
            memories: Lista de mem√≥rias

        Returns:
            Dict {topic: [mem√≥rias]}
        """
        clusters = {}

        for memory in memories:
            keywords = memory.get('keywords', '').split(',')

            # Detectar t√≥pico principal
            topic = self._identify_main_topic(keywords)

            if topic not in clusters:
                clusters[topic] = []

            clusters[topic].append(memory)

        return clusters

    def _identify_main_topic(self, keywords: List[str]) -> str:
        """
        Identifica t√≥pico principal baseado em keywords

        Args:
            keywords: Lista de keywords

        Returns:
            Nome do t√≥pico
        """
        if not keywords or not keywords[0]:
            return "geral"

        keywords_lower = [k.lower().strip() for k in keywords if k]

        topic_mapping = {
            "trabalho": ["trabalho", "emprego", "empresa", "carreira", "chefe", "colega"],
            "fam√≠lia": ["esposa", "marido", "filho", "filha", "pai", "mae", "familia"],
            "sa√∫de": ["saude", "doen√ßa", "ansiedade", "depressao", "insonia", "terapia"],
            "relacionamento": ["amigo", "namoro", "amor", "relacionamento"],
            "lazer": ["viagem", "hobby", "leitura"],
        }

        for topic, topic_keywords in topic_mapping.items():
            if any(kw in " ".join(keywords_lower) for kw in topic_keywords):
                return topic

        return "geral"

    def _create_consolidated_memory(self, user_id: str, topic: str,
                                    memories: List[Dict], lookback_days: int):
        """
        Cria mem√≥ria consolidada e salva no ChromaDB

        Args:
            user_id: ID do usu√°rio
            topic: T√≥pico do cluster
            memories: Mem√≥rias do cluster
            lookback_days: Per√≠odo de lookback
        """
        # Gerar resumo com LLM
        summary = self._generate_summary_with_llm(topic, memories)

        # IDs das conversas originais
        source_ids = [mem['id'] for mem in memories]

        # Calcular m√©tricas agregadas
        avg_tension = sum(m.get('tension_level', 0) for m in memories) / len(memories)
        avg_affective = sum(m.get('affective_charge', 0) for m in memories) / len(memories)
        avg_depth = sum(m.get('existential_depth', 0) for m in memories) / len(memories)

        # Per√≠odo da consolida√ß√£o
        timestamps = [datetime.fromisoformat(m['timestamp']) for m in memories]
        period_start = min(timestamps).strftime("%Y-%m-%d")
        period_end = max(timestamps).strftime("%Y-%m-%d")

        # Construir documento consolidado
        doc_content = f"""
=== MEM√ìRIA CONSOLIDADA ===
T√ìPICO: {topic.upper()}
PER√çODO: {period_start} a {period_end} ({len(memories)} conversas)

{summary}

M√âTRICAS DO PER√çODO:
- Tens√£o m√©dia: {avg_tension:.2f}
- Carga afetiva m√©dia: {avg_affective:.2f}
- Profundidade m√©dia: {avg_depth:.2f}
"""

        # Metadata
        metadata = {
            "user_id": user_id,
            "type": "consolidated",
            "topic": topic,
            "period_start": period_start,
            "period_end": period_end,
            "count": len(memories),
            "source_ids": json.dumps(source_ids),
            "avg_tension": round(avg_tension, 2),
            "avg_affective": round(avg_affective, 2),
            "avg_depth": round(avg_depth, 2),
            "timestamp": datetime.now().isoformat(),
            "recency_tier": "consolidated",  # Tier especial
        }

        # Salvar no ChromaDB
        chroma_id = f"consolidated_{user_id}_{topic}_{period_end}"

        from langchain.schema import Document
        doc = Document(page_content=doc_content, metadata=metadata)

        try:
            # Tentar adicionar
            self.db.vectorstore.add_documents([doc], ids=[chroma_id])
            logger.info(f"‚úÖ Mem√≥ria consolidada criada: {chroma_id}")
        except Exception as e:
            # Se j√° existe, substituir
            if "already exists" in str(e).lower():
                logger.info(f"   Substituindo mem√≥ria consolidada existente: {chroma_id}")
                self.db.vectorstore.delete([chroma_id])
                self.db.vectorstore.add_documents([doc], ids=[chroma_id])
            else:
                logger.error(f"‚ùå Erro ao criar mem√≥ria consolidada: {e}")

    def _generate_summary_with_llm(self, topic: str, memories: List[Dict]) -> str:
        """
        Gera resumo tem√°tico das mem√≥rias usando LLM

        Args:
            topic: T√≥pico do cluster
            memories: Lista de mem√≥rias

        Returns:
            Resumo gerado
        """
        # Construir prompt com as mem√≥rias
        memories_text = "\n\n".join([
            f"[{mem['timestamp'][:10]}] Usu√°rio: {mem['user_input'][:200]}\nJung: {mem['ai_response'][:200]}"
            for mem in memories[:10]  # Limitar a 10 para n√£o estourar tokens
        ])

        prompt = f"""Voc√™ √© um sistema de consolida√ß√£o de mem√≥rias do Jung.

Analise as {len(memories)} conversas abaixo sobre o tema "{topic}" e gere um RESUMO CONSOLIDADO estruturado:

CONVERSAS:
{memories_text}

Gere um resumo seguindo este formato:

FATOS CONSOLIDADOS:
- [Liste 3-5 fatos principais mencionados repetidamente]

PADR√ïES EMOCIONAIS:
- [Descreva padr√µes emocionais recorrentes, gatilhos, sentimentos]

EVOLU√á√ÉO:
- [Descreva como o tema evoluiu ao longo do per√≠odo, se houve mudan√ßas]

Seja conciso mas informativo. M√°ximo 200 palavras."""

        try:
            if self.db.anthropic_client:
                response = self.db.anthropic_client.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=500,
                    messages=[{"role": "user", "content": prompt}]
                )
                summary = response.content[0].text.strip()
            elif self.db.xai_client:
                response = self.db.xai_client.chat.completions.create(
                    model="grok-beta",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=500
                )
                summary = response.choices[0].message.content.strip()
            else:
                # Fallback: resumo manual b√°sico
                summary = f"Consolida√ß√£o de {len(memories)} conversas sobre {topic}."

            return summary

        except Exception as e:
            logger.error(f"Erro ao gerar resumo com LLM: {e}")
            return f"Consolida√ß√£o de {len(memories)} conversas sobre {topic}."


def run_consolidation_job(db_manager):
    """
    Job para rodar consolida√ß√£o em todos os usu√°rios

    Args:
        db_manager: HybridDatabaseManager instance
    """
    logger.info("üîÑ Iniciando job de consolida√ß√£o de mem√≥rias")

    consolidator = MemoryConsolidator(db_manager)

    # Buscar todos os usu√°rios
    cursor = db_manager.conn.cursor()
    cursor.execute("SELECT DISTINCT user_id FROM conversations")
    user_ids = [row[0] for row in cursor.fetchall()]

    logger.info(f"   Consolidando mem√≥rias para {len(user_ids)} usu√°rios")

    for user_id in user_ids:
        try:
            consolidator.consolidate_user_memories(user_id, lookback_days=90)
        except Exception as e:
            logger.error(f"Erro ao consolidar mem√≥rias de {user_id}: {e}")

    logger.info("‚úÖ Job de consolida√ß√£o conclu√≠do")
```

#### **4.2 Agendar Background Job**

**Adicionar em `telegram_bot.py` ou criar `jobs.py`:**

```python
from apscheduler.schedulers.background import BackgroundScheduler
from jung_memory_consolidation import run_consolidation_job

# Inicializar scheduler
scheduler = BackgroundScheduler()

# Agendar consolida√ß√£o mensal (todo dia 1 √†s 03:00)
scheduler.add_job(
    func=lambda: run_consolidation_job(db_manager),
    trigger='cron',
    day=1,
    hour=3,
    minute=0
)

scheduler.start()
logger.info("‚úÖ Scheduler de consolida√ß√£o iniciado (mensal)")
```

#### **4.3 Fact-Conversation Linking**

**Modificar `save_conversation()` para linkar fatos:**

```python
# Ap√≥s salvar fatos (linha ~1245 em jung_core.py)
# Adicionar linking ao metadata

# Buscar fatos extra√≠dos desta conversa
cursor.execute("""
    SELECT id FROM user_facts_v2
    WHERE source_conversation_id = ? AND is_current = 1
""", (conversation_id,))
fact_ids = [row[0] for row in cursor.fetchall()]

# Adicionar ao metadata antes de salvar no ChromaDB
metadata["extracted_fact_ids"] = ",".join(map(str, fact_ids))
```

### Checklist de Implementa√ß√£o Fase 4

- [ ] Criar `jung_memory_consolidation.py`
- [ ] Implementar `MemoryConsolidator` class
- [ ] Implementar `_cluster_by_topic()`
- [ ] Implementar `_generate_summary_with_llm()`
- [ ] Implementar `_create_consolidated_memory()`
- [ ] Adicionar job mensal no scheduler
- [ ] Testar consolida√ß√£o manual com um usu√°rio
- [ ] Validar que mem√≥rias consolidadas aparecem nas buscas
- [ ] Adicionar fact-conversation linking no metadata

---

## **FASE 5: Context Building Otimizado** üìù

### Objetivo
Melhorar a fun√ß√£o `build_rich_context()` para construir contexto hier√°rquico e estratificado, com compress√£o inteligente.

### Implementa√ß√£o

#### **5.1 Refatorar `build_rich_context()`**
**Modificar jung_core.py:1505-1618:**

```python
def build_rich_context(self, user_id: str, current_input: str,
                      k_memories: int = None,
                      chat_history: List[Dict] = None) -> str:
    """
    Constr√≥i contexto HIER√ÅRQUICO e ESTRATIFICADO

    Combina:
    - Hist√≥rico imediato (sempre incluir)
    - Fatos relevantes ao input (busca inteligente)
    - Mem√≥rias sem√¢nticas (reranked, agrupadas por rec√™ncia)
    - Padr√µes detectados (se relevantes)
    - Mem√≥rias consolidadas (se existirem)

    Args:
        user_id: ID do usu√°rio
        current_input: Input atual do usu√°rio
        k_memories: N√∫mero de mem√≥rias (None = adaptativo)
        chat_history: Hist√≥rico da conversa atual

    Returns:
        Contexto formatado e hier√°rquico
    """
    logger.info(f"üèóÔ∏è Construindo contexto hier√°rquico para user_id={user_id}")

    user = self.get_user(user_id)
    name = user['user_name'] if user else "Usu√°rio"

    context_parts = []

    # ===== LAYER 1: HIST√ìRICO IMEDIATO =====
    context_parts.append("=== CONVERSA ATUAL ===\n")

    if chat_history and len(chat_history) > 0:
        recent = chat_history[-6:] if len(chat_history) > 6 else chat_history

        for msg in recent:
            role = "üë§ Usu√°rio" if msg["role"] == "user" else "ü§ñ Jung"
            content = msg["content"][:150] + "..." if len(msg["content"]) > 150 else msg["content"]
            context_parts.append(f"{role}: {content}")

    context_parts.append("")

    # ===== LAYER 2: FATOS RELEVANTES =====
    relevant_facts = self._search_relevant_facts(user_id, current_input)

    if relevant_facts:
        context_parts.append("=== FATOS RELEVANTES ===\n")
        context_parts.append(self._format_facts_hierarchically(relevant_facts))
        context_parts.append("")

    # ===== LAYER 3: MEM√ìRIAS SEM√ÇNTICAS =====
    memories = self.semantic_search(user_id, current_input, k=k_memories, chat_history=chat_history)

    if memories:
        context_parts.append("=== MEM√ìRIAS RELACIONADAS ===\n")

        # Separar por tipo e rec√™ncia
        consolidated = [m for m in memories if m.get('metadata', {}).get('type') == 'consolidated']
        regular = [m for m in memories if m.get('metadata', {}).get('type') != 'consolidated']

        # Agrupar regulares por rec√™ncia
        recent = [m for m in regular if m.get('metadata', {}).get('recency_tier') == 'recent']
        older = [m for m in regular if m.get('metadata', {}).get('recency_tier') != 'recent']

        # Mem√≥rias consolidadas primeiro (se existirem)
        if consolidated:
            context_parts.append("üì¶ Padr√µes de Longo Prazo (Consolidado):")
            for mem in consolidated[:1]:  # Apenas 1 consolidada
                context_parts.append(f"- {mem['full_document'][:300]}...")
            context_parts.append("")

        # Mem√≥rias recentes
        if recent:
            context_parts.append("üïê Recente (√∫ltimos 30 dias):")
            for i, mem in enumerate(recent[:3], 1):
                timestamp = mem.get('timestamp', '')[:10]
                context_parts.append(f"{i}. [{timestamp}] {mem['user_input'][:100]}...")
            context_parts.append("")

        # Mem√≥rias antigas (se relevantes)
        if older:
            context_parts.append("üìö Hist√≥rico:")
            for i, mem in enumerate(older[:2], 1):
                timestamp = mem.get('timestamp', '')[:10]
                context_parts.append(f"{i}. [{timestamp}] {mem['user_input'][:100]}...")
            context_parts.append("")

    # ===== LAYER 4: PADR√ïES DETECTADOS =====
    patterns = self._get_relevant_patterns(user_id, current_input)

    if patterns:
        context_parts.append("=== PADR√ïES OBSERVADOS ===\n")
        for pattern in patterns[:2]:
            context_parts.append(f"- {pattern['pattern_name']}: {pattern['pattern_description']}")
        context_parts.append("")

    # Juntar tudo
    full_context = "\n".join(context_parts)

    # Comprimir se necess√°rio
    full_context = self._compress_context_if_needed(full_context, max_tokens=2000)

    logger.info(f"‚úÖ Contexto constru√≠do: {len(full_context)} caracteres")

    return full_context

def _search_relevant_facts(self, user_id: str, query: str) -> List[Dict]:
    """
    Busca fatos relevantes ao input atual

    Args:
        user_id: ID do usu√°rio
        query: Input do usu√°rio

    Returns:
        Lista de fatos relevantes
    """
    # Extrair nomes e t√≥picos da query
    mentioned_names = self._extract_names_from_text(query)
    mentioned_topics = self._detect_topics_in_text(query)

    cursor = self.conn.cursor()

    # Verificar estrutura V2
    cursor.execute("""
        SELECT name FROM sqlite_master
        WHERE type='table' AND name='user_facts_v2'
    """)
    use_v2 = cursor.fetchone() is not None

    relevant_facts = []

    # Buscar fatos sobre pessoas mencionadas
    if mentioned_names:
        for name in mentioned_names:
            if use_v2:
                cursor.execute("""
                    SELECT fact_category, fact_type, fact_attribute, fact_value, confidence
                    FROM user_facts_v2
                    WHERE user_id = ? AND fact_value LIKE ? AND is_current = 1
                    LIMIT 5
                """, (user_id, f"%{name}%"))
            else:
                cursor.execute("""
                    SELECT fact_category, fact_key, fact_value
                    FROM user_facts
                    WHERE user_id = ? AND fact_value LIKE ? AND is_current = 1
                    LIMIT 5
                """, (user_id, f"%{name}%"))

            relevant_facts.extend([dict(row) for row in cursor.fetchall()])

    # Buscar fatos sobre t√≥picos mencionados
    if mentioned_topics:
        for topic in mentioned_topics:
            category_map = {
                "trabalho": "TRABALHO",
                "fam√≠lia": "RELACIONAMENTO",
                "sa√∫de": "RELACIONAMENTO",
            }
            category = category_map.get(topic, "RELACIONAMENTO")

            if use_v2:
                cursor.execute("""
                    SELECT fact_category, fact_type, fact_attribute, fact_value, confidence
                    FROM user_facts_v2
                    WHERE user_id = ? AND fact_category = ? AND is_current = 1
                    LIMIT 5
                """, (user_id, category))
            else:
                cursor.execute("""
                    SELECT fact_category, fact_key, fact_value
                    FROM user_facts
                    WHERE user_id = ? AND fact_category = ? AND is_current = 1
                    LIMIT 5
                """, (user_id, category))

            relevant_facts.extend([dict(row) for row in cursor.fetchall()])

    return relevant_facts

def _format_facts_hierarchically(self, facts: List[Dict]) -> str:
    """
    Formata fatos de forma hier√°rquica

    Args:
        facts: Lista de fatos

    Returns:
        String formatada
    """
    if not facts:
        return ""

    # Agrupar por categoria
    by_category = {}
    for fact in facts:
        category = fact.get('fact_category', 'OUTROS')
        if category not in by_category:
            by_category[category] = []
        by_category[category].append(fact)

    lines = []
    for category, cat_facts in by_category.items():
        lines.append(f"{category}:")
        for fact in cat_facts[:5]:  # Limitar a 5 por categoria
            if 'fact_type' in fact:  # V2
                lines.append(f"  - {fact['fact_type']}.{fact['fact_attribute']}: {fact['fact_value']}")
            else:  # V1
                lines.append(f"  - {fact['fact_key']}: {fact['fact_value']}")

    return "\n".join(lines)

def _get_relevant_patterns(self, user_id: str, query: str) -> List[Dict]:
    """
    Busca padr√µes relevantes ao input

    Args:
        user_id: ID do usu√°rio
        query: Input do usu√°rio

    Returns:
        Lista de padr√µes relevantes
    """
    cursor = self.conn.cursor()

    # Buscar padr√µes que ocorrem frequentemente
    cursor.execute("""
        SELECT pattern_type, pattern_name, pattern_description, frequency_count
        FROM user_patterns
        WHERE user_id = ?
        ORDER BY frequency_count DESC, confidence_score DESC
        LIMIT 5
    """, (user_id,))

    patterns = [dict(row) for row in cursor.fetchall()]

    # Filtrar por relev√¢ncia √† query (simples: keywords)
    query_lower = query.lower()
    relevant = [
        p for p in patterns
        if any(word in query_lower for word in p['pattern_name'].lower().split())
    ]

    return relevant if relevant else patterns[:2]

def _compress_context_if_needed(self, context: str, max_tokens: int = 2000) -> str:
    """
    Comprime contexto se exceder limite de tokens

    Args:
        context: Contexto completo
        max_tokens: Limite de tokens

    Returns:
        Contexto original ou comprimido
    """
    # Estimativa: 1 token ‚âà 4 caracteres
    estimated_tokens = len(context) // 4

    if estimated_tokens <= max_tokens:
        return context

    logger.warning(f"‚ö†Ô∏è Contexto muito longo ({estimated_tokens} tokens), comprimindo...")

    # Estrat√©gia de compress√£o: manter apenas essencial
    # 1. Sempre manter hist√≥rico atual (primeiras linhas)
    # 2. Resumir fatos (manter apenas 3 por categoria)
    # 3. Reduzir mem√≥rias (manter apenas 2 recentes + 1 antiga)

    # Por simplicidade, truncar e logar warning
    # (Implementa√ß√£o completa com LLM seria mais sofisticada)
    max_chars = max_tokens * 4
    truncated = context[:max_chars] + "\n\n[... contexto truncado para otimiza√ß√£o ...]"

    logger.warning(f"   Contexto truncado para {max_chars} caracteres")

    return truncated
```

### Checklist de Implementa√ß√£o Fase 5

- [ ] Refatorar `build_rich_context()` para hier√°rquico
- [ ] Implementar `_search_relevant_facts()`
- [ ] Implementar `_format_facts_hierarchically()`
- [ ] Implementar `_get_relevant_patterns()`
- [ ] Implementar `_compress_context_if_needed()`
- [ ] Testar com usu√°rio que tem muitas mem√≥rias (>100)
- [ ] Validar que contexto est√° organizado e leg√≠vel
- [ ] Verificar que mem√≥rias consolidadas aparecem no contexto

---

## **FASE 6: Monitoramento e Feedback Loop** üìä

### Objetivo
Implementar m√©tricas para monitorar qualidade do sistema de mem√≥ria e identificar problemas.

### Implementa√ß√£o

#### **6.1 Memory Quality Metrics**

**Criar novo arquivo:** `jung_memory_metrics.py`

```python
"""
jung_memory_metrics.py - M√©tricas de Qualidade da Mem√≥ria

Monitora:
- Cobertura de mem√≥rias
- Gaps temporais
- Taxas de retrieval
- Qualidade de consolida√ß√£o
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List

logger = logging.getLogger(__name__)


class MemoryQualityMetrics:
    """
    Calcula e monitora m√©tricas de qualidade do sistema de mem√≥ria
    """

    def __init__(self, db_manager):
        """
        Args:
            db_manager: HybridDatabaseManager instance
        """
        self.db = db_manager

    def calculate_coverage(self, user_id: str) -> float:
        """
        Calcula % de conversas que t√™m mem√≥rias recuper√°veis no ChromaDB

        Args:
            user_id: ID do usu√°rio

        Returns:
            Float entre 0 e 1
        """
        cursor = self.db.conn.cursor()

        # Total de conversas no SQLite
        cursor.execute("""
            SELECT COUNT(*) as total
            FROM conversations
            WHERE user_id = ?
        """, (user_id,))
        total = cursor.fetchone()['total']

        if total == 0:
            return 0.0

        # Conversas com chroma_id (salvas no ChromaDB)
        cursor.execute("""
            SELECT COUNT(*) as with_chroma
            FROM conversations
            WHERE user_id = ? AND chroma_id IS NOT NULL
        """, (user_id,))
        with_chroma = cursor.fetchone()['with_chroma']

        coverage = with_chroma / total

        logger.info(f"üìä Cobertura ChromaDB para {user_id}: {coverage:.1%} ({with_chroma}/{total})")

        return coverage

    def detect_memory_gaps(self, user_id: str, gap_threshold_days: int = 7) -> List[Dict]:
        """
        Identifica per√≠odos sem mem√≥rias (gaps)

        Args:
            user_id: ID do usu√°rio
            gap_threshold_days: M√≠nimo de dias para considerar gap

        Returns:
            Lista de gaps detectados
        """
        cursor = self.db.conn.cursor()

        cursor.execute("""
            SELECT timestamp
            FROM conversations
            WHERE user_id = ?
            ORDER BY timestamp ASC
        """, (user_id,))

        timestamps = [datetime.fromisoformat(row['timestamp']) for row in cursor.fetchall()]

        if len(timestamps) < 2:
            return []

        gaps = []
        for i in range(len(timestamps) - 1):
            time_diff = timestamps[i + 1] - timestamps[i]

            if time_diff.days >= gap_threshold_days:
                gaps.append({
                    'start': timestamps[i].strftime("%Y-%m-%d"),
                    'end': timestamps[i + 1].strftime("%Y-%m-%d"),
                    'duration_days': time_diff.days
                })

        if gaps:
            logger.warning(f"‚ö†Ô∏è {len(gaps)} gaps de mem√≥ria detectados para {user_id}")
            for gap in gaps:
                logger.warning(f"   Gap: {gap['start']} ‚Üí {gap['end']} ({gap['duration_days']} dias)")

        return gaps

    def calculate_retrieval_stats(self, user_id: str, last_n_conversations: int = 20) -> Dict:
        """
        Calcula estat√≠sticas de retrieval (quantas mem√≥rias recuperadas por busca)

        Args:
            user_id: ID do usu√°rio
            last_n_conversations: √öltimas N conversas para an√°lise

        Returns:
            Dict com estat√≠sticas
        """
        # Esta m√©trica requer tracking de quantas mem√≥rias foram recuperadas
        # Por simplicidade, retornar estrutura vazia (implementar tracking posterior)

        return {
            "avg_memories_retrieved": 0,
            "min_memories": 0,
            "max_memories": 0,
        }

    def generate_user_report(self, user_id: str) -> str:
        """
        Gera relat√≥rio completo de m√©tricas para um usu√°rio

        Args:
            user_id: ID do usu√°rio

        Returns:
            Relat√≥rio formatado
        """
        logger.info(f"üìä Gerando relat√≥rio de m√©tricas para {user_id}")

        # M√©tricas
        coverage = self.calculate_coverage(user_id)
        gaps = self.detect_memory_gaps(user_id)

        # Total de conversas
        cursor = self.db.conn.cursor()
        cursor.execute("""
            SELECT COUNT(*) as total,
                   MIN(timestamp) as first,
                   MAX(timestamp) as last
            FROM conversations
            WHERE user_id = ?
        """, (user_id,))
        stats = dict(cursor.fetchone())

        # Consolida√ß√µes existentes
        cursor.execute("""
            SELECT COUNT(*) as consolidated
            FROM conversations
            WHERE user_id = ? AND chroma_id LIKE 'consolidated_%'
        """, (user_id,))
        consolidated_count = cursor.fetchone()['consolidated']

        # Montar relat√≥rio
        report = f"""
========================================
RELAT√ìRIO DE M√âTRICAS DE MEM√ìRIA
========================================
Usu√°rio: {user_id}

ESTAT√çSTICAS GERAIS:
- Total de conversas: {stats['total']}
- Primeira conversa: {stats['first'][:10] if stats['first'] else 'N/A'}
- √öltima conversa: {stats['last'][:10] if stats['last'] else 'N/A'}

COBERTURA CHROMADB:
- Cobertura: {coverage:.1%}
- Mem√≥rias consolidadas: {consolidated_count}

GAPS TEMPORAIS:
- Gaps detectados (‚â•7 dias): {len(gaps)}
"""

        if gaps:
            report += "\nDETALHES DOS GAPS:\n"
            for i, gap in enumerate(gaps[:5], 1):
                report += f"{i}. {gap['start']} ‚Üí {gap['end']} ({gap['duration_days']} dias)\n"

        report += "\n========================================\n"

        return report


def generate_system_metrics(db_manager) -> str:
    """
    Gera m√©tricas globais do sistema

    Args:
        db_manager: HybridDatabaseManager instance

    Returns:
        Relat√≥rio do sistema
    """
    logger.info("üìä Gerando m√©tricas globais do sistema")

    cursor = db_manager.conn.cursor()

    # Total de usu√°rios
    cursor.execute("SELECT COUNT(DISTINCT user_id) as total FROM conversations")
    total_users = cursor.fetchone()['total']

    # Total de conversas
    cursor.execute("SELECT COUNT(*) as total FROM conversations")
    total_conversations = cursor.fetchone()['total']

    # Conversas no ChromaDB
    cursor.execute("SELECT COUNT(*) as total FROM conversations WHERE chroma_id IS NOT NULL")
    chroma_conversations = cursor.fetchone()['total']

    # Mem√≥rias consolidadas
    cursor.execute("SELECT COUNT(*) as total FROM conversations WHERE chroma_id LIKE 'consolidated_%'")
    consolidated = cursor.fetchone()['total']

    # Cobertura m√©dia
    avg_coverage = chroma_conversations / total_conversations if total_conversations > 0 else 0

    report = f"""
========================================
M√âTRICAS GLOBAIS DO SISTEMA
========================================
Data: {datetime.now().strftime("%Y-%m-%d %H:%M")}

ESTAT√çSTICAS:
- Total de usu√°rios: {total_users}
- Total de conversas: {total_conversations}
- Conversas no ChromaDB: {chroma_conversations}
- Mem√≥rias consolidadas: {consolidated}

COBERTURA:
- Cobertura m√©dia: {avg_coverage:.1%}

========================================
"""

    return report
```

#### **6.2 Endpoint de Diagn√≥stico**

**Adicionar em `admin_web/routes.py` ou arquivo equivalente:**

```python
@router.get("/admin/memory-metrics/{user_id}")
async def memory_metrics(user_id: str, admin: Dict = Depends(require_auth)):
    """
    Retorna m√©tricas de mem√≥ria para um usu√°rio
    """
    from jung_memory_metrics import MemoryQualityMetrics

    metrics = MemoryQualityMetrics(db_manager)
    report = metrics.generate_user_report(user_id)

    return {"report": report}

@router.get("/admin/system-metrics")
async def system_metrics(admin: Dict = Depends(require_master)):
    """
    Retorna m√©tricas globais do sistema (apenas Master)
    """
    from jung_memory_metrics import generate_system_metrics

    report = generate_system_metrics(db_manager)

    return {"report": report}
```

### Checklist de Implementa√ß√£o Fase 6

- [ ] Criar `jung_memory_metrics.py`
- [ ] Implementar `MemoryQualityMetrics` class
- [ ] Implementar `calculate_coverage()`
- [ ] Implementar `detect_memory_gaps()`
- [ ] Implementar `generate_user_report()`
- [ ] Implementar `generate_system_metrics()`
- [ ] Adicionar endpoints de diagn√≥stico no admin
- [ ] Testar com 3-5 usu√°rios diferentes
- [ ] Criar dashboard visual (opcional, futura itera√ß√£o)

---

## üìÖ Roadmap de Implementa√ß√£o Sugerido

### **Sprint 1 (Semana 1-2): Fase 1 - Metadata Enriquecido**
**Objetivo:** Adicionar campos temporais, emocionais e tem√°ticos ao metadata

**Tarefas:**
1. Modificar `save_conversation()` para adicionar novos campos ao metadata
2. Implementar fun√ß√µes auxiliares (`_calculate_recency_tier`, `_get_dominant_archetype`, etc.)
3. Implementar `calculate_temporal_boost()`
4. Testar com 10 conversas e validar metadata no ChromaDB
5. Documentar novos campos

**Entreg√°vel:** Metadata enriquecido funcionando em produ√ß√£o

---

### **Sprint 2 (Semana 3): Fase 2 - Query Enrichment**
**Objetivo:** Enriquecer queries com contexto estruturado

**Tarefas:**
1. Implementar `_build_enriched_query()`
2. Implementar `_extract_names_from_text()`
3. Implementar `_detect_topics_in_text()`
4. Integrar no `semantic_search()`
5. (Opcional) Implementar HyDE para queries curtas
6. Testar com queries diversas

**Entreg√°vel:** Query enrichment funcionando, melhorando relev√¢ncia

---

### **Sprint 3 (Semana 4-5): Fase 3 - Busca Multi-Stage**
**Objetivo:** Two-stage retrieval com reranking inteligente

**Tarefas:**
1. Refatorar `semantic_search()` para two-stage
2. Implementar `_calculate_adaptive_k()`
3. Implementar `_rerank_memories()` com 6 boosts
4. Testar e comparar resultados antes/depois
5. Ajustar pesos dos boosts baseado em testes

**Entreg√°vel:** Sistema de busca two-stage + reranking em produ√ß√£o

---

### **Sprint 4 (Semana 6): Fase 5 - Context Building**
**Objetivo:** Contexto hier√°rquico e estratificado

**Tarefas:**
1. Refatorar `build_rich_context()` para hier√°rquico
2. Implementar `_search_relevant_facts()`
3. Implementar `_format_facts_hierarchically()`
4. Implementar `_get_relevant_patterns()`
5. Implementar `_compress_context_if_needed()`
6. Testar com usu√°rios que t√™m muitas mem√≥rias

**Entreg√°vel:** Contexto otimizado e organizado

---

### **Sprint 5 (Semana 7-8): Fase 4 - Consolida√ß√£o**
**Objetivo:** Background job de consolida√ß√£o

**Tarefas:**
1. Criar `jung_memory_consolidation.py`
2. Implementar `MemoryConsolidator` class
3. Implementar clustering e summarization
4. Adicionar job mensal no scheduler
5. Testar consolida√ß√£o manual
6. Adicionar fact-conversation linking

**Entreg√°vel:** Sistema de consolida√ß√£o rodando mensalmente

---

### **Sprint 6 (Semana 9): Fase 6 - M√©tricas**
**Objetivo:** Dashboard de monitoramento

**Tarefas:**
1. Criar `jung_memory_metrics.py`
2. Implementar m√©tricas de cobertura, gaps, etc.
3. Adicionar endpoints de diagn√≥stico no admin
4. Testar com m√∫ltiplos usu√°rios
5. (Opcional) Criar dashboard visual

**Entreg√°vel:** Sistema de m√©tricas e diagn√≥stico funcionando

---

## üéØ Benef√≠cios Esperados

### **1. Consist√™ncia de Mem√≥ria**
- ‚úÖ Agente n√£o perde contexto mesmo ap√≥s semanas/meses sem intera√ß√£o
- ‚úÖ Mem√≥rias relevantes s√£o sempre recuperadas, independente de quando ocorreram
- ‚úÖ Consolida√ß√£o reduz redund√¢ncia e cria "mem√≥ria epis√≥dica"

### **2. Relev√¢ncia Aumentada**
- ‚úÖ Retrieval captura n√£o apenas similaridade vetorial, mas:
  - Contexto temporal (recente vs hist√≥rico)
  - Intensidade emocional
  - Men√ß√µes a pessoas espec√≠ficas
  - T√≥picos relevantes
- ‚úÖ k adaptativo evita sobrecarga ou falta de contexto

### **3. Escalabilidade**
- ‚úÖ Sistema funciona eficientemente com 10 ou 10.000 conversas
- ‚úÖ Consolida√ß√£o previne crescimento exponencial de mem√≥rias redundantes

### **4. Experi√™ncia do Usu√°rio**
- ‚úÖ Sensa√ß√£o de "Jung realmente me conhece e lembra de tudo"
- ‚úÖ Respostas mais contextualizadas e personalizadas
- ‚úÖ Continuidade em conversas mesmo ap√≥s longos per√≠odos

### **5. Observabilidade**
- ‚úÖ M√©tricas permitem identificar problemas de mem√≥ria
- ‚úÖ Gaps temporais s√£o detectados e podem ser investigados
- ‚úÖ Cobertura de ChromaDB √© monitorada

---

## üìö Refer√™ncias T√©cnicas

### Arquivos Principais a Modificar

1. **jung_core.py**
   - Linhas 1100-1264: `save_conversation()` - Adicionar metadata enriquecido
   - Linhas 1369-1472: `semantic_search()` - Two-stage retrieval
   - Linhas 1505-1618: `build_rich_context()` - Contexto hier√°rquico

2. **Novos Arquivos a Criar**
   - `jung_memory_consolidation.py` - Sistema de consolida√ß√£o
   - `jung_memory_metrics.py` - M√©tricas de qualidade

3. **Arquivos de Configura√ß√£o**
   - `telegram_bot.py` ou `jobs.py` - Scheduler de consolida√ß√£o

### Depend√™ncias

**Existentes:**
- ChromaDB + LangChain
- OpenAI Embeddings (`text-embedding-3-small`)
- SQLite
- Anthropic Claude API

**Novas (instalar se necess√°rio):**
- `apscheduler` - Para background jobs de consolida√ß√£o

### Comandos de Instala√ß√£o

```bash
pip install apscheduler
```

---

## ‚ö†Ô∏è Considera√ß√µes de Implementa√ß√£o

### **1. Backward Compatibility**
- Metadata antigo (sem novos campos) deve continuar funcionando
- Adicionar verifica√ß√µes de exist√™ncia antes de acessar novos campos

### **2. Performance**
- Two-stage retrieval aumenta carga: monitorar tempos de resposta
- Consolida√ß√£o deve rodar em hor√°rios de baixo uso (03:00)
- Compression de contexto s√≥ quando necess√°rio

### **3. Testes**
- Testar com usu√°rios reais (variados: novos, antigos, ativos, inativos)
- Comparar qualidade de respostas antes/depois de cada fase
- Validar que n√£o h√° vazamento de mem√≥rias entre usu√°rios

### **4. Rollback Plan**
- Manter c√≥digo antigo comentado durante transi√ß√£o
- Criar flag de feature toggle para desabilitar novos recursos se necess√°rio
- Backup do banco ChromaDB antes de grandes mudan√ßas

---

## ‚úÖ Crit√©rios de Sucesso

### **Fase 1 (Metadata)**
- [ ] Metadata enriquecido salvo em 100% das novas conversas
- [ ] Campos temporais corretamente populados
- [ ] Nenhum erro ao buscar mem√≥rias antigas (sem novos campos)

### **Fase 2 (Query Enrichment)**
- [ ] Queries enriquecidas incluem nomes e t√≥picos detectados
- [ ] Relev√¢ncia de resultados melhora (valida√ß√£o manual com 10 queries)

### **Fase 3 (Two-Stage)**
- [ ] Reranking altera ordem de resultados em ‚â•50% das buscas
- [ ] k adaptativo varia entre 3 e 12 conforme esperado
- [ ] Boosts aplicados corretamente (logs confirmam)

### **Fase 4 (Consolida√ß√£o)**
- [ ] Job mensal roda sem erros
- [ ] Mem√≥rias consolidadas s√£o criadas para clusters ‚â•5 conversas
- [ ] Resumos com LLM s√£o coerentes e informativos

### **Fase 5 (Context Building)**
- [ ] Contexto hier√°rquico √© leg√≠vel e organizado
- [ ] Mem√≥rias consolidadas aparecem quando relevantes
- [ ] Compression s√≥ ativa quando contexto > 2000 tokens

### **Fase 6 (M√©tricas)**
- [ ] Relat√≥rios de usu√°rio gerados sem erros
- [ ] Gaps temporais corretamente detectados
- [ ] Cobertura ChromaDB > 95% para usu√°rios ativos

---

## üìù Notas Finais

Este plano foi desenhado para ser implementado de forma **incremental e modular**, permitindo testar e ajustar cada fase antes de prosseguir.

**Prioridades:**
1. **Essencial:** Fases 1, 2, 3 (Metadata + Query + Two-Stage)
2. **Importante:** Fase 5 (Context Building)
3. **Desej√°vel:** Fases 4, 6 (Consolida√ß√£o + M√©tricas)

**Estimativa de Esfor√ßo Total:** 8-10 semanas de desenvolvimento

**Data de Cria√ß√£o:** 2026-01-02
**√öltima Atualiza√ß√£o:** 2026-01-02
**Vers√£o:** 1.0
