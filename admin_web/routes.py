from fastapi import APIRouter, Request, Depends, HTTPException, status
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
import os
from typing import Dict, List, Optional
import logging
from datetime import datetime
import json

# MIGRADO: Agora usa sistema session-based multi-tenant
# Apenas Master Admin pode acessar essas rotas
from admin_web.auth.middleware import require_master

# Importar core do Jung (opcional - pode falhar se depend√™ncias n√£o estiverem dispon√≠veis)
JUNG_CORE_ERROR = None
try:
    from jung_core import DatabaseManager, JungianEngine, Config
    JUNG_CORE_AVAILABLE = True
except Exception as e:
    import traceback
    JUNG_CORE_ERROR = traceback.format_exc()
    logging.error(f"‚ùå Erro ao importar jung_core: {e}")
    logging.error(f"Traceback:\n{JUNG_CORE_ERROR}")
    DatabaseManager = None
    JungianEngine = None
    Config = None
    JUNG_CORE_AVAILABLE = False

router = APIRouter(prefix="/admin", tags=["admin"])
templates = Jinja2Templates(directory="admin_web/templates")
logger = logging.getLogger(__name__)

# Inicializar componentes (Singleton pattern simples)
_db_manager = None

def get_db():
    global _db_manager
    if not JUNG_CORE_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="Database n√£o dispon√≠vel - jung_core n√£o carregado"
        )
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager

# ============================================================================
# AUTENTICA√á√ÉO
# ============================================================================
# A autentica√ß√£o agora √© gerenciada por admin_web/auth.py
# A fun√ß√£o verify_credentials foi importada acima e usa bcrypt para senhas hashadas

# ============================================================================
# ROTAS DE P√ÅGINA (HTML)
# ============================================================================

@router.get("/test")
async def test_route():
    """Rota de teste simples - n√£o requer autentica√ß√£o"""
    return {
        "status": "ok",
        "message": "Admin routes carregadas com sucesso!",
        "jung_core_available": JUNG_CORE_AVAILABLE
    }

@router.get("/", response_class=HTMLResponse)
async def dashboard(request: Request, admin: Dict = Depends(require_master)):
    """Dashboard principal - com fallback para quando jung_core n√£o est√° dispon√≠vel"""
    
    if not JUNG_CORE_AVAILABLE:
        # Dashboard de diagn√≥stico quando jung_core n√£o est√° dispon√≠vel
        import sys
        import platform
        
        # Tentar importar depend√™ncias individualmente para diagn√≥stico
        deps_status = {}
        for dep in ["openai", "chromadb", "langchain", "langchain_openai", "langchain_chroma"]:
            try:
                __import__(dep)
                deps_status[dep] = "‚úÖ OK"
            except ImportError as e:
                deps_status[dep] = f"‚ùå {str(e)[:50]}"
        
        return templates.TemplateResponse("dashboard.html", {
            "request": request,
            "jung_core_available": False,
            "total_users": 0,
            "total_interactions": 0,
            "total_conflicts": 0,
            "users": [],
            "diagnostic_mode": True,
            "python_version": platform.python_version(),
            "dependencies": deps_status,
            "error_message": "jung_core n√£o p√¥de ser carregado.",
            "error_traceback": JUNG_CORE_ERROR
        })
    
    # Modo normal com jung_core dispon√≠vel
    db = get_db()
    
    # Estat√≠sticas Gerais
    sqlite_users = db.get_all_users(platform="telegram")
    total_interactions = sum(u.get('total_messages', 0) for u in sqlite_users)
    
    # Conflitos
    cursor = db.conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM archetype_conflicts")
    total_conflicts = cursor.fetchone()[0]
    
    return templates.TemplateResponse("dashboard.html", {
        "request": request,
        "jung_core_available": True,
        "total_users": len(sqlite_users),
        "total_interactions": total_interactions,
        "total_conflicts": total_conflicts,
        "users": sqlite_users[:5],  # Top 5 recentes
        "diagnostic_mode": False
    })

@router.get("/users", response_class=HTMLResponse)
async def users_list(request: Request, admin: Dict = Depends(require_master)):
    """Lista de usu√°rios"""
    db = get_db()
    users = db.get_all_users(platform="telegram")
    return templates.TemplateResponse("users.html", {"request": request, "users": users})

@router.get("/sync-check", response_class=HTMLResponse)
async def sync_check_page(request: Request, admin: Dict = Depends(require_master)):
    """P√°gina de diagn√≥stico de sincroniza√ß√£o"""
    return templates.TemplateResponse("sync_check.html", {"request": request})

@router.get("/user/{user_id}/analysis", response_class=HTMLResponse)
async def user_analysis_page(request: Request, user_id: str, admin: Dict = Depends(require_master)):
    """P√°gina de an√°lise MBTI/Jungiana do usu√°rio"""
    db = get_db()

    # Buscar usu√°rio
    user = db.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="Usu√°rio n√£o encontrado")

    # Buscar conversas
    conversations = db.get_user_conversations(user_id, limit=50)
    total_conversations = db.count_conversations(user_id)

    # Buscar conflitos
    cursor = db.conn.cursor()
    cursor.execute("""
        SELECT COUNT(*) as count FROM archetype_conflicts WHERE user_id = ?
    """, (user_id,))
    total_conflicts = cursor.fetchone()[0]

    return templates.TemplateResponse("user_analysis.html", {
        "request": request,
        "user": user,
        "user_id": user_id,
        "total_conversations": total_conversations,
        "total_conflicts": total_conflicts,
        "conversations": conversations[:10]  # √öltimas 10 para preview
    })

@router.get("/user/{user_id}/agent-data", response_class=HTMLResponse)
async def user_agent_data_page(request: Request, user_id: str, admin: Dict = Depends(require_master)):
    """
    P√°gina de Dados do Agente

    Mostra:
    - Relat√≥rio resumido (total conversas, reativas, proativas, status)
    - 10 √∫ltimas mensagens reativas (conversa√ß√£o normal)
    - 10 √∫ltimas mensagens proativas (sistema proativo)
    """
    db = get_db()

    # Buscar usu√°rio
    user = db.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="Usu√°rio n√£o encontrado")

    cursor = db.conn.cursor()
    # Configurar row_factory para acessar colunas por nome
    cursor.row_factory = lambda cursor, row: {col[0]: row[idx] for idx, col in enumerate(cursor.description)}

    # ============================================================
    # 1. RELAT√ìRIO RESUMIDO
    # ============================================================

    # Total de conversas
    cursor.execute("SELECT COUNT(*) as count FROM conversations WHERE user_id = ?", (user_id,))
    total_conversations = cursor.fetchone()['count']

    # Conversas reativas (todas exceto plataforma 'proactive')
    cursor.execute("""
        SELECT COUNT(*) as count FROM conversations
        WHERE user_id = ? AND platform != 'proactive'
    """, (user_id,))
    reactive_count = cursor.fetchone()['count']

    # Mensagens proativas (tabela proactive_approaches)
    cursor.execute("""
        SELECT COUNT(*) as count FROM proactive_approaches
        WHERE user_id = ?
    """, (user_id,))
    proactive_count = cursor.fetchone()['count']

    # Primeira intera√ß√£o
    cursor.execute("""
        SELECT MIN(timestamp) as first_ts FROM conversations WHERE user_id = ?
    """, (user_id,))
    first_interaction = cursor.fetchone()['first_ts'] or "N/A"

    # √öltima atividade
    cursor.execute("""
        SELECT MAX(timestamp) as last_ts FROM conversations WHERE user_id = ?
    """, (user_id,))
    last_activity = cursor.fetchone()['last_ts'] or "N/A"

    # Status proativo (√∫ltima proativa + timestamp)
    cursor.execute("""
        SELECT timestamp FROM proactive_approaches
        WHERE user_id = ?
        ORDER BY timestamp DESC
        LIMIT 1
    """, (user_id,))
    last_proactive = cursor.fetchone()

    if last_proactive:
        from datetime import datetime, timedelta
        now = datetime.now()
        last_timestamp = datetime.fromisoformat(last_proactive.get('timestamp'))
        hours_since = (now - last_timestamp).total_seconds() / 3600

        # Cooldown de 12h (mesmo do sistema proativo)
        cooldown_hours = 12
        if hours_since < cooldown_hours:
            hours_left = cooldown_hours - hours_since
            proactive_status = f"‚è∏Ô∏è  Cooldown ({hours_left:.1f}h restantes)"
        else:
            proactive_status = "‚úÖ Ativo (pode receber mensagem)"
    else:
        proactive_status = "üÜï Nunca recebeu mensagem proativa"

    # Taxa de resposta (aproximada - conversas reativas / total)
    response_rate = int((reactive_count / total_conversations * 100)) if total_conversations > 0 else 0

    summary = {
        "total_conversations": total_conversations,
        "reactive_count": reactive_count,
        "proactive_count": proactive_count,
        "first_interaction": first_interaction[:16] if first_interaction != "N/A" else "N/A",
        "last_activity": last_activity[:16] if last_activity != "N/A" else "N/A",
        "proactive_status": proactive_status,
        "response_rate": response_rate
    }

    # ============================================================
    # 2. MENSAGENS REATIVAS (√∫ltimas 10)
    # ============================================================
    cursor.execute("""
        SELECT
            user_input,
            ai_response,
            timestamp,
            keywords
        FROM conversations
        WHERE user_id = ? AND platform != 'proactive'
        ORDER BY timestamp DESC
        LIMIT 10
    """, (user_id,))

    reactive_messages = []
    for row in cursor.fetchall():
        reactive_messages.append({
            "user_input": row.get('user_input', '') or "",
            "bot_response": row.get('ai_response', '') or "",
            "timestamp": row.get('timestamp', '')[:16] if row.get('timestamp') else "N/A",
            "keywords": row.get('keywords', '').split(',') if row.get('keywords') else []
        })

    # ============================================================
    # 3. MENSAGENS PROATIVAS (√∫ltimas 10)
    # ============================================================
    # Por enquanto, apenas mensagens de insights (sem JOIN com strategic_questions que pode n√£o existir)
    cursor.execute("""
        SELECT
            autonomous_insight,
            timestamp,
            archetype_primary,
            archetype_secondary,
            topic_extracted,
            knowledge_domain
        FROM proactive_approaches
        WHERE user_id = ?
        ORDER BY timestamp DESC
        LIMIT 10
    """, (user_id,))

    proactive_messages = []
    for row in cursor.fetchall():
        # Montar o par arquet√≠pico
        archetype_pair = f"{row.get('archetype_primary', '')} + {row.get('archetype_secondary', '')}" if row.get('archetype_primary') else None

        # Por enquanto, todas s√£o insights (perguntas estrat√©gicas ser√£o implementadas depois)
        message_type = 'insight'

        proactive_messages.append({
            "message": row.get('autonomous_insight', '') or "",
            "timestamp": row.get('timestamp', '')[:16] if row.get('timestamp') else "N/A",
            "message_type": message_type,
            "archetype_pair": archetype_pair,
            "topic": row.get('topic_extracted'),
            "target_dimension": None  # Ser√° preenchido quando strategic_questions existir
        })

    return templates.TemplateResponse("user_agent_data.html", {
        "request": request,
        "user": user,
        "user_id": user_id,
        "summary": summary,
        "reactive_messages": reactive_messages,
        "proactive_messages": proactive_messages
    })

# ============================================================================
# ROTAS DE API (HTMX / JSON)
# ============================================================================

@router.get("/api/sync-status")
async def get_sync_status(admin: Dict = Depends(require_master)):
    """Retorna status de sincroniza√ß√£o para o header"""
    # L√≥gica simplificada para o header
    return HTMLResponse(
        '<span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-green-100 text-green-800">Sistema Online</span>'
    )

@router.get("/api/diagnose")
async def run_diagnosis(admin: Dict = Depends(require_master)):
    """Roda diagn√≥stico completo (SQLite vs Chroma)"""
    db = get_db()
    
    # SQLite Stats
    sqlite_users = db.get_all_users(platform="telegram")
    sqlite_count = sum(u.get('total_messages', 0) for u in sqlite_users)
    
    # Chroma Stats
    chroma_count = 0
    chroma_status = "Desconectado"
    
    if db.chroma_enabled:
        try:
            chroma_count = db.vectorstore._collection.count()
            chroma_status = "Conectado"
        except Exception as e:
            chroma_status = f"Erro: {str(e)}"
    
    # Renderizar resultado (fragmento HTML)
    html = f"""
    <div class="grid grid-cols-1 gap-5 sm:grid-cols-2">
        <div class="bg-white overflow-hidden shadow rounded-lg">
            <div class="px-4 py-5 sm:p-6">
                <dt class="text-sm font-medium text-gray-500 truncate">SQLite (Metadados)</dt>
                <dd class="mt-1 text-3xl font-semibold text-gray-900">{sqlite_count}</dd>
            </div>
        </div>
        <div class="bg-white overflow-hidden shadow rounded-lg">
            <div class="px-4 py-5 sm:p-6">
                <dt class="text-sm font-medium text-gray-500 truncate">ChromaDB (Vetores)</dt>
                <dd class="mt-1 text-3xl font-semibold text-gray-900">{chroma_count}</dd>
                <dd class="mt-1 text-sm text-gray-500">{chroma_status}</dd>
            </div>
        </div>
    </div>
    """
    
    if abs(sqlite_count - chroma_count) > 5:
        html += """
        <div class="mt-4 bg-red-50 border-l-4 border-red-400 p-4">
            <div class="flex">
                <div class="flex-shrink-0">‚ö†Ô∏è</div>
                <div class="ml-3">
                    <p class="text-sm text-red-700">
                        Descasamento detectado! Diferen√ßa de {diff} registros.
                    </p>
                </div>
            </div>
        </div>
        """.format(diff=abs(sqlite_count - chroma_count))
    else:
        html += """
        <div class="mt-4 bg-green-50 border-l-4 border-green-400 p-4">
            <div class="flex">
                <div class="flex-shrink-0">‚úÖ</div>
                <div class="ml-3">
                    <p class="text-sm text-green-700">
                        Sincroniza√ß√£o saud√°vel.
                    </p>
                </div>
            </div>
        </div>
        """

    return HTMLResponse(html)

@router.post("/api/user/{user_id}/analyze-mbti")
async def analyze_user_mbti(request: Request, user_id: str, admin: Dict = Depends(require_master)):
    """Analisa padr√£o MBTI do usu√°rio usando Grok"""
    import re
    import json
    from openai import OpenAI

    db = get_db()

    # Verificar se XAI_API_KEY est√° dispon√≠vel
    xai_api_key = os.getenv("XAI_API_KEY")
    if not xai_api_key:
        return JSONResponse({
            "error": "XAI_API_KEY n√£o configurada",
            "type_indicator": "XXXX",
            "confidence": 0,
            "summary": "Configure a vari√°vel XAI_API_KEY para habilitar an√°lise MBTI"
        }, status_code=503)

    # Buscar conversas do usu√°rio
    conversations = db.get_user_conversations(user_id, limit=30)

    if len(conversations) < 5:
        return JSONResponse({
            "error": "Conversas insuficientes",
            "type_indicator": "XXXX",
            "confidence": 0,
            "summary": f"S√£o necess√°rias pelo menos 5 conversas para an√°lise. Atualmente: {len(conversations)}"
        }, status_code=400)

    # Extrair inputs do usu√°rio
    user_inputs = [conv['user_input'] for conv in conversations if conv.get('user_input')]

    if len(user_inputs) < 5:
        return JSONResponse({
            "error": "Dados insuficientes",
            "type_indicator": "XXXX",
            "confidence": 0,
            "summary": "Dados de conversas insuficientes para an√°lise"
        }, status_code=400)

    # Preparar amostra
    sample_size = min(15, len(user_inputs))
    first_inputs = user_inputs[:sample_size // 2]
    last_inputs = user_inputs[-(sample_size // 2):] if len(user_inputs) > sample_size // 2 else []

    inputs_text = "**Mensagens Iniciais:**\n"
    inputs_text += "\n".join([f"- {inp[:150]}..." for inp in first_inputs])

    if last_inputs:
        inputs_text += "\n\n**Mensagens Recentes:**\n"
        inputs_text += "\n".join([f"- {inp[:150]}..." for inp in last_inputs])

    # Calcular estat√≠sticas
    total_conversations = len(conversations)
    avg_tension = sum(conv.get('tension_level', 0) for conv in conversations) / total_conversations
    avg_affective = sum(conv.get('affective_charge', 0) for conv in conversations) / total_conversations

    # Prompt para Grok
    prompt = f"""
Analise o padr√£o psicol√≥gico deste usu√°rio seguindo princ√≠pios junguianos e o modelo MBTI.

**ESTAT√çSTICAS:**
- Total de intera√ß√µes: {total_conversations}
- Tens√£o m√©dia: {avg_tension:.2f}/10
- Carga afetiva m√©dia: {avg_affective:.1f}/100

**MENSAGENS DO USU√ÅRIO:**
{inputs_text}

Retorne JSON com esta estrutura EXATA:
{{
    "type_indicator": "XXXX (ex: INFP)",
    "confidence": 0-100,
    "dimensions": {{
        "E_I": {{
            "score": -100 a +100 (negativo=E, positivo=I),
            "interpretation": "An√°lise com evid√™ncias",
            "key_indicators": ["indicador1", "indicador2"]
        }},
        "S_N": {{"score": -100 a +100, "interpretation": "...", "key_indicators": [...]}},
        "T_F": {{"score": -100 a +100, "interpretation": "...", "key_indicators": [...]}},
        "J_P": {{"score": -100 a +100, "interpretation": "...", "key_indicators": [...]}}
    }},
    "dominant_function": "Ex: Ni (Intui√ß√£o Introvertida)",
    "auxiliary_function": "Ex: Fe",
    "summary": "Resumo anal√≠tico em 2-3 frases",
    "potentials": ["potencial1", "potencial2"],
    "challenges": ["desafio1", "desafio2"],
    "recommendations": ["recomenda√ß√£o1", "recomenda√ß√£o2"]
}}
"""

    try:
        # Chamar Grok
        client = OpenAI(
            api_key=xai_api_key,
            base_url="https://api.x.ai/v1"
        )

        response = client.chat.completions.create(
            model="grok-4-fast-reasoning",
            messages=[
                {"role": "system", "content": "Voc√™ √© um analista jungiano especializado em MBTI. Responda APENAS com JSON v√°lido."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        content = response.choices[0].message.content

        # Extrair JSON
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            analysis = json.loads(json_match.group())
        else:
            analysis = json.loads(content)

        return JSONResponse(analysis)

    except Exception as e:
        logger.error(f"Erro na an√°lise MBTI: {e}")
        return JSONResponse({
            "error": str(e),
            "type_indicator": "XXXX",
            "confidence": 0,
            "summary": f"Erro ao processar an√°lise: {str(e)}"
        }, status_code=500)

@router.get("/user/{user_id}/psychometrics", response_class=HTMLResponse)
async def user_psychometrics_page(request: Request, user_id: str, admin: Dict = Depends(require_master)):
    """P√°gina de an√°lises psicom√©tricas completas (Big Five, EQ, VARK, Schwartz)"""
    db = get_db()

    # Buscar usu√°rio
    user = db.get_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="Usu√°rio n√£o encontrado")

    # Verificar se an√°lise j√° existe (cache)
    psychometrics_data = db.get_psychometrics(user_id)

    # Se n√£o existe ou est√° desatualizada, gerar nova
    if not psychometrics_data:
        logger.info(f"üß™ Gerando an√°lises psicom√©tricas para {user_id}...")

        try:
            # Gerar todas as 4 an√°lises
            big_five = db.analyze_big_five(user_id, min_conversations=20)
            eq = db.analyze_emotional_intelligence(user_id)
            vark = db.analyze_learning_style(user_id, min_conversations=20)
            values = db.analyze_personal_values(user_id, min_conversations=20)

            # Verificar se houve erros
            errors = []
            if "error" in big_five:
                errors.append(f"Big Five: {big_five['error']}")
            if "error" in eq:
                errors.append(f"EQ: {eq['error']}")
            if "error" in vark:
                errors.append(f"VARK: {vark['error']}")
            if "error" in values:
                errors.append(f"Values: {values['error']}")

            if errors:
                # Renderizar p√°gina com erro
                return templates.TemplateResponse("user_psychometrics.html", {
                    "request": request,
                    "user": user,
                    "user_id": user_id,
                    "error": " | ".join(errors),
                    "conversations_count": db.count_conversations(user_id)
                })

            # Salvar no banco
            db.save_psychometrics(user_id, big_five, eq, vark, values)

            # Buscar dados salvos
            psychometrics_data = db.get_psychometrics(user_id)

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar psicometria: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=500, detail=f"Erro ao gerar an√°lise: {str(e)}")

    # Parse JSON fields
    import json as json_lib

    schwartz_values = {}
    if psychometrics_data.get('schwartz_values'):
        try:
            schwartz_values = json_lib.loads(psychometrics_data['schwartz_values'])
        except:
            schwartz_values = {}

    eq_details = {}
    if psychometrics_data.get('eq_details'):
        try:
            eq_details = json_lib.loads(psychometrics_data['eq_details'])
        except:
            eq_details = {}

    # Stats
    total_conversations = db.count_conversations(user_id)

    # An√°lise de Qualidade
    quality_analysis = None
    try:
        from quality_detector import QualityDetector

        detector = QualityDetector(db)
        conversations = db.get_user_conversations(user_id, limit=100)

        quality_analysis = detector.analyze_quality(
            user_id=user_id,
            psychometrics=psychometrics_data,
            conversations=conversations
        )

        # Salvar an√°lise de qualidade no banco
        version = psychometrics_data.get('version', 1)
        detector.save_quality_analysis(user_id, version, quality_analysis)

        logger.info(f"‚úì An√°lise de qualidade: {quality_analysis['overall_quality']} ({quality_analysis['quality_score']}%)")

    except Exception as e:
        logger.warning(f"‚ö† Erro ao gerar an√°lise de qualidade: {e}")
        # N√£o falha se an√°lise de qualidade der erro
        pass

    # Renderizar template
    return templates.TemplateResponse("user_psychometrics.html", {
        "request": request,
        "user": user,
        "user_id": user_id,
        "psychometrics": psychometrics_data,
        "schwartz_values": schwartz_values,
        "eq_details": eq_details,
        "total_conversations": total_conversations,
        "quality_analysis": quality_analysis
    })

@router.post("/api/user/{user_id}/regenerate-psychometrics")
async def regenerate_psychometrics(user_id: str, admin: Dict = Depends(require_master)):
    """For√ßa regenera√ß√£o das an√°lises psicom√©tricas (cria nova vers√£o)"""
    db = get_db()

    try:
        logger.info(f"üîÑ Regenerando an√°lises psicom√©tricas para {user_id}...")

        # Gerar todas as 4 an√°lises
        big_five = db.analyze_big_five(user_id, min_conversations=20)
        eq = db.analyze_emotional_intelligence(user_id)
        vark = db.analyze_learning_style(user_id, min_conversations=20)
        values = db.analyze_personal_values(user_id, min_conversations=20)

        # Verificar erros
        if "error" in big_five or "error" in eq or "error" in vark or "error" in values:
            error_msg = big_five.get("error") or eq.get("error") or vark.get("error") or values.get("error")
            return JSONResponse({"error": error_msg}, status_code=400)

        # Salvar (vai criar nova vers√£o)
        db.save_psychometrics(user_id, big_five, eq, vark, values)

        return JSONResponse({"success": True, "message": "An√°lises regeneradas com sucesso!"})

    except Exception as e:
        logger.error(f"‚ùå Erro ao regenerar psicometria: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@router.post("/api/user/{user_id}/generate-personal-report")
async def generate_personal_report(user_id: str, admin: Dict = Depends(require_master)):
    """
    Gera laudo psicom√©trico detalhado para o USU√ÅRIO
    6 par√°grafos focados em autoconhecimento e desenvolvimento pessoal
    """
    from llm_providers import create_llm_provider
    import json as json_lib

    db = get_db()

    try:
        logger.info(f"üîç [PERSONAL REPORT] Iniciando gera√ß√£o para user_id: {user_id}")

        # Buscar dados psicom√©tricos
        psychometrics = db.get_psychometrics(user_id)
        if not psychometrics:
            return JSONResponse({"error": "An√°lises psicom√©tricas n√£o encontradas"}, status_code=404)

        logger.info(f"üîç [PERSONAL REPORT] psychometrics type ANTES convers√£o: {type(psychometrics)}")

        # Converter Row para dict
        psychometrics = dict(psychometrics)

        logger.info(f"üîç [PERSONAL REPORT] psychometrics type AP√ìS convers√£o: {type(psychometrics)}")
        logger.info(f"üîç [PERSONAL REPORT] psychometrics keys: {list(psychometrics.keys())}")

        user = db.get_user(user_id)
        logger.info(f"üîç [PERSONAL REPORT] user type ANTES convers√£o: {type(user)}")

        if user:
            user = dict(user)  # Converter Row para dict
        else:
            user = {}

        logger.info(f"üîç [PERSONAL REPORT] user type AP√ìS convers√£o: {type(user)}")
        logger.info(f"üîç [PERSONAL REPORT] user keys: {list(user.keys())}")

        # Parse JSON fields
        schwartz_values = {}
        eq_details = {}
        executive_summary = []

        try:
            schwartz_str = psychometrics.get('schwartz_values', '{}')
            logger.info(f"üîç [PERSONAL REPORT] schwartz_str: {type(schwartz_str)} = {schwartz_str[:100] if schwartz_str else 'None'}")
            schwartz_values = json_lib.loads(schwartz_str) if schwartz_str else {}
            logger.info(f"üîç [PERSONAL REPORT] schwartz_values parsed: {type(schwartz_values)}")
        except Exception as e:
            logger.error(f"‚ùå [PERSONAL REPORT] Erro ao parsear schwartz_values: {e}")
            pass

        try:
            eq_str = psychometrics.get('eq_details', '{}')
            logger.info(f"üîç [PERSONAL REPORT] eq_str: {type(eq_str)}")
            eq_details = json_lib.loads(eq_str) if eq_str else {}
            logger.info(f"üîç [PERSONAL REPORT] eq_details parsed: {type(eq_details)}")
        except Exception as e:
            logger.error(f"‚ùå [PERSONAL REPORT] Erro ao parsear eq_details: {e}")
            pass

        try:
            summary_str = psychometrics.get('executive_summary', '[]')
            logger.info(f"üîç [PERSONAL REPORT] summary_str: {type(summary_str)}")
            executive_summary = json_lib.loads(summary_str) if summary_str else []
            logger.info(f"üîç [PERSONAL REPORT] executive_summary parsed: {type(executive_summary)} = {executive_summary}")

            # Se for dict, converter para lista de valores
            if isinstance(executive_summary, dict):
                logger.warning(f"‚ö†Ô∏è [PERSONAL REPORT] executive_summary √© dict, convertendo para lista")
                executive_summary = list(executive_summary.values()) if executive_summary else []

            # Garantir que √© lista
            if not isinstance(executive_summary, list):
                logger.warning(f"‚ö†Ô∏è [PERSONAL REPORT] executive_summary n√£o √© lista, usando []")
                executive_summary = []

        except Exception as e:
            logger.error(f"‚ùå [PERSONAL REPORT] Erro ao parsear executive_summary: {e}")
            executive_summary = []

        logger.info("üîç [PERSONAL REPORT] Iniciando constru√ß√£o do contexto f-string...")

        # Construir se√ß√µes seguras
        try:
            nome = user.get('user_name') or user.get('first_name') or 'Usu√°rio'
            nome_str = str(nome)
        except:
            nome_str = 'Usu√°rio'

        # Big Five scores - garantir valores num√©ricos
        def safe_score(val, default=0):
            try:
                return float(val if val is not None else default)
            except:
                return float(default)

        openness = safe_score(psychometrics.get('openness_score'))
        conscientiousness = safe_score(psychometrics.get('conscientiousness_score'))
        extraversion = safe_score(psychometrics.get('extraversion_score'))
        agreeableness = safe_score(psychometrics.get('agreeableness_score'))
        neuroticism = safe_score(psychometrics.get('neuroticism_score'))

        eq_score = safe_score(psychometrics.get('eq_score'))
        eq_self_awareness = safe_score(eq_details.get('self_awareness'))
        eq_self_management = safe_score(eq_details.get('self_management'))
        eq_social_awareness = safe_score(eq_details.get('social_awareness'))
        eq_relationship = safe_score(eq_details.get('relationship_management'))

        vark_visual = safe_score(psychometrics.get('vark_visual'))
        vark_auditory = safe_score(psychometrics.get('vark_auditory'))
        vark_reading = safe_score(psychometrics.get('vark_reading'))
        vark_kinesthetic = safe_score(psychometrics.get('vark_kinesthetic'))

        # JSON-safe strings
        schwartz_json = json_lib.dumps(schwartz_values, indent=2, ensure_ascii=False)
        executive_items = '\n'.join('- ' + str(item) for item in executive_summary)

        # Preparar contexto para o LLM
        context = f"""
PERFIL PSICOM√âTRICO DO USU√ÅRIO:

NOME: {nome_str}

BIG FIVE (OCEAN):
- Openness (Abertura): {openness:.1f}/10
- Conscientiousness (Conscienciosidade): {conscientiousness:.1f}/10
- Extraversion (Extrovers√£o): {extraversion:.1f}/10
- Agreeableness (Amabilidade): {agreeableness:.1f}/10
- Neuroticism (Neuroticismo): {neuroticism:.1f}/10

INTELIG√äNCIA EMOCIONAL:
- Score Geral: {eq_score:.1f}/10
- Autoconsci√™ncia: {eq_self_awareness:.1f}/10
- Autogest√£o: {eq_self_management:.1f}/10
- Consci√™ncia Social: {eq_social_awareness:.1f}/10
- Gest√£o de Relacionamentos: {eq_relationship:.1f}/10

ESTILO DE APRENDIZAGEM (VARK):
- Visual: {vark_visual:.1f}/10
- Auditivo: {vark_auditory:.1f}/10
- Leitura/Escrita: {vark_reading:.1f}/10
- Cinest√©sico: {vark_kinesthetic:.1f}/10

VALORES DE SCHWARTZ:
{schwartz_json}

RESUMO EXECUTIVO:
{executive_items}
"""

        logger.info("‚úÖ [PERSONAL REPORT] Contexto f-string constru√≠do com sucesso!")
        logger.info(f"üîç [PERSONAL REPORT] Tamanho do contexto: {len(context)} caracteres")

        # Gerar laudo com Claude
        logger.info("üîç [PERSONAL REPORT] Criando provider LLM...")
        llm = create_llm_provider("claude")
        logger.info("‚úÖ [PERSONAL REPORT] Provider LLM criado!")

        prompt = f"""Voc√™ √© um psic√≥logo organizacional especializado em an√°lises psicom√©tricas.

Com base nos dados abaixo, gere um LAUDO PSICOM√âTRICO PESSOAL detalhado para o pr√≥prio usu√°rio.

{context}

INSTRU√á√ïES:
1. Escreva 6 par√°grafos densos e informativos
2. Foco em AUTOCONHECIMENTO e DESENVOLVIMENTO PESSOAL
3. Tom emp√°tico, respeitoso e encorajador
4. Use dados concretos dos testes para embasar insights
5. Forne√ßa recomenda√ß√µes pr√°ticas e acion√°veis
6. Ajude o usu√°rio a entender seus pontos fortes e √°reas de crescimento

ESTRUTURA SUGERIDA:
- Par√°grafo 1: Vis√£o geral do perfil e principais caracter√≠sticas
- Par√°grafo 2: Big Five - Tra√ßos de personalidade e como impactam o dia a dia
- Par√°grafo 3: Intelig√™ncia Emocional - Pontos fortes e oportunidades
- Par√°grafo 4: Estilo de Aprendizagem - Como aprende melhor e dicas pr√°ticas
- Par√°grafo 5: Valores Pessoais - O que te motiva e guia suas decis√µes
- Par√°grafo 6: S√≠ntese e recomenda√ß√µes para desenvolvimento cont√≠nuo

IMPORTANTE:
- N√ÉO use bullet points ou listas
- Escreva em PAR√ÅGRAFOS corridos
- Seja espec√≠fico e personalizado
- Use linguagem acess√≠vel (n√£o jarg√µes excessivos)
- Seja honesto sobre pontos de aten√ß√£o, mas sempre construtivo

Gere o laudo:"""

        response = llm.get_response(prompt, max_tokens=2000)
        report_text = response.strip()

        return JSONResponse({
            "success": True,
            "report": report_text
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar laudo pessoal: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)


@router.post("/api/user/{user_id}/generate-hr-report")
async def generate_hr_report(user_id: str, admin: Dict = Depends(require_master)):
    """
    Gera laudo psicom√©trico detalhado para o RH/GESTOR
    6 par√°grafos focados em adequa√ß√£o organizacional e gest√£o de talentos
    """
    from llm_providers import create_llm_provider
    import json as json_lib

    db = get_db()

    try:
        # Buscar dados psicom√©tricos
        psychometrics = db.get_psychometrics(user_id)
        if not psychometrics:
            return JSONResponse({"error": "An√°lises psicom√©tricas n√£o encontradas"}, status_code=404)

        # Converter Row para dict
        psychometrics = dict(psychometrics)

        user = db.get_user(user_id)
        if user:
            user = dict(user)  # Converter Row para dict
        else:
            user = {}

        # Parse JSON fields with error handling
        schwartz_values = {}
        eq_details = {}
        executive_summary = []

        try:
            schwartz_str = psychometrics.get('schwartz_values', '{}')
            schwartz_values = json_lib.loads(schwartz_str) if schwartz_str else {}
        except:
            pass

        try:
            eq_str = psychometrics.get('eq_details', '{}')
            eq_details = json_lib.loads(eq_str) if eq_str else {}
        except:
            pass

        try:
            summary_str = psychometrics.get('executive_summary', '[]')
            executive_summary = json_lib.loads(summary_str) if summary_str else []

            # Se for dict, converter para lista de valores
            if isinstance(executive_summary, dict):
                executive_summary = list(executive_summary.values()) if executive_summary else []

            # Garantir que √© lista
            if not isinstance(executive_summary, list):
                executive_summary = []

        except:
            executive_summary = []

        # Construir se√ß√µes seguras
        try:
            nome = user.get('user_name') or user.get('first_name') or 'Colaborador'
            nome_str = str(nome)
        except:
            nome_str = 'Colaborador'

        # Big Five scores - garantir valores num√©ricos
        def safe_score(val, default=0):
            try:
                return float(val if val is not None else default)
            except:
                return float(default)

        openness = safe_score(psychometrics.get('openness_score'))
        conscientiousness = safe_score(psychometrics.get('conscientiousness_score'))
        extraversion = safe_score(psychometrics.get('extraversion_score'))
        agreeableness = safe_score(psychometrics.get('agreeableness_score'))
        neuroticism = safe_score(psychometrics.get('neuroticism_score'))

        eq_score = safe_score(psychometrics.get('eq_score'))
        eq_self_awareness = safe_score(eq_details.get('self_awareness'))
        eq_self_management = safe_score(eq_details.get('self_management'))
        eq_social_awareness = safe_score(eq_details.get('social_awareness'))
        eq_relationship = safe_score(eq_details.get('relationship_management'))

        vark_visual = safe_score(psychometrics.get('vark_visual'))
        vark_auditory = safe_score(psychometrics.get('vark_auditory'))
        vark_reading = safe_score(psychometrics.get('vark_reading'))
        vark_kinesthetic = safe_score(psychometrics.get('vark_kinesthetic'))

        # JSON-safe strings
        schwartz_json = json_lib.dumps(schwartz_values, indent=2, ensure_ascii=False)
        executive_items = '\n'.join('- ' + str(item) for item in executive_summary)

        # Preparar contexto para o LLM
        context = f"""
PERFIL PSICOM√âTRICO DO COLABORADOR:

NOME: {nome_str}
ID: {user_id}

BIG FIVE (OCEAN):
- Openness (Abertura): {openness:.1f}/10
- Conscientiousness (Conscienciosidade): {conscientiousness:.1f}/10
- Extraversion (Extrovers√£o): {extraversion:.1f}/10
- Agreeableness (Amabilidade): {agreeableness:.1f}/10
- Neuroticism (Neuroticismo): {neuroticism:.1f}/10

INTELIG√äNCIA EMOCIONAL:
- Score Geral: {eq_score:.1f}/10
- Autoconsci√™ncia: {eq_self_awareness:.1f}/10
- Autogest√£o: {eq_self_management:.1f}/10
- Consci√™ncia Social: {eq_social_awareness:.1f}/10
- Gest√£o de Relacionamentos: {eq_relationship:.1f}/10

ESTILO DE APRENDIZAGEM (VARK):
- Visual: {vark_visual:.1f}/10
- Auditivo: {vark_auditory:.1f}/10
- Leitura/Escrita: {vark_reading:.1f}/10
- Cinest√©sico: {vark_kinesthetic:.1f}/10

VALORES DE SCHWARTZ:
{schwartz_json}

RESUMO EXECUTIVO:
{executive_items}
"""

        # Gerar laudo com Claude
        llm = create_llm_provider("claude")

        prompt = f"""Voc√™ √© um consultor de RH especializado em avalia√ß√£o psicom√©trica e gest√£o de talentos.

Com base nos dados abaixo, gere um LAUDO PSICOM√âTRICO ORGANIZACIONAL detalhado para o gestor/RH.

{context}

INSTRU√á√ïES:
1. Escreva 6 par√°grafos densos e informativos
2. Foco em ADEQUA√á√ÉO ORGANIZACIONAL, GEST√ÉO DE TALENTOS e PERFORMANCE
3. Tom profissional, objetivo e estrat√©gico
4. Use dados concretos dos testes para embasar recomenda√ß√µes
5. Identifique fit cultural, potencial de lideran√ßa, e riscos/oportunidades
6. Forne√ßa insights acion√°veis para gest√£o e desenvolvimento

ESTRUTURA SUGERIDA:
- Par√°grafo 1: Perfil comportamental geral e adequa√ß√£o √† cultura organizacional
- Par√°grafo 2: Big Five - Impacto na performance, trabalho em equipe e lideran√ßa
- Par√°grafo 3: Intelig√™ncia Emocional - Capacidade de gest√£o, conflitos e relacionamentos
- Par√°grafo 4: Estilo de Aprendizagem - Como maximizar treinamentos e desenvolvimento
- Par√°grafo 5: Valores e Motivadores - Alinhamento com valores da empresa e engajamento
- Par√°grafo 6: Recomenda√ß√µes estrat√©gicas para gest√£o, desenvolvimento e aloca√ß√£o

IMPORTANTE:
- N√ÉO use bullet points ou listas
- Escreva em PAR√ÅGRAFOS corridos
- Seja direto sobre pontos de aten√ß√£o e riscos (mas sempre profissional)
- Foque em como maximizar o potencial do colaborador
- Considere sucess√£o, mobilidade interna e desenvolvimento de carreira
- Use linguagem corporativa e estrat√©gica

Gere o laudo:"""

        response = llm.get_response(prompt, max_tokens=2000)
        report_text = response.strip()

        return JSONResponse({
            "success": True,
            "report": report_text
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar laudo RH: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/user/{user_id}/psychometrics/download-pdf")
async def download_psychometrics_pdf(user_id: str, admin: Dict = Depends(require_master)):
    """
    Download de relat√≥rio psicom√©trico em PDF

    Gera PDF profissional com todas as 4 an√°lises:
    - Big Five (OCEAN)
    - Intelig√™ncia Emocional (EQ)
    - VARK (Estilos de Aprendizagem)
    - Valores de Schwartz
    """
    from fastapi.responses import StreamingResponse
    from pdf_generator import generate_psychometric_pdf
    import json as json_lib

    db = get_db()

    try:
        # Buscar usu√°rio
        user = db.get_user(user_id)
        if not user:
            raise HTTPException(status_code=404, detail="Usu√°rio n√£o encontrado")

        # Buscar an√°lises psicom√©tricas
        psychometrics_data = db.get_psychometrics(user_id)

        if not psychometrics_data:
            raise HTTPException(
                status_code=404,
                detail="An√°lises psicom√©tricas n√£o encontradas. Gere as an√°lises primeiro."
            )

        # Extrair dados de cada an√°lise
        big_five = json_lib.loads(psychometrics_data.get('big_five_data', '{}'))
        eq_data = {
            'eq_score': psychometrics_data.get('eq_score'),
            'eq_level': psychometrics_data.get('eq_level'),
            'eq_details': json_lib.loads(psychometrics_data.get('eq_details', '{}'))
        }
        vark_data = json_lib.loads(psychometrics_data.get('vark_data', '{}'))
        schwartz_data = json_lib.loads(psychometrics_data.get('schwartz_values', '{}'))

        # Contar conversas
        total_conversations = db.count_conversations(user_id)

        # Gerar PDF
        pdf_buffer = generate_psychometric_pdf(
            user_name=user['user_name'],
            total_conversations=total_conversations,
            big_five=big_five,
            eq=eq_data,
            vark=vark_data,
            values=schwartz_data
        )

        # Preparar resposta
        filename = f"relatorio_psicometrico_{user['user_name'].replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.pdf"

        # Garantir que o buffer est√° no in√≠cio
        pdf_buffer.seek(0)

        return StreamingResponse(
            iter([pdf_buffer.read()]),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"'
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar PDF: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erro ao gerar PDF: {str(e)}")

# ============================================================================
# üîç DIAGN√ìSTICO DE VAZAMENTO DE MEM√ìRIA
# ============================================================================

@router.get("/api/diagnose-facts")
async def diagnose_facts(admin: Dict = Depends(require_master)):
    """
    API para diagnosticar vazamento de mem√≥ria entre usu√°rios.
    Retorna todos os fatos de todos os usu√°rios para an√°lise.
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()

        # 1. Listar todos os usu√°rios
        cursor.execute("SELECT user_id, user_name, platform FROM users ORDER BY user_name")
        users = cursor.fetchall()

        users_list = []
        for user in users:
            users_list.append({
                "user_id": user['user_id'],
                "user_name": user['user_name'],
                "platform": user['platform']
            })

        # 2. Fatos por usu√°rio
        facts_by_user = {}
        for user in users:
            user_id = user['user_id']

            cursor.execute("""
                SELECT fact_category, fact_key, fact_value, is_current, version,
                       source_conversation_id
                FROM user_facts
                WHERE user_id = ?
                ORDER BY fact_category, fact_key, version DESC
            """, (user_id,))

            facts = cursor.fetchall()

            facts_by_user[user_id] = {
                "user_name": user['user_name'],
                "facts": []
            }

            for fact in facts:
                facts_by_user[user_id]["facts"].append({
                    "category": fact['fact_category'],
                    "key": fact['fact_key'],
                    "value": fact['fact_value'],
                    "is_current": bool(fact['is_current']),
                    "version": fact['version'],
                    "source_conversation_id": fact['source_conversation_id']
                })

        # 3. Verificar integridade
        cursor.execute("""
            SELECT COUNT(*) as count FROM user_facts WHERE user_id IS NULL OR user_id = ''
        """)
        null_facts_count = cursor.fetchone()['count']

        # 4. Buscar duplicatas
        cursor.execute("""
            SELECT fact_category, fact_key, fact_value, COUNT(DISTINCT user_id) as user_count,
                   GROUP_CONCAT(DISTINCT user_id) as user_ids
            FROM user_facts
            WHERE is_current = 1
            GROUP BY fact_category, fact_key, fact_value
            HAVING user_count > 1
        """)

        duplicates = cursor.fetchall()
        duplicates_list = []
        for dup in duplicates:
            duplicates_list.append({
                "category": dup['fact_category'],
                "key": dup['fact_key'],
                "value": dup['fact_value'],
                "user_count": dup['user_count'],
                "user_ids": dup['user_ids'].split(',') if dup['user_ids'] else []
            })

        return JSONResponse({
            "success": True,
            "users": users_list,
            "facts_by_user": facts_by_user,
            "integrity": {
                "null_facts_count": null_facts_count,
                "has_null_facts": null_facts_count > 0
            },
            "duplicates": duplicates_list,
            "has_leaks": len(duplicates_list) > 0
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao diagnosticar fatos: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/api/diagnose-chromadb")
async def diagnose_chromadb(admin: Dict = Depends(require_master)):
    """
    API para diagnosticar vazamento de mem√≥ria no ChromaDB.
    Retorna todas as conversas salvas no ChromaDB com seus metadados.
    """
    try:
        db = get_db()

        # Verificar se ChromaDB est√° habilitado
        if not db.chroma_enabled:
            return JSONResponse({
                "success": False,
                "error": "ChromaDB est√° desabilitado",
                "chroma_enabled": False
            })

        # Buscar TODOS os documentos do ChromaDB (sem filtro)
        # Isso vai revelar se h√° documentos com user_id errado
        try:
            # Get the collection directly
            collection = db.vectorstore._collection

            # Get all documents
            all_docs = collection.get(
                include=["metadatas", "documents"]
            )

            # Organizar por usu√°rio
            docs_by_user = {}
            total_docs = len(all_docs['ids'])

            for i in range(total_docs):
                doc_id = all_docs['ids'][i]
                metadata = all_docs['metadatas'][i]
                document = all_docs['documents'][i]

                user_id = metadata.get('user_id', 'N/A')
                user_name = metadata.get('user_name', 'N/A')

                if user_id not in docs_by_user:
                    docs_by_user[user_id] = {
                        "user_name": user_name,
                        "document_count": 0,
                        "documents": []
                    }

                docs_by_user[user_id]["document_count"] += 1
                docs_by_user[user_id]["documents"].append({
                    "doc_id": doc_id,
                    "user_input": metadata.get('user_input', ''),
                    "ai_response": metadata.get('ai_response', ''),
                    "conversation_id": metadata.get('conversation_id', 'N/A'),
                    "timestamp": metadata.get('timestamp', 'N/A'),
                    "preview": document[:200] if document else ""
                })

            # Buscar usu√°rios cadastrados
            cursor = db.conn.cursor()
            cursor.execute("SELECT user_id, user_name FROM users")
            registered_users = {row['user_id']: row['user_name'] for row in cursor.fetchall()}

            # Verificar integridade
            orphan_docs = []
            for user_id in docs_by_user.keys():
                if user_id not in registered_users and user_id != 'N/A':
                    orphan_docs.append({
                        "user_id": user_id,
                        "document_count": docs_by_user[user_id]["document_count"]
                    })

            return JSONResponse({
                "success": True,
                "chroma_enabled": True,
                "total_documents": total_docs,
                "registered_users": list(registered_users.keys()),
                "users_with_documents": list(docs_by_user.keys()),
                "docs_by_user": docs_by_user,
                "orphan_docs": orphan_docs,
                "has_orphans": len(orphan_docs) > 0
            })

        except Exception as e:
            logger.error(f"‚ùå Erro ao acessar ChromaDB: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return JSONResponse({"error": f"Erro ao acessar ChromaDB: {str(e)}"}, status_code=500)

    except Exception as e:
        logger.error(f"‚ùå Erro ao diagnosticar ChromaDB: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/api/conversation/{conversation_id}")
async def get_conversation_detail(conversation_id: int, admin: Dict = Depends(require_master)):
    """
    Retorna detalhes completos de uma conversa espec√≠fica.
    """
    try:
        db = get_db()
        cursor = db.conn.cursor()

        cursor.execute("""
            SELECT c.id, c.user_id, c.user_input, c.ai_response, c.timestamp,
                   u.user_name, u.platform
            FROM conversations c
            LEFT JOIN users u ON c.user_id = u.user_id
            WHERE c.id = ?
        """, (conversation_id,))

        conv = cursor.fetchone()

        if not conv:
            return JSONResponse({"error": "Conversa n√£o encontrada"}, status_code=404)

        return JSONResponse({
            "success": True,
            "conversation": {
                "id": conv['id'],
                "user_id": conv['user_id'],
                "user_name": conv['user_name'],
                "platform": conv['platform'],
                "timestamp": conv['timestamp'],
                "user_input": conv['user_input'],
                "ai_response": conv['ai_response']
            }
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar conversa: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# üîç SISTEMA DE EVID√äNCIAS (Evidence System 2.0)
# ============================================================================

@router.get("/user/{user_id}/psychometrics/{dimension}/evidence")
async def get_dimension_evidence(
    user_id: str,
    dimension: str,
    admin: Dict = Depends(require_master)
):
    """
    Retorna evid√™ncias (cita√ß√µes literais) que embasam um score espec√≠fico

    Dimens√µes v√°lidas:
    - openness
    - conscientiousness
    - extraversion
    - agreeableness
    - neuroticism
    """
    try:
        from evidence_extractor import EvidenceExtractor
        from llm_providers import create_llm_provider
        import json as json_lib

        db = get_db()

        # Validar dimens√£o
        valid_dimensions = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        if dimension not in valid_dimensions:
            raise HTTPException(
                status_code=400,
                detail=f"Dimens√£o inv√°lida. Use: {', '.join(valid_dimensions)}"
            )

        # Buscar an√°lise psicom√©trica do usu√°rio
        psychometrics = db.get_psychometrics(user_id)
        if not psychometrics:
            raise HTTPException(
                status_code=404,
                detail="An√°lise psicom√©trica n√£o encontrada para este usu√°rio"
            )

        # Criar extrator de evid√™ncias
        claude_provider = create_llm_provider("claude")
        extractor = EvidenceExtractor(db, claude_provider)

        # Verificar se evid√™ncias j√° existem
        existing_evidence = extractor.get_evidence_for_dimension(
            user_id=user_id,
            dimension=dimension,
            psychometric_version=psychometrics.get('version')
        )

        # Se evid√™ncias n√£o existem, extrair on-demand
        if not existing_evidence:
            logger.info(f"üîç Evid√™ncias n√£o encontradas para {user_id}/{dimension}. Extraindo...")

            # Buscar conversas
            conversations = db.get_user_conversations(user_id, limit=50)

            if len(conversations) < 10:
                return JSONResponse({
                    "dimension": dimension,
                    "score": psychometrics.get(f'{dimension}_score', 0),
                    "level": psychometrics.get(f'{dimension}_level', 'N/A'),
                    "evidence_available": False,
                    "message": f"Dados insuficientes ({len(conversations)} conversas, m√≠nimo 10)"
                })

            # Extrair evid√™ncias para esta dimens√£o
            big_five_scores = {
                dimension: {
                    'score': psychometrics.get(f'{dimension}_score', 50),
                    'level': psychometrics.get(f'{dimension}_level', 'M√©dio')
                }
            }

            evidence_list = extractor._extract_dimension_evidence(
                dimension=dimension,
                conversations=conversations,
                expected_score=big_five_scores[dimension]['score']
            )

            # Salvar evid√™ncias
            if evidence_list:
                all_evidence = {dimension: evidence_list}
                extractor.save_evidence_to_db(
                    user_id=user_id,
                    psychometric_version=psychometrics.get('version', 1),
                    all_evidence=all_evidence
                )

                existing_evidence = extractor.get_evidence_for_dimension(
                    user_id=user_id,
                    dimension=dimension,
                    psychometric_version=psychometrics.get('version')
                )

        # Formatar resposta
        return JSONResponse({
            "dimension": dimension,
            "score": psychometrics.get(f'{dimension}_score', 0),
            "level": psychometrics.get(f'{dimension}_level', 'N/A'),
            "description": psychometrics.get(f'{dimension}_description', ''),
            "evidence_available": len(existing_evidence) > 0,
            "num_evidence": len(existing_evidence),
            "evidence": existing_evidence[:10],  # Top 10 evid√™ncias
            "total_evidence": len(existing_evidence),
            "extraction_cached": len(existing_evidence) > 0
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar evid√™ncias: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@router.post("/user/{user_id}/psychometrics/extract-evidence")
async def extract_all_evidence(
    user_id: str,
    admin: Dict = Depends(require_master)
):
    """
    Extrai evid√™ncias para todas as dimens√µes do Big Five
    (Processo pode demorar ~30-60s)
    """
    try:
        from evidence_extractor import EvidenceExtractor
        from llm_providers import create_llm_provider
        import json as json_lib

        db = get_db()

        # Buscar an√°lise psicom√©trica
        psychometrics = db.get_psychometrics(user_id)
        if not psychometrics:
            raise HTTPException(
                status_code=404,
                detail="An√°lise psicom√©trica n√£o encontrada"
            )

        # Buscar conversas
        conversations = db.get_user_conversations(user_id, limit=50)

        if len(conversations) < 10:
            return JSONResponse({
                "success": False,
                "message": f"Dados insuficientes ({len(conversations)} conversas, m√≠nimo 10)"
            })

        # Criar extrator
        claude_provider = create_llm_provider("claude")
        extractor = EvidenceExtractor(db, claude_provider)

        # Preparar scores Big Five
        big_five_scores = {
            'openness': {
                'score': psychometrics.get('openness_score', 50),
                'level': psychometrics.get('openness_level', 'M√©dio')
            },
            'conscientiousness': {
                'score': psychometrics.get('conscientiousness_score', 50),
                'level': psychometrics.get('conscientiousness_level', 'M√©dio')
            },
            'extraversion': {
                'score': psychometrics.get('extraversion_score', 50),
                'level': psychometrics.get('extraversion_level', 'M√©dio')
            },
            'agreeableness': {
                'score': psychometrics.get('agreeableness_score', 50),
                'level': psychometrics.get('agreeableness_level', 'M√©dio')
            },
            'neuroticism': {
                'score': psychometrics.get('neuroticism_score', 50),
                'level': psychometrics.get('neuroticism_level', 'M√©dio')
            }
        }

        # Extrair evid√™ncias
        logger.info(f"üîç Extraindo evid√™ncias para {user_id}...")
        all_evidence = extractor.extract_evidence_for_user(
            user_id=user_id,
            psychometric_version=psychometrics.get('version', 1),
            conversations=conversations,
            big_five_scores=big_five_scores
        )

        # Salvar no banco
        total_saved = extractor.save_evidence_to_db(
            user_id=user_id,
            psychometric_version=psychometrics.get('version', 1),
            all_evidence=all_evidence
        )

        # Contar evid√™ncias por dimens√£o
        evidence_counts = {
            dimension: len(evidence_list)
            for dimension, evidence_list in all_evidence.items()
        }

        return JSONResponse({
            "success": True,
            "total_evidence_extracted": total_saved,
            "evidence_by_dimension": evidence_counts,
            "message": f"Evid√™ncias extra√≠das com sucesso para {len(all_evidence)} dimens√µes"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro ao extrair evid√™ncias: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================
# JUNG LAB - SISTEMA DE RUMINA√á√ÉO (Admin Dashboard)
# ============================================================

@router.get("/jung-lab", response_class=HTMLResponse)
async def jung_lab_dashboard(
    request: Request,
    admin: Dict = Depends(require_master)
):
    """Dashboard do Sistema de Rumina√ß√£o Cognitiva (Admin only)"""
    from rumination_config import ADMIN_USER_ID
    from jung_rumination import RuminationEngine
    import os

    db = get_db()
    rumination = RuminationEngine(db)

    # Buscar estat√≠sticas gerais
    stats = rumination.get_stats(ADMIN_USER_ID)

    # Buscar √∫ltimos fragmentos
    cursor = db.conn.cursor()
    cursor.execute("""
        SELECT id, fragment_type, content, source_quote, emotional_weight,
               datetime(created_at, 'localtime') as created_at
        FROM rumination_fragments
        WHERE user_id = ?
        ORDER BY created_at DESC
        LIMIT 10
    """, (ADMIN_USER_ID,))
    fragments = [dict(row) for row in cursor.fetchall()]

    # Buscar tens√µes ativas
    cursor.execute("""
        SELECT id, tension_type, pole_a_content, pole_b_content,
               tension_description, intensity, maturity_score, status,
               datetime(first_detected_at, 'localtime') as created_at,
               datetime(last_revisited_at, 'localtime') as last_revisit
        FROM rumination_tensions
        WHERE user_id = ? AND status != 'archived'
        ORDER BY maturity_score DESC, first_detected_at DESC
        LIMIT 10
    """, (ADMIN_USER_ID,))
    tensions = [dict(row) for row in cursor.fetchall()]

    # Buscar insights (ready e delivered)
    cursor.execute("""
        SELECT id, symbol_content, question_content, full_message, depth_score, status,
               datetime(crystallized_at, 'localtime') as created_at,
               datetime(delivered_at, 'localtime') as delivered_at
        FROM rumination_insights
        WHERE user_id = ?
        ORDER BY crystallized_at DESC
        LIMIT 10
    """, (ADMIN_USER_ID,))
    insights = [dict(row) for row in cursor.fetchall()]

    # Verificar se scheduler est√° rodando
    scheduler_running = os.path.exists("rumination_scheduler.pid")
    scheduler_pid = None
    if scheduler_running:
        try:
            with open("rumination_scheduler.pid", "r") as f:
                scheduler_pid = int(f.read().strip())
        except:
            scheduler_running = False

    return templates.TemplateResponse(
        "jung_lab.html",
        {
            "request": request,
            "username": username,
            "stats": stats,
            "fragments": fragments,
            "tensions": tensions,
            "insights": insights,
            "scheduler_running": scheduler_running,
            "scheduler_pid": scheduler_pid
        }
    )


@router.post("/api/jung-lab/digest")
async def run_manual_digest(
    admin: Dict = Depends(require_master)
):
    """Executa digest√£o manual do sistema de rumina√ß√£o"""
    from rumination_config import ADMIN_USER_ID
    from jung_rumination import RuminationEngine

    try:
        db = get_db()
        rumination = RuminationEngine(db)

        # Executar digest√£o
        digest_stats = rumination.digest(ADMIN_USER_ID)

        # Verificar se h√° insights para entregar
        delivered_id = rumination.check_and_deliver(ADMIN_USER_ID)

        # Obter estat√≠sticas atualizadas
        stats = rumination.get_stats(ADMIN_USER_ID)

        return JSONResponse({
            "success": True,
            "digest_stats": digest_stats,
            "delivered_insight_id": delivered_id,
            "current_stats": stats,
            "message": "Digest√£o executada com sucesso"
        })

    except Exception as e:
        logger.error(f"‚ùå Erro na digest√£o manual: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@router.post("/api/jung-lab/scheduler/{action}")
async def control_scheduler(
    action: str,
    admin: Dict = Depends(require_master)
):
    """Controla o scheduler de rumina√ß√£o (start/stop)"""
    import subprocess
    import os
    import signal
    import sys

    pid_file = "rumination_scheduler.pid"

    try:
        if action == "start":
            # Verificar se j√° est√° rodando
            if os.path.exists(pid_file):
                return JSONResponse({
                    "success": False,
                    "message": "Scheduler j√° est√° rodando"
                }, status_code=400)

            # Iniciar processo em background
            python_exe = sys.executable
            process = subprocess.Popen(
                [python_exe, "rumination_scheduler.py"],
                start_new_session=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )

            # Salvar PID
            with open(pid_file, "w") as f:
                f.write(str(process.pid))

            return JSONResponse({
                "success": True,
                "pid": process.pid,
                "message": "Scheduler iniciado com sucesso"
            })

        elif action == "stop":
            # Verificar se est√° rodando
            if not os.path.exists(pid_file):
                return JSONResponse({
                    "success": False,
                    "message": "Scheduler n√£o est√° rodando"
                }, status_code=400)

            # Ler PID e matar processo
            with open(pid_file, "r") as f:
                pid = int(f.read().strip())

            try:
                os.kill(pid, signal.SIGTERM)
                os.remove(pid_file)

                return JSONResponse({
                    "success": True,
                    "message": "Scheduler parado com sucesso"
                })
            except ProcessLookupError:
                # Processo j√° morreu, apenas remove PID file
                os.remove(pid_file)
                return JSONResponse({
                    "success": True,
                    "message": "Scheduler n√£o estava rodando (PID file removido)"
                })

        else:
            return JSONResponse({
                "success": False,
                "message": f"A√ß√£o inv√°lida: {action}"
            }, status_code=400)

    except Exception as e:
        logger.error(f"‚ùå Erro ao controlar scheduler: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/api/jung-lab/diagnose")
async def diagnose_rumination(
    admin: Dict = Depends(require_master)
):
    """
    Diagn√≥stico completo do sistema de rumina√ß√£o
    Verifica conversas, fragmentos, tens√µes e poss√≠veis problemas
    """
    from rumination_config import ADMIN_USER_ID
    from jung_rumination import RuminationEngine

    try:
        db = get_db()
        cursor = db.conn.cursor()

        diagnosis = {
            "admin_user_id": ADMIN_USER_ID,
            "conversations": {},
            "rumination_tables": {},
            "problems": [],
            "recommendations": []
        }

        # 1. VERIFICAR CONVERSAS
        cursor.execute('SELECT COUNT(*) FROM conversations')
        total_conversations = cursor.fetchone()[0]
        diagnosis["conversations"]["total"] = total_conversations

        cursor.execute('SELECT COUNT(*) FROM conversations WHERE user_id = ?', (ADMIN_USER_ID,))
        admin_conversations = cursor.fetchone()[0]
        diagnosis["conversations"]["admin_total"] = admin_conversations

        if admin_conversations > 0:
            # Conversas por plataforma
            cursor.execute('''
                SELECT platform, COUNT(*) as count
                FROM conversations
                WHERE user_id = ?
                GROUP BY platform
            ''', (ADMIN_USER_ID,))
            diagnosis["conversations"]["by_platform"] = {
                row[0] or "NULL": row[1] for row in cursor.fetchall()
            }

            # √öltima conversa
            cursor.execute('''
                SELECT timestamp, platform, user_input
                FROM conversations
                WHERE user_id = ?
                ORDER BY timestamp DESC
                LIMIT 1
            ''', (ADMIN_USER_ID,))
            last = cursor.fetchone()
            if last:
                diagnosis["conversations"]["last"] = {
                    "timestamp": last[0],
                    "platform": last[1],
                    "preview": last[2][:100] if last[2] else None
                }

            # √öltimas 5 conversas com plataforma (para debug)
            cursor.execute('''
                SELECT timestamp, platform, user_input
                FROM conversations
                WHERE user_id = ?
                ORDER BY timestamp DESC
                LIMIT 5
            ''', (ADMIN_USER_ID,))
            diagnosis["conversations"]["recent_samples"] = [
                {
                    "timestamp": row[0],
                    "platform": row[1],
                    "preview": row[2][:60] if row[2] else None
                }
                for row in cursor.fetchall()
            ]
        else:
            diagnosis["problems"].append({
                "severity": "CRITICAL",
                "issue": "N√£o h√° conversas do admin no banco de dados",
                "details": f"User ID configurado: {ADMIN_USER_ID}"
            })
            diagnosis["recommendations"].append({
                "action": "Verificar se o bot est√° rodando e recebendo mensagens",
                "steps": [
                    "1. Enviar mensagem de teste no Telegram",
                    "2. Verificar logs do Railway para erros",
                    f"3. Confirmar que seu Telegram ID √©: {ADMIN_USER_ID}",
                    "4. Verificar se o bot est√° salvando conversas corretamente"
                ]
            })

        # 2. VERIFICAR TABELAS DE RUMINA√á√ÉO
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '%rumination%'")
        tables = [row[0] for row in cursor.fetchall()]

        if not tables:
            diagnosis["problems"].append({
                "severity": "HIGH",
                "issue": "Tabelas de rumina√ß√£o n√£o existem",
                "details": "As tabelas deveriam ser criadas automaticamente"
            })
            diagnosis["recommendations"].append({
                "action": "Reiniciar o servi√ßo web para criar as tabelas",
                "steps": [
                    "1. Fazer deploy no Railway",
                    "2. Aguardar inicializa√ß√£o completa",
                    "3. Acessar /admin/jung-lab novamente"
                ]
            })
        else:
            diagnosis["rumination_tables"]["found"] = tables

            for table in tables:
                cursor.execute(f'SELECT COUNT(*) FROM {table} WHERE user_id = ?', (ADMIN_USER_ID,))
                count = cursor.fetchone()[0]
                diagnosis["rumination_tables"][table] = {
                    "count": count
                }

                if count > 0:
                    # Get sample
                    cursor.execute(f'SELECT * FROM {table} WHERE user_id = ? LIMIT 1', (ADMIN_USER_ID,))
                    diagnosis["rumination_tables"][table]["has_data"] = True

            # Verificar problemas espec√≠ficos
            frag_count = diagnosis["rumination_tables"].get("rumination_fragments", {}).get("count", 0)
            tension_count = diagnosis["rumination_tables"].get("rumination_tensions", {}).get("count", 0)

            if admin_conversations > 0 and frag_count == 0:
                # Verificar se tem conversas telegram
                cursor.execute('''
                    SELECT COUNT(*) FROM conversations
                    WHERE user_id = ? AND platform = 'telegram'
                ''', (ADMIN_USER_ID,))
                telegram_count = cursor.fetchone()[0]

                if telegram_count == 0:
                    diagnosis["problems"].append({
                        "severity": "HIGH",
                        "issue": "H√° conversas mas NENHUMA tem platform='telegram'",
                        "details": f"Hook de rumina√ß√£o s√≥ processa platform='telegram'. Conversas: {admin_conversations}, Telegram: {telegram_count}"
                    })
                    diagnosis["recommendations"].append({
                        "action": "FIX: Atualizar conversas antigas para platform='telegram'",
                        "steps": [
                            "1. Executar SQL: UPDATE conversations SET platform='telegram' WHERE user_id='367f9e509e396d51' AND (platform IS NULL OR platform != 'telegram')",
                            "2. Enviar nova mensagem no Telegram",
                            "3. Verificar se agora cria fragmentos"
                        ]
                    })
                else:
                    diagnosis["problems"].append({
                        "severity": "HIGH",
                        "issue": "H√° conversas mas n√£o h√° fragmentos",
                        "details": f"O hook de rumina√ß√£o pode n√£o estar sendo chamado ou a LLM n√£o est√° extraindo fragmentos. Telegram: {telegram_count}/{admin_conversations}"
                    })
                    diagnosis["recommendations"].append({
                        "action": "Verificar logs do bot para erros no hook de rumina√ß√£o",
                        "steps": [
                            "1. Verificar logs Railway para warnings: '‚ö†Ô∏è Erro no hook de rumina√ß√£o'",
                            "2. Verificar se h√° mensagem 'üß† Rumina√ß√£o: Ingest√£o executada' nos logs",
                            "3. Testar enviar nova mensagem e verificar se cria fragmentos"
                        ]
                    })

            if frag_count > 0 and tension_count == 0:
                diagnosis["problems"].append({
                    "severity": "MEDIUM",
                    "issue": "H√° fragmentos mas n√£o h√° tens√µes",
                    "details": f"Com {frag_count} fragmentos, deveria haver pelo menos algumas tens√µes detectadas"
                })
                diagnosis["recommendations"].append({
                    "action": "Verificar detec√ß√£o de tens√µes",
                    "steps": [
                        "1. Enviar mais mensagens com temas contradit√≥rios",
                        "2. Verificar logs da LLM durante detec√ß√£o",
                        f"3. Considerar que pode precisar de mais fragmentos (atual: {frag_count})"
                    ]
                })

        # 3. STATUS GERAL
        if len(diagnosis["problems"]) == 0:
            if admin_conversations > 0:
                diagnosis["status"] = "OK"
                diagnosis["message"] = "Sistema funcionando corretamente"
            else:
                diagnosis["status"] = "NO_DATA"
                diagnosis["message"] = "Sistema pronto mas sem dados para processar"
        else:
            diagnosis["status"] = "ERROR"
            diagnosis["message"] = f"Encontrados {len(diagnosis['problems'])} problemas"

        return JSONResponse(diagnosis)

    except Exception as e:
        logger.error(f"‚ùå Erro no diagn√≥stico: {e}", exc_info=True)
        return JSONResponse({
            "status": "ERROR",
            "error": str(e),
            "problems": [{
                "severity": "CRITICAL",
                "issue": "Erro ao executar diagn√≥stico",
                "details": str(e)
            }]
        }, status_code=500)


@router.post("/api/jung-lab/fix-platform")
async def fix_platform_issue(
    admin: Dict = Depends(require_master)
):
    """
    FIX autom√°tico: Atualiza conversas antigas para platform='telegram'
    Resolve o problema de conversas sem platform definido
    """
    from rumination_config import ADMIN_USER_ID

    try:
        db = get_db()
        cursor = db.conn.cursor()

        # Verificar quantas conversas ser√£o atualizadas
        cursor.execute('''
            SELECT COUNT(*) FROM conversations
            WHERE user_id = ?
            AND (platform IS NULL OR platform NOT IN ('telegram', 'proactive', 'proactive_rumination'))
        ''', (ADMIN_USER_ID,))
        count_to_update = cursor.fetchone()[0]

        if count_to_update == 0:
            return JSONResponse({
                "success": True,
                "updated": 0,
                "message": "Nenhuma conversa precisa ser atualizada"
            })

        # Atualizar conversas
        cursor.execute('''
            UPDATE conversations
            SET platform = 'telegram'
            WHERE user_id = ?
            AND (platform IS NULL OR platform NOT IN ('telegram', 'proactive', 'proactive_rumination'))
        ''', (ADMIN_USER_ID,))
        db.conn.commit()

        logger.info(f"‚úÖ Platform fix: {count_to_update} conversas atualizadas para platform='telegram'")

        return JSONResponse({
            "success": True,
            "updated": count_to_update,
            "message": f"‚úÖ {count_to_update} conversas atualizadas. Agora envie uma nova mensagem no Telegram para testar."
        })

    except Exception as e:
        logger.error(f"‚ùå Erro no fix de platform: {e}", exc_info=True)
        return JSONResponse({
            "success": False,
            "error": str(e)
        }, status_code=500)


@router.get("/api/jung-lab/debug-full")
async def debug_rumination_full(
    admin: Dict = Depends(require_master)
):
    """
    Debug completo do sistema de rumina√ß√£o
    Executa todos os testes para identificar problemas
    """
    from rumination_config import ADMIN_USER_ID, MIN_TENSION_LEVEL
    import inspect

    try:
        db = get_db()
        cursor = db.conn.cursor()

        debug_result = {
            "config": {},
            "tables": {},
            "conversations": {},
            "telegram_conversations": {},
            "fragments": {},
            "hook_code": {},
            "imports": {},
            "problems": [],
            "recommendations": []
        }

        # TESTE 1: Configura√ß√£o
        debug_result["config"] = {
            "admin_user_id": ADMIN_USER_ID,
            "min_tension_level": MIN_TENSION_LEVEL
        }

        # TESTE 2: Tabelas
        tables = ['rumination_fragments', 'rumination_tensions', 'rumination_insights', 'rumination_log']

        for table in tables:
            cursor.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'")
            exists = cursor.fetchone()

            if exists:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]

                cursor.execute(f"PRAGMA table_info({table})")
                columns = [row[1] for row in cursor.fetchall()]

                debug_result["tables"][table] = {
                    "exists": True,
                    "count": count,
                    "columns": columns
                }
            else:
                debug_result["tables"][table] = {"exists": False}
                debug_result["problems"].append(f"Tabela {table} n√£o existe")

        # TESTE 3: Conversas do admin
        cursor.execute('SELECT COUNT(*) FROM conversations WHERE user_id = ?', (ADMIN_USER_ID,))
        total_convs = cursor.fetchone()[0]

        debug_result["conversations"]["total"] = total_convs

        if total_convs > 0:
            cursor.execute('''
                SELECT platform, COUNT(*) as count
                FROM conversations
                WHERE user_id = ?
                GROUP BY platform
            ''', (ADMIN_USER_ID,))

            by_platform = {(row[0] or 'NULL'): row[1] for row in cursor.fetchall()}
            debug_result["conversations"]["by_platform"] = by_platform

            # √öltimas 3
            cursor.execute('''
                SELECT id, timestamp, platform, user_input
                FROM conversations
                WHERE user_id = ?
                ORDER BY timestamp DESC
                LIMIT 3
            ''', (ADMIN_USER_ID,))

            debug_result["conversations"]["recent"] = [
                {
                    "id": row[0],
                    "timestamp": row[1],
                    "platform": row[2] or 'NULL',
                    "preview": row[3][:80] if row[3] else None
                }
                for row in cursor.fetchall()
            ]

        # TESTE 4: Conversas telegram
        cursor.execute('''
            SELECT COUNT(*) FROM conversations
            WHERE user_id = ? AND platform = 'telegram'
        ''', (ADMIN_USER_ID,))
        telegram_count = cursor.fetchone()[0]

        debug_result["telegram_conversations"]["count"] = telegram_count

        if telegram_count > 0:
            cursor.execute('''
                SELECT id, timestamp, user_input
                FROM conversations
                WHERE user_id = ? AND platform = 'telegram'
                ORDER BY timestamp DESC
                LIMIT 3
            ''', (ADMIN_USER_ID,))

            debug_result["telegram_conversations"]["recent"] = [
                {
                    "id": row[0],
                    "timestamp": row[1],
                    "preview": row[2][:80] if row[2] else None
                }
                for row in cursor.fetchall()
            ]

        # TESTE 5: Fragmentos
        cursor.execute('SELECT COUNT(*) FROM rumination_fragments WHERE user_id = ?', (ADMIN_USER_ID,))
        frag_count = cursor.fetchone()[0]

        debug_result["fragments"]["count"] = frag_count

        if frag_count > 0:
            cursor.execute('''
                SELECT id, fragment_type, content, emotional_weight, created_at
                FROM rumination_fragments
                WHERE user_id = ?
                ORDER BY created_at DESC
                LIMIT 3
            ''', (ADMIN_USER_ID,))

            debug_result["fragments"]["recent"] = [
                {
                    "id": row[0],
                    "type": row[1],
                    "content": row[2][:80],
                    "weight": row[3],
                    "created_at": row[4]
                }
                for row in cursor.fetchall()
            ]

        # TESTE 6: Hook de rumina√ß√£o
        try:
            import jung_core

            source = inspect.getsource(jung_core.HybridDatabaseManager.save_conversation)

            has_hook = "Hook rumina√ß√£o" in source or "Sistema de Rumina√ß√£o" in source or "HOOK: Sistema de Rumina√ß√£o" in source

            debug_result["hook_code"]["present"] = has_hook
            debug_result["hook_code"]["lines_count"] = len([l for l in source.split('\n') if 'rumina√ß√£o' in l.lower() or 'rumination' in l.lower()])

            if not has_hook:
                debug_result["problems"].append("C√≥digo do hook N√ÉO encontrado em save_conversation")

        except Exception as e:
            debug_result["hook_code"]["error"] = str(e)
            debug_result["problems"].append(f"Erro ao verificar hook: {e}")

        # TESTE 7: Imports
        try:
            from rumination_config import ADMIN_USER_ID as test_id
            debug_result["imports"]["rumination_config"] = "OK"
        except Exception as e:
            debug_result["imports"]["rumination_config"] = f"ERRO: {e}"
            debug_result["problems"].append(f"Erro ao importar rumination_config: {e}")

        try:
            from jung_rumination import RuminationEngine
            rumination = RuminationEngine(db)
            debug_result["imports"]["jung_rumination"] = "OK"
            debug_result["imports"]["engine_admin_id"] = rumination.admin_user_id
        except Exception as e:
            debug_result["imports"]["jung_rumination"] = f"ERRO: {e}"
            debug_result["problems"].append(f"Erro ao importar/inicializar RuminationEngine: {e}")

        # AN√ÅLISE DE PROBLEMAS
        if telegram_count == 0 and total_convs > 0:
            debug_result["problems"].append("CR√çTICO: H√° conversas mas nenhuma tem platform='telegram'")
            debug_result["recommendations"].append("Executar fix platform: POST /admin/api/jung-lab/fix-platform")

        if telegram_count > 0 and frag_count == 0:
            debug_result["problems"].append("CR√çTICO: H√° conversas telegram mas nenhum fragmento foi criado")
            debug_result["recommendations"].append("Hook n√£o est√° sendo executado ou LLM n√£o est√° extraindo fragmentos")
            debug_result["recommendations"].append("Verificar logs do Railway para mensagens: 'üîç Hook rumina√ß√£o', 'üîç INGEST chamado'")

        if total_convs == 0:
            debug_result["problems"].append("Nenhuma conversa do admin no banco")
            debug_result["recommendations"].append("Enviar mensagem no Telegram e verificar se √© salva")

        # Status geral
        if len(debug_result["problems"]) == 0:
            debug_result["status"] = "OK"
            debug_result["message"] = "Sistema aparentemente funcional"
        else:
            debug_result["status"] = "ERROR"
            debug_result["message"] = f"{len(debug_result['problems'])} problema(s) encontrado(s)"

        return JSONResponse(debug_result)

    except Exception as e:
        logger.error(f"‚ùå Erro no debug completo: {e}", exc_info=True)
        return JSONResponse({
            "status": "ERROR",
            "error": str(e),
            "problems": [f"Erro fatal ao executar debug: {e}"]
        }, status_code=500)


@router.get("/api/jung-lab/why-no-insights")
async def why_no_insights(
    _admin: Dict = Depends(require_master)
):
    """
    Diagn√≥stico espec√≠fico: Por que n√£o h√° insights sendo gerados?
    Analisa maturidade das tens√µes e identifica bloqueios
    """
    from rumination_config import (
        ADMIN_USER_ID, MIN_MATURITY_FOR_SYNTHESIS,
        MIN_DAYS_FOR_SYNTHESIS, MIN_EVIDENCE_FOR_SYNTHESIS,
        MATURITY_WEIGHTS
    )
    from datetime import datetime

    try:
        db = get_db()
        cursor = db.conn.cursor()

        result = {
            "config": {
                "MIN_MATURITY_FOR_SYNTHESIS": MIN_MATURITY_FOR_SYNTHESIS,
                "MIN_DAYS_FOR_SYNTHESIS": MIN_DAYS_FOR_SYNTHESIS,
                "MIN_EVIDENCE_FOR_SYNTHESIS": MIN_EVIDENCE_FOR_SYNTHESIS,
                "MATURITY_WEIGHTS": MATURITY_WEIGHTS
            },
            "tensions": [],
            "problem_identified": None,
            "solution": None
        }

        # Buscar todas as tens√µes
        cursor.execute("""
            SELECT id, tension_type, status, intensity, maturity_score,
                   evidence_count, revisit_count, first_detected_at,
                   last_revisited_at, last_evidence_at
            FROM rumination_tensions
            WHERE user_id = ?
            ORDER BY maturity_score DESC
        """, (ADMIN_USER_ID,))

        tensions = cursor.fetchall()

        if not tensions:
            result["problem_identified"] = "N√£o h√° tens√µes detectadas"
            result["solution"] = "Sistema precisa detectar tens√µes primeiro. Continue usando o bot normalmente."
            return JSONResponse(result)

        for t_row in tensions:
            t = dict(t_row)
            days_old = (datetime.now() - datetime.fromisoformat(t['first_detected_at'])).days

            # Calcular maturidade manualmente
            time_factor = min(1.0, days_old / 7.0)
            evidence_factor = min(1.0, t['evidence_count'] / 5.0)
            revisit_factor = min(1.0, t['revisit_count'] / 4.0)
            connection_factor = 0.0
            intensity_factor = t['intensity']

            calculated_maturity = (
                time_factor * MATURITY_WEIGHTS['time'] +
                evidence_factor * MATURITY_WEIGHTS['evidence'] +
                revisit_factor * MATURITY_WEIGHTS['revisit'] +
                connection_factor * MATURITY_WEIGHTS['connection'] +
                intensity_factor * MATURITY_WEIGHTS['intensity']
            )

            # Checklist
            checks = {
                "maturity_ok": t['maturity_score'] >= MIN_MATURITY_FOR_SYNTHESIS,
                "days_ok": days_old >= MIN_DAYS_FOR_SYNTHESIS,
                "evidence_ok": t['evidence_count'] >= MIN_EVIDENCE_FOR_SYNTHESIS
            }

            ready = all(checks.values())

            tension_info = {
                "id": t['id'],
                "type": t['tension_type'],
                "status": t['status'],
                "days_old": days_old,
                "intensity": round(t['intensity'], 2),
                "maturity": {
                    "score": round(t['maturity_score'], 3),
                    "calculated": round(calculated_maturity, 3),
                    "needed": MIN_MATURITY_FOR_SYNTHESIS,
                    "ok": checks["maturity_ok"]
                },
                "evidence": {
                    "count": t['evidence_count'],
                    "needed": MIN_EVIDENCE_FOR_SYNTHESIS,
                    "ok": checks["evidence_ok"]
                },
                "days": {
                    "count": days_old,
                    "needed": MIN_DAYS_FOR_SYNTHESIS,
                    "ok": checks["days_ok"]
                },
                "factors": {
                    "time": round(time_factor, 3),
                    "evidence": round(evidence_factor, 3),
                    "revisit": round(revisit_factor, 3),
                    "connection": round(connection_factor, 3),
                    "intensity": round(intensity_factor, 3)
                },
                "ready_for_synthesis": ready,
                "blocking_factors": []
            }

            # Identificar bloqueios
            if not checks["maturity_ok"]:
                tension_info["blocking_factors"].append(
                    f"Maturidade insuficiente: {t['maturity_score']:.2f} < {MIN_MATURITY_FOR_SYNTHESIS}"
                )
            if not checks["days_ok"]:
                tension_info["blocking_factors"].append(
                    f"Tempo insuficiente: {days_old} dias < {MIN_DAYS_FOR_SYNTHESIS} dias"
                )
            if not checks["evidence_ok"]:
                tension_info["blocking_factors"].append(
                    f"Evid√™ncias insuficientes: {t['evidence_count']} < {MIN_EVIDENCE_FOR_SYNTHESIS}"
                )

            result["tensions"].append(tension_info)

        # An√°lise geral
        if not result["tensions"]:
            result["problem_identified"] = "N√£o h√° tens√µes no sistema"
            result["solution"] = "Continue conversando para que o sistema detecte contradi√ß√µes"
        else:
            ready_count = sum(1 for t in result["tensions"] if t["ready_for_synthesis"])

            if ready_count > 0:
                result["problem_identified"] = None
                result["solution"] = f"{ready_count} tens√£o(√µes) pronta(s) para s√≠ntese! O sistema deve gerar insights em breve."
            else:
                # Identificar bloqueio mais comum
                all_blocks = []
                for t in result["tensions"]:
                    all_blocks.extend(t["blocking_factors"])

                if "Evid√™ncias insuficientes" in str(all_blocks):
                    result["problem_identified"] = "üêõ BUG CR√çTICO: Evid√™ncias n√£o est√£o sendo contadas"
                    result["solution"] = {
                        "bug": "A fun√ß√£o _count_related_fragments() em jung_rumination.py sempre retorna 0",
                        "impact": "Novas evid√™ncias NUNCA s√£o adicionadas √†s tens√µes",
                        "why": "evidence_count permanece em 1 (apenas a evid√™ncia inicial)",
                        "consequence": "evidence_factor fica em 0.2 (1/5), impedindo maturidade de atingir 0.75",
                        "fix_needed": "Implementar busca sem√¢ntica de fragmentos relacionados usando ChromaDB",
                        "temporary_workaround": "Ajustar MIN_EVIDENCE_FOR_SYNTHESIS para 1 temporariamente"
                    }
                elif "Tempo insuficiente" in str(all_blocks):
                    oldest = max(t["days_old"] for t in result["tensions"])
                    result["problem_identified"] = f"Tens√µes muito recentes (mais antiga: {oldest} dias)"
                    result["solution"] = f"Aguardar {MIN_DAYS_FOR_SYNTHESIS - oldest} dias ou continue conversando"
                elif "Maturidade insuficiente" in str(all_blocks):
                    highest_maturity = max(t["maturity"]["score"] for t in result["tensions"])
                    result["problem_identified"] = f"Maturidade m√°xima: {highest_maturity:.2f} < {MIN_MATURITY_FOR_SYNTHESIS}"
                    result["solution"] = "Continuar conversando para acumular mais evid√™ncias e revisitas"

        return JSONResponse(result)

    except Exception as e:
        logger.error(f"‚ùå Erro em why-no-insights: {e}", exc_info=True)
        return JSONResponse({
            "error": str(e),
            "problem_identified": "Erro ao executar diagn√≥stico"
        }, status_code=500)


@router.get("/api/jung-lab/export-fragments")
async def export_fragments(
    _admin: Dict = Depends(require_master)
):
    """
    Exporta todos os fragmentos de rumina√ß√£o para an√°lise
    """
    from rumination_config import ADMIN_USER_ID

    try:
        db = get_db()
        cursor = db.conn.cursor()

        cursor.execute("""
            SELECT id, user_id, content, emotional_weight,
                   context_type, detected_at, metadata
            FROM rumination_fragments
            WHERE user_id = ?
            ORDER BY detected_at DESC
        """, (ADMIN_USER_ID,))

        fragments = [dict(row) for row in cursor.fetchall()]

        return JSONResponse({
            "total": len(fragments),
            "fragments": fragments
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao exportar fragmentos: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/api/jung-lab/export-tensions")
async def export_tensions(
    _admin: Dict = Depends(require_master)
):
    """
    Exporta todas as tens√µes para an√°lise detalhada
    """
    from rumination_config import ADMIN_USER_ID

    try:
        db = get_db()
        cursor = db.conn.cursor()

        cursor.execute("""
            SELECT id, user_id, tension_type, pole_a, pole_b,
                   pole_a_fragment_ids, pole_b_fragment_ids,
                   status, intensity, maturity_score, evidence_count,
                   revisit_count, first_detected_at, last_revisited_at,
                   last_evidence_at, resolved_at, metadata
            FROM rumination_tensions
            WHERE user_id = ?
            ORDER BY first_detected_at DESC
        """, (ADMIN_USER_ID,))

        tensions = [dict(row) for row in cursor.fetchall()]

        return JSONResponse({
            "total": len(tensions),
            "tensions": tensions
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao exportar tens√µes: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@router.get("/api/jung-lab/export-insights")
async def export_insights(
    _admin: Dict = Depends(require_master)
):
    """
    Exporta todos os insights gerados
    """
    from rumination_config import ADMIN_USER_ID

    try:
        db = get_db()
        cursor = db.conn.cursor()

        cursor.execute("""
            SELECT id, user_id, tension_id, insight_type,
                   content, confidence_score, status,
                   generated_at, delivered_at, user_feedback,
                   metadata
            FROM rumination_insights
            WHERE user_id = ?
            ORDER BY generated_at DESC
        """, (ADMIN_USER_ID,))

        insights = [dict(row) for row in cursor.fetchall()]

        return JSONResponse({
            "total": len(insights),
            "insights": insights
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao exportar insights: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# JUNG MIND - MAPA MENTAL DO SISTEMA DE RUMINA√á√ÉO
# ============================================================================

@router.get("/jung-mind", response_class=HTMLResponse)
async def jung_mind_page(request: Request):
    """
    P√°gina do mapa mental Jung Mind
    Visualiza√ß√£o hier√°rquica: Jung (centro) ‚Üí Fragmentos ‚Üí Tens√µes ‚Üí Insights
    Com sinapses (conex√µes laterais) entre elementos com s√≠mbolos/temas comuns
    """
    return templates.TemplateResponse("jung_mind.html", {
        "request": request
    })

@router.get("/api/jung-mind-data")
async def jung_mind_data():
    """
    API que retorna dados do jung-lab formatados para Vis.js Network

    Retorna:
        {
            "nodes": [
                {"id": "jung", "label": "Jung", "type": "center", ...},
                {"id": "frag_1", "label": "...", "type": "fragment", "category": "valor", ...},
                {"id": "tension_1", "label": "...", "type": "tension", ...},
                {"id": "insight_1", "label": "...", "type": "insight", ...}
            ],
            "edges": [
                {"from": "jung", "to": "frag_1", "type": "hierarchy"},
                {"from": "frag_1", "to": "tension_1", "type": "hierarchy"},
                {"from": "tension_1", "to": "insight_1", "type": "hierarchy"},
                {"from": "frag_1", "to": "frag_5", "type": "synapse", "reason": "tema: trabalho"}
            ]
        }
    """
    try:
        # Importar ADMIN_USER_ID
        try:
            from rumination_config import ADMIN_USER_ID
        except ImportError:
            logger.error("‚ùå N√£o foi poss√≠vel importar ADMIN_USER_ID de rumination_config")
            ADMIN_USER_ID = "367f9e509e396d51"  # Fallback hardcoded
            logger.info(f"   Usando fallback ADMIN_USER_ID: {ADMIN_USER_ID}")

        db = get_db()
        conn = db.conn
        cursor = conn.cursor()

        # Verificar se tabelas existem
        cursor.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name IN ('rumination_fragments', 'rumination_tensions', 'rumination_insights')
        """)
        existing_tables = [row[0] for row in cursor.fetchall()]

        if not existing_tables:
            logger.warning("‚ö†Ô∏è Nenhuma tabela de rumina√ß√£o encontrada")
            return JSONResponse({
                "nodes": [{
                    "id": "jung",
                    "label": "JUNG",
                    "type": "center",
                    "title": "Sistema de Rumina√ß√£o ainda n√£o inicializado",
                    "level": 0,
                    "color": "#6b7280",
                    "shape": "star",
                    "size": 40
                }],
                "edges": [],
                "stats": {
                    "total_fragments": 0,
                    "total_tensions": 0,
                    "total_insights": 0,
                    "total_synapses": 0
                },
                "warning": "Tabelas de rumina√ß√£o n√£o encontradas. Sistema ainda n√£o foi inicializado."
            })

        logger.info(f"‚úÖ Tabelas encontradas: {existing_tables}")

        nodes = []
        edges = []

        # ===== N√ì CENTRAL: JUNG =====
        nodes.append({
            "id": "jung",
            "label": "JUNG",
            "type": "center",
            "title": "Sistema de Rumina√ß√£o Cognitiva<br>Identidade do Agente",
            "level": 0,
            "color": "#a78bfa",
            "shape": "star",
            "size": 40
        })

        # ===== FRAGMENTOS =====
        cursor.execute("""
            SELECT id, fragment_type, content, emotional_weight, created_at, context
            FROM rumination_fragments
            WHERE user_id = ?
            ORDER BY created_at DESC
            LIMIT 200
        """, (ADMIN_USER_ID,))

        fragments = cursor.fetchall()
        logger.info(f"üìä Fragmentos encontrados: {len(fragments)}")
        fragment_themes = {}  # Para detectar sinapses

        for frag in fragments:
            frag_id = f"frag_{frag[0]}"
            frag_type = frag[1]
            content = frag[2]
            weight = frag[3]
            created_at = frag[4]
            context = frag[5]

            # Extrair palavras-chave para sinapses (simplificado)
            keywords = set(word.lower() for word in content.split() if len(word) > 4)
            fragment_themes[frag_id] = keywords

            nodes.append({
                "id": frag_id,
                "label": content[:30] + "..." if len(content) > 30 else content,
                "type": "fragment",
                "category": frag_type,
                "title": f"<b>Fragmento ({frag_type})</b><br>" +
                         f"{content}<br><br>" +
                         f"Peso emocional: {weight:.2f}<br>" +
                         f"Criado: {created_at}",
                "level": 1,
                "color": {
                    "valor": "#10b981",
                    "cren√ßa": "#3b82f6",
                    "comportamento": "#f59e0b",
                    "desejo": "#ec4899",
                    "medo": "#ef4444"
                }.get(frag_type, "#6b7280"),
                "shape": "dot",
                "size": 10 + (weight * 15),
                "full_data": {
                    "type": frag_type,
                    "content": content,
                    "weight": weight,
                    "created_at": created_at,
                    "context": context
                }
            })

            # Conex√£o hier√°rquica: Jung ‚Üí Fragmento
            edges.append({
                "from": "jung",
                "to": frag_id,
                "type": "hierarchy",
                "color": {"color": "#4b5563", "opacity": 0.3},
                "width": 1,
                "dashes": False
            })

        # ===== TENS√ïES =====
        cursor.execute("""
            SELECT id, tension_type, pole_a_content, pole_b_content,
                   intensity, maturity_score, status, first_detected_at, last_evidence_at,
                   pole_a_fragment_ids, pole_b_fragment_ids
            FROM rumination_tensions
            WHERE user_id = ?
            ORDER BY maturity_score DESC, first_detected_at DESC
            LIMIT 100
        """, (ADMIN_USER_ID,))

        tensions = cursor.fetchall()
        logger.info(f"üìä Tens√µes encontradas: {len(tensions)}")
        tension_fragments = {}  # Mapear tens√£o ‚Üí fragmentos relacionados

        for tension in tensions:
            tension_id = f"tension_{tension[0]}"
            t_type = tension[1]
            pole_a = tension[2]
            pole_b = tension[3]
            intensity = tension[4]
            maturity = tension[5]
            t_status = tension[6]
            first_detected_at = tension[7]
            last_evidence = tension[8]
            pole_a_fragment_ids = tension[9]
            pole_b_fragment_ids = tension[10]

            # Parse fragment IDs from JSON columns
            pole_a_ids = json.loads(pole_a_fragment_ids) if pole_a_fragment_ids else []
            pole_b_ids = json.loads(pole_b_fragment_ids) if pole_b_fragment_ids else []
            related_fragments = [f"frag_{fid}" for fid in (pole_a_ids + pole_b_ids)]
            tension_fragments[tension_id] = related_fragments

            nodes.append({
                "id": tension_id,
                "label": f"{t_type}\\n{intensity:.0%}",
                "type": "tension",
                "title": f"<b>Tens√£o: {t_type}</b><br><br>" +
                         f"Polo A: {pole_a}<br>" +
                         f"Polo B: {pole_b}<br><br>" +
                         f"Intensidade: {intensity:.0%}<br>" +
                         f"Maturidade: {maturity:.0%}<br>" +
                         f"Status: {t_status}<br>" +
                         f"√öltima evid√™ncia: {last_evidence}",
                "level": 2,
                "color": "#f59e0b" if t_status == "active" else "#6b7280",
                "shape": "diamond",
                "size": 15 + (maturity * 15),
                "full_data": {
                    "type": t_type,
                    "pole_a": pole_a,
                    "pole_b": pole_b,
                    "intensity": intensity,
                    "maturity": maturity,
                    "status": t_status,
                    "first_detected_at": first_detected_at
                }
            })

            # Conex√µes hier√°rquicas: Fragmentos ‚Üí Tens√£o
            for frag_id in related_fragments:
                edges.append({
                    "from": frag_id,
                    "to": tension_id,
                    "type": "hierarchy",
                    "color": {"color": "#f59e0b", "opacity": 0.4},
                    "width": 2,
                    "dashes": False
                })

        # ===== INSIGHTS =====
        cursor.execute("""
            SELECT id, source_tension_id, symbol_content, question_content,
                   full_message, depth_score, status, crystallized_at
            FROM rumination_insights
            WHERE user_id = ?
            ORDER BY crystallized_at DESC
            LIMIT 50
        """, (ADMIN_USER_ID,))

        insights = cursor.fetchall()
        logger.info(f"üìä Insights encontrados: {len(insights)}")

        for insight in insights:
            insight_id = f"insight_{insight[0]}"
            source_tension_id = f"tension_{insight[1]}" if insight[1] else None
            symbol = insight[2] or ""
            question = insight[3] or ""
            thought = insight[4] or ""
            depth = insight[5] or 0.5
            i_status = insight[6]
            crystallized_at = insight[7]

            nodes.append({
                "id": insight_id,
                "label": symbol[:40] + "..." if len(symbol) > 40 else symbol,
                "type": "insight",
                "title": f"<b>Insight ({i_status})</b><br><br>" +
                         f"<i>{symbol}</i><br><br>" +
                         f"{thought[:200]}...<br><br>" +
                         f"Quest√£o: {question}<br>" +
                         f"Profundidade: {depth:.0%}<br>" +
                         f"Cristalizado: {crystallized_at}",
                "level": 3,
                "color": "#ec4899" if i_status == "ready" else "#8b5cf6",
                "shape": "box",
                "size": 12 + (depth * 18),
                "full_data": {
                    "symbol": symbol,
                    "question": question,
                    "thought": thought,
                    "depth": depth,
                    "status": i_status,
                    "crystallized_at": crystallized_at
                }
            })

            # Conex√£o hier√°rquica: Tens√£o ‚Üí Insight
            if source_tension_id:
                edges.append({
                    "from": source_tension_id,
                    "to": insight_id,
                    "type": "hierarchy",
                    "color": {"color": "#ec4899", "opacity": 0.5},
                    "width": 3,
                    "dashes": False
                })

        # ===== SINAPSES (CONEX√ïES LATERAIS) =====
        # Conectar fragmentos com temas/palavras comuns
        fragment_ids = list(fragment_themes.keys())
        for i, frag_id_1 in enumerate(fragment_ids):
            for frag_id_2 in fragment_ids[i+1:]:
                # Calcular intersec√ß√£o de keywords
                common = fragment_themes[frag_id_1] & fragment_themes[frag_id_2]
                if len(common) >= 2:  # M√≠nimo 2 palavras em comum
                    edges.append({
                        "from": frag_id_1,
                        "to": frag_id_2,
                        "type": "synapse",
                        "title": f"Temas comuns: {', '.join(list(common)[:3])}",
                        "color": {"color": "#8b5cf6", "opacity": 0.15},
                        "width": 1,
                        "dashes": True,
                        "smooth": {"type": "curvedCW", "roundness": 0.2}
                    })

        stats = {
            "total_fragments": len(fragments),
            "total_tensions": len(tensions),
            "total_insights": len(insights),
            "total_synapses": sum(1 for e in edges if e["type"] == "synapse")
        }

        logger.info(f"üìä TOTAIS: {len(nodes)} n√≥s, {len(edges)} edges")
        logger.info(f"üìä STATS: {stats}")

        return JSONResponse({
            "nodes": nodes,
            "edges": edges,
            "stats": stats
        })

    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar dados do jung-mind: {e}", exc_info=True)
        import traceback
        return JSONResponse({
            "error": str(e),
            "traceback": traceback.format_exc()
        }, status_code=500)
