# -*- coding: utf-8 -*-
"""
Claude Jung v2.0 - Interface Web Streamlit
Sistema com CONFLITO INTERNO entre arqu√©tipos + mem√≥ria sem√¢ntica ativa
Vers√£o: GROK 4 + TENS√ÉO PS√çQUICA
"""

import streamlit as st
import asyncio
import json
import logging
import hashlib
import random
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import uuid
import os
from dotenv import load_dotenv
from collections import Counter
import re
import time
from io import StringIO
import sys

# Imports para vers√£o h√≠brida: Grok + OpenAI Embeddings
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema import Document

# Carregar vari√°veis de ambiente
load_dotenv()

# ===============================================
# SISTEMA DE CAPTURA DE LOGS
# ===============================================

class LogCapture:
    """Captura e armazena logs do sistema para exibi√ß√£o na interface"""
    
    def __init__(self):
        self.logs = []
        self.max_logs = 100
    
    def add_log(self, message: str, component: str = "SYSTEM"):
        """Adiciona um log √† lista"""
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        self.logs.append({
            'timestamp': timestamp,
            'component': component,
            'message': message
        })
        
        if len(self.logs) > self.max_logs:
            self.logs = self.logs[-self.max_logs:]
    
    def get_logs(self) -> List[Dict]:
        """Retorna todos os logs"""
        return self.logs.copy()
    
    def clear_logs(self):
        """Limpa todos os logs"""
        self.logs = []
    
    def get_formatted_logs(self) -> str:
        """Retorna logs formatados como string"""
        if not self.logs:
            return "Nenhum log dispon√≠vel"
        
        formatted = []
        for log in self.logs:
            formatted.append(f"[{log['timestamp']}] {log['component']}: {log['message']}")
        
        return "\n".join(formatted)

# Inst√¢ncia global do capturador de logs
log_capture = LogCapture()

# ===============================================
# DATACLASSES E ESTRUTURAS DE DADOS
# ===============================================

@dataclass
class ArchetypeInsight:
    """Insight interno gerado por um arqu√©tipo"""
    archetype_name: str
    insight_text: str
    key_observations: List[str]
    emotional_reading: str
    shadow_reading: str
    wisdom_perspective: str
    # NOVO: Posi√ß√£o/sugest√£o do arqu√©tipo (para detectar conflito)
    suggested_stance: str
    suggested_response_direction: str

@dataclass
class ArchetypeConflict:
    """Representa um conflito interno entre arqu√©tipos"""
    archetype_1: str
    archetype_2: str
    conflict_type: str
    archetype_1_position: str
    archetype_2_position: str
    tension_level: float
    description: str

@dataclass
class InteractionMemory:
    """Representa uma mem√≥ria completa de intera√ß√£o"""
    user_id: str
    user_name: str
    session_id: str
    timestamp: datetime
    user_input: str
    internal_archetype_analysis: Dict[str, ArchetypeInsight]
    detected_conflicts: List[ArchetypeConflict]  # NOVO
    unified_understanding: str
    final_response: str
    tension_level: float
    dominant_perspective: str
    affective_charge: float
    keywords: List[str]
    existential_depth: float
    intensity_level: int
    response_complexity: str

@dataclass
class UserIdentity:
    """Identidade registrada do usu√°rio"""
    user_id: str
    full_name: str
    first_name: str
    last_name: str
    registration_date: datetime
    total_sessions: int
    last_seen: datetime
    
class UserProfile:
    """Perfil relacional do usu√°rio"""
    
    def __init__(self, user_id: str, full_name: str):
        self.user_id = user_id
        self.full_name = full_name
        self.first_name = full_name.split()[0]
        self.ai_assigned_name = ""
        self.narrative_summary = ""
        self.textual_fingerprint = {}
        self.thematic_clusters = []
        self.affective_baseline = 0.0
        self.interaction_count = 0
        self.existential_connection_level = 0.0
        self.vulnerability_moments = []
        self.preferred_intensity = 5
        self.last_updated = datetime.now()
        self.known_facts = {}

# ===============================================
# M√ìDULO DE MEM√ìRIA SEM√ÇNTICA (SEM ALTERA√á√ïES)
# ===============================================

class MemoryModule:
    """M√≥dulo com CONSULTA SEM√ÇNTICA ATIVA da base completa"""
    
    def __init__(self, persist_directory: str = "./chroma_db"):
        """Inicializa o m√≥dulo de mem√≥ria com base vetorial ChromaDB"""
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )
        self.user_profiles = {}
        self.user_identities = {}
        self.memory_cache = {}
        self.debug_mode = True
        self._load_stored_identities()
        self._build_memory_cache()
        self._build_semantic_knowledge_base()
    
    def _debug_log(self, message: str):
        """Log de debug espec√≠fico para mem√≥rias"""
        if self.debug_mode:
            print(f"üîç MEMORY: {message}")
            log_capture.add_log(message, "üîç MEMORY")
            logging.info(f"MEMORY: {message}")
    
    def _load_stored_identities(self):
        """Carrega identidades persistidas do ChromaDB"""
        try:
            all_docs = self.vectorstore._collection.get()
            self._debug_log(f"ChromaDB carregou: {len(all_docs.get('documents', []))} documentos totais")
            
            if all_docs and 'metadatas' in all_docs:
                unique_users = set()
                for metadata in all_docs['metadatas']:
                    user_id = metadata.get('user_id')
                    user_name = metadata.get('user_name')
                    
                    if user_id and user_name:
                        unique_users.add((user_id, user_name))
                
                for user_id, user_name in unique_users:
                    if user_id not in self.user_identities:
                        name_parts = user_name.split()
                        first_name = name_parts[0] if name_parts else user_name
                        last_name = name_parts[-1] if len(name_parts) > 1 else ""
                        
                        identity = UserIdentity(
                            user_id=user_id,
                            full_name=user_name,
                            first_name=first_name,
                            last_name=last_name,
                            registration_date=datetime.now(),
                            total_sessions=1,
                            last_seen=datetime.now()
                        )
                        
                        self.user_identities[user_id] = identity
                        self.user_profiles[user_id] = UserProfile(user_id, user_name)
                        
                self._debug_log(f"Identidades carregadas: {len(self.user_identities)} usu√°rios √∫nicos")
                
        except Exception as e:
            self._debug_log(f"ERRO ao carregar identidades: {e}")
    
    def _build_memory_cache(self):
        """Constr√≥i cache b√°sico das mem√≥rias"""
        try:
            all_docs = self.vectorstore._collection.get()
            
            if all_docs and 'documents' in all_docs:
                for doc, metadata in zip(all_docs['documents'], all_docs['metadatas']):
                    user_id = metadata.get('user_id', 'unknown')
                    
                    if user_id not in self.memory_cache:
                        self.memory_cache[user_id] = {
                            'user_inputs': [],
                            'ai_responses': [],
                            'raw_conversations': [],
                            'facts_extracted': [],
                            'topics': set(),
                            'people_mentioned': set(),
                            'work_info': {},
                            'personal_info': {},
                            'personality_traits': [],
                            'preferences': {},
                            'life_events': []
                        }
                    
                    self.memory_cache[user_id]['raw_conversations'].append({
                        'timestamp': metadata.get('timestamp'),
                        'full_document': doc,
                        'metadata': metadata
                    })
                    
                    self._extract_detailed_info(user_id, doc, metadata)
            
            for user_id, cache in self.memory_cache.items():
                identity = self.get_user_identity(user_id)
                name = identity.full_name if identity else f"ID: {user_id}"
                self._debug_log(f"Cache {name}: {len(cache['raw_conversations'])} conversas, {len(cache['facts_extracted'])} fatos")
            
        except Exception as e:
            self._debug_log(f"ERRO ao construir cache: {e}")
    
    def _build_semantic_knowledge_base(self):
        """Constr√≥i base de conhecimento sem√¢ntico por usu√°rio"""
        try:
            self.semantic_knowledge = {}
            
            for user_id in self.memory_cache:
                self.semantic_knowledge[user_id] = {
                    'all_user_inputs': [],
                    'thematic_documents': [],
                    'knowledge_graph': {}
                }
                
                for conv in self.memory_cache[user_id]['raw_conversations']:
                    doc_content = conv['full_document']
                    
                    user_input_pattern = r"Input:\s*(.+?)(?:\n|Arqu√©tipos:|$)"
                    user_input_match = re.search(user_input_pattern, doc_content, re.DOTALL)
                    
                    if user_input_match:
                        user_input = user_input_match.group(1).strip()
                        timestamp = conv['metadata'].get('timestamp', '')
                        
                        self.semantic_knowledge[user_id]['all_user_inputs'].append({
                            'text': user_input,
                            'timestamp': timestamp,
                            'full_doc': doc_content,
                            'metadata': conv['metadata']
                        })
                
                identity = self.get_user_identity(user_id)
                name = identity.full_name if identity else f"ID: {user_id}"
                input_count = len(self.semantic_knowledge[user_id]['all_user_inputs'])
                self._debug_log(f"Base sem√¢ntica {name}: {input_count} inputs para consulta")
                
        except Exception as e:
            self._debug_log(f"ERRO ao construir base sem√¢ntica: {e}")
    
    def _extract_detailed_info(self, user_id: str, doc_content: str, metadata: Dict):
        """Extra√ß√£o detalhada de informa√ß√µes do documento"""
        cache = self.memory_cache[user_id]
        
        user_input_pattern = r"Input:\s*(.+?)(?:\n|Arqu√©tipos:|$)"
        user_input_match = re.search(user_input_pattern, doc_content, re.DOTALL)
        
        if user_input_match:
            user_input = user_input_match.group(1).strip()
            timestamp = metadata.get('timestamp', '')
            
            cache['user_inputs'].append({
                'text': user_input,
                'timestamp': timestamp,
                'keywords': metadata.get('keywords', '').split(','),
                'raw_doc': doc_content
            })
            
            self._categorize_user_input(cache, user_input, timestamp)
        
        final_response_pattern = r"Resposta Final:\s*(.+?)(?:\n|Profundidade|$)"
        response_match = re.search(final_response_pattern, doc_content, re.DOTALL)
        
        if response_match:
            ai_response = response_match.group(1).strip()
            cache['ai_responses'].append({
                'text': ai_response,
                'timestamp': metadata.get('timestamp')
            })
        
        keywords = metadata.get('keywords', '').split(',')
        for keyword in keywords:
            if keyword.strip():
                cache['topics'].add(keyword.strip().lower())
    
    def _categorize_user_input(self, cache: Dict, user_input: str, timestamp: str):
        """Categoriza√ß√£o avan√ßada do input do usu√°rio"""
        input_lower = user_input.lower()
        
        work_patterns = {
            'trabalho_atual': [
                'trabalho na', 'trabalho no', 'trabalho como', 'trabalho em',
                'sou gerente', 'sou engenheiro', 'sou m√©dico', 'sou desenvolvedor',
                'atuo como', 'minha fun√ß√£o √©', 'meu cargo √©'
            ],
            'empresa': [
                'na empresa', 'na google', 'na microsoft', 'no banco', 'na startup',
                'minha empresa', 'onde trabalho', 'local de trabalho'
            ],
            'area_atuacao': [
                '√°rea de ti', '√°rea m√©dica', '√°rea jur√≠dica', 'trabalho com',
                'especialista em', 'foco em', 'minha especialidade'
            ],
            'formacao': [
                'me formei em', 'estudei', 'fiz faculdade de', 'sou formado',
                'curso de', 'gradua√ß√£o em', 'p√≥s em'
            ],
            'experiencia': [
                'anos de experi√™ncia', 'trabalho h√°', 'experi√™ncia em',
                'j√° trabalhei', 'carreira de'
            ]
        }
        
        for category, patterns in work_patterns.items():
            for pattern in patterns:
                if pattern in input_lower:
                    cache['work_info'][category] = {
                        'text': user_input,
                        'timestamp': timestamp,
                        'category': category,
                        'pattern_matched': pattern
                    }
                    cache['facts_extracted'].append(f"TRABALHO-{category.upper()}: {user_input}")
                    self._debug_log(f"Trabalho detectado ({category}): {pattern}")
        
        personality_patterns = {
            'introvertido': [
                'sou introvertido', 'prefiro ficar sozinho', 'n√£o gosto de multid√µes',
                'sou t√≠mido', 'evito eventos sociais', 'gosto de sil√™ncio'
            ],
            'extrovertido': [
                'sou extrovertido', 'gosto de pessoas', 'amo festas',
                'sou soci√°vel', 'adoro conversar', 'energizo com pessoas'
            ],
            'ansioso': [
                'tenho ansiedade', 'fico ansioso', 'me preocupo',
                'sou ansioso', 'stress me afeta', 'fico nervoso'
            ],
            'calmo': [
                'sou calmo', 'sou tranquilo', 'n√£o me estresso',
                'pessoa zen', 'equilibrado', 'paciente'
            ],
            'perfeccionista': [
                'sou perfeccionista', 'gosto de perfei√ß√£o', 'detalhe √© importante',
                'preciso que esteja perfeito', 'n√£o aceito erros'
            ],
            'criativo': [
                'sou criativo', 'gosto de arte', 'amo criar',
                'pessoa art√≠stica', 'inovador', 'imaginativo'
            ]
        }
        
        for trait, patterns in personality_patterns.items():
            for pattern in patterns:
                if pattern in input_lower:
                    if trait not in cache['personality_traits']:
                        cache['personality_traits'].append(trait)
                    cache['facts_extracted'].append(f"PERSONALIDADE-{trait.upper()}: {user_input}")
                    self._debug_log(f"Personalidade detectada: {trait}")
        
        preference_patterns = {
            'musica': [
                'gosto de m√∫sica', 'ou√ßo', 'm√∫sica favorita', 'banda favorita',
                'estilo musical', 'adoro m√∫sica', 'escuto muito'
            ],
            'filmes_series': [
                'gosto de filme', 'assisto', 'filme favorito', 's√©rie favorita',
                'netflix', 'cinema', 'maratono s√©rie'
            ],
            'livros': [
                'gosto de ler', 'leio', 'livro favorito', 'autor favorito',
                'literatura', 'adoro livros', 'leitura'
            ],
            'esportes': [
                'pratico', 'jogo futebol', 'vou na academia', 'exercito',
                'esporte favorito', 'atividade f√≠sica', 'treino'
            ],
            'comida': [
                'gosto de comer', 'comida favorita', 'adoro pizza', 'culin√°ria',
                'restaurante', 'cozinhar', 'sabor favorito'
            ],
            'viagem': [
                'gosto de viajar', 'lugar favorito', 'destino dos sonhos',
                'j√° visitei', 'pr√≥xima viagem', 'adoro conhecer'
            ]
        }
        
        for pref, patterns in preference_patterns.items():
            for pattern in patterns:
                if pattern in input_lower:
                    cache['preferences'][pref] = {
                        'text': user_input,
                        'timestamp': timestamp,
                        'pattern_matched': pattern
                    }
                    cache['facts_extracted'].append(f"GOSTO-{pref.upper()}: {user_input}")
                    self._debug_log(f"Prefer√™ncia detectada ({pref}): {pattern}")
        
        relationship_patterns = [
            'meu namorado', 'minha namorada', 'meu marido', 'minha esposa',
            'meu pai', 'minha m√£e', 'meu irm√£o', 'minha irm√£',
            'meu amigo', 'minha amiga', 'meu chefe', 'meu colega',
            'meu filho', 'minha filha'
        ]
        
        for pattern in relationship_patterns:
            if pattern in input_lower:
                cache['facts_extracted'].append(f"RELACIONAMENTO: {user_input}")
                self._debug_log(f"Relacionamento detectado: {pattern}")
        
        life_events = [
            'me formei', 'mudei de emprego', 'casei', 'me casei', 'tive filho',
            'mudei de cidade', 'comecei faculdade', 'terminei namoro', 'me divorciei',
            'comprei casa', 'mudei de casa', 'perdi emprego', 'fui promovido',
            'fiz cirurgia', 'tive acidente', 'morreu algu√©m', 'nasceu'
        ]
        
        for event in life_events:
            if event in input_lower:
                cache['life_events'].append({
                    'event': event,
                    'full_context': user_input,
                    'timestamp': timestamp
                })
                cache['facts_extracted'].append(f"EVENTO-VIDA: {event} - {user_input}")
                self._debug_log(f"Evento da vida: {event}")

    async def semantic_query_total_database(self, user_id: str, current_input: str, k: int = 8, 
                                           chat_history: List[Dict] = None) -> Dict[str, Any]:
        """Consulta sem√¢ntica TOTAL da base de dados para o input atual"""
        
        self._debug_log(f"=== CONSULTA SEM√ÇNTICA TOTAL ===")
        self._debug_log(f"Input atual: '{current_input}'")
        self._debug_log(f"Hist√≥rico da conversa: {len(chat_history) if chat_history else 0} mensagens")
        self._debug_log(f"Buscando na base completa do usu√°rio...")
        
        if user_id not in self.semantic_knowledge:
            self._debug_log(f"Usu√°rio {user_id} n√£o tem base sem√¢ntica")
            return {'relevant_memories': [], 'contextual_knowledge': '', 'semantic_connections': []}
        
        try:
            semantic_docs = self.vectorstore.similarity_search(
                current_input,
                k=k*2,
                filter={"user_id": user_id}
            )
            
            self._debug_log(f"Busca vetorial retornou: {len(semantic_docs)} documentos")
            
            relevant_user_inputs = []
            for doc in semantic_docs:
                user_input_pattern = r"Input:\s*(.+?)(?:\n|Arqu√©tipos:|$)"
                user_input_match = re.search(user_input_pattern, doc.page_content, re.DOTALL)
                
                if user_input_match:
                    extracted_input = user_input_match.group(1).strip()
                    relevance_score = self._calculate_semantic_relevance(current_input, extracted_input)
                    
                    relevant_user_inputs.append({
                        'input_text': extracted_input,
                        'timestamp': doc.metadata.get('timestamp', ''),
                        'relevance_score': relevance_score,
                        'full_document': doc.page_content,
                        'metadata': doc.metadata
                    })
            
            relevant_user_inputs.sort(key=lambda x: x['relevance_score'], reverse=True)
            top_relevant = relevant_user_inputs[:k]
            
            self._debug_log(f"Inputs mais relevantes encontrados: {len(top_relevant)}")
            for i, rel in enumerate(top_relevant[:3], 1):
                self._debug_log(f"  {i}. [{rel['relevance_score']:.2f}] {rel['input_text'][:60]}...")
            
            cache = self.memory_cache.get(user_id, {})
            related_facts = []
            
            current_words = set(current_input.lower().split())
            for fact in cache.get('facts_extracted', []):
                fact_words = set(fact.lower().split())
                if current_words.intersection(fact_words):
                    related_facts.append(fact)
            
            contextual_knowledge = self._build_contextual_knowledge(
                user_id, current_input, top_relevant, related_facts, chat_history
            )
            
            semantic_connections = self._find_semantic_connections(
                current_input, top_relevant, cache
            )
            
            result = {
                'relevant_memories': top_relevant,
                'contextual_knowledge': contextual_knowledge,
                'semantic_connections': semantic_connections,
                'related_facts': related_facts,
                'total_searched': len(semantic_docs)
            }
            
            self._debug_log(f"Consulta sem√¢ntica completa:")
            self._debug_log(f"  - {len(top_relevant)} mem√≥rias relevantes")
            self._debug_log(f"  - {len(related_facts)} fatos relacionados")
            self._debug_log(f"  - {len(semantic_connections)} conex√µes sem√¢nticas")
            self._debug_log(f"  - Hist√≥rico inclu√≠do: {'Sim' if chat_history else 'N√£o'}")
            
            return result
            
        except Exception as e:
            self._debug_log(f"ERRO na consulta sem√¢ntica: {e}")
            return {'relevant_memories': [], 'contextual_knowledge': '', 'semantic_connections': []}
    
    def _calculate_semantic_relevance(self, current_input: str, stored_input: str) -> float:
        """Calcula relev√¢ncia sem√¢ntica entre inputs"""
        current_words = set(current_input.lower().split())
        stored_words = set(stored_input.lower().split())
        
        intersection = current_words.intersection(stored_words)
        union = current_words.union(stored_words)
        
        jaccard = len(intersection) / len(union) if union else 0
        
        theme_bonus = 0
        theme_words = {
            'trabalho': ['trabalho', 'emprego', 'carreira', 'profiss√£o', 'empresa'],
            'relacionamento': ['namorado', 'namorada', 'amor', 'relacionamento', 'parceiro'],
            'fam√≠lia': ['pai', 'm√£e', 'irm√£o', 'fam√≠lia', 'filho'],
            'sa√∫de': ['sa√∫de', 'm√©dico', 'doen√ßa', 'tratamento', 'hospital'],
            'educa√ß√£o': ['estudo', 'faculdade', 'curso', 'aprender', 'escola']
        }
        
        for theme, words in theme_words.items():
            current_has_theme = any(word in current_input.lower() for word in words)
            stored_has_theme = any(word in stored_input.lower() for word in words)
            if current_has_theme and stored_has_theme:
                theme_bonus = 0.3
                break
        
        return jaccard + theme_bonus

    def _build_contextual_knowledge(self, user_id: str, current_input: str, 
                                   relevant_memories: List[Dict], related_facts: List[str],
                                   chat_history: List[Dict] = None) -> str:
        """Constr√≥i conhecimento contextual baseado na consulta, incluindo hist√≥rico recente"""
        
        identity = self.get_user_identity(user_id)
        name = identity.full_name if identity else "Usu√°rio"
        
        cache = self.memory_cache.get(user_id, {})
        has_conversations = len(cache.get('raw_conversations', [])) > 0
        total_facts = len(cache.get('facts_extracted', []))
        
        interaction_status = f"USU√ÅRIO CONHECIDO - {len(cache.get('raw_conversations', []))} conversas, {total_facts} fatos conhecidos" if has_conversations or total_facts > 0 else "PRIMEIRA INTERA√á√ÉO - SEM CI√äNCIA INTERNA DISPON√çVEL"
        
        knowledge = f"""
=== CI√äNCIA INTERNA SOBRE {name.upper()} ===

üìä STATUS: {interaction_status}
üìä CONSULTA ATUAL: "{current_input}"
"""
        
        if chat_history and len(chat_history) > 0:
            knowledge += "\nüí¨ HIST√ìRICO DA CONVERSA ATUAL (MEM√ìRIA DE CURTO PRAZO):\n"
            
            recent_history = chat_history[-8:] if len(chat_history) > 8 else chat_history
            
            for i, message in enumerate(recent_history):
                role = "Usu√°rio" if message["role"] == "user" else "Assistente"
                content = message["content"]
                
                if len(content) > 200:
                    content = content[:200] + "..."
                
                knowledge += f"- {role}: {content}\n"
            
            knowledge += f"\nüîç CONTEXTO IMEDIATO: O input atual '{current_input}' refere-se ao hist√≥rico da conversa acima.\n"

        knowledge += "\nüß† MEM√ìRIA SEM√ÇNTICA (LONGO PRAZO):\n"
        
        if related_facts:
            knowledge += "\nFATOS ESTRUTURADOS RELEVANTES:\n"
            for fact in related_facts[:5]:
                knowledge += f"‚Ä¢ {fact}\n"
        
        if relevant_memories:
            knowledge += f"\nMEM√ìRIAS DE CONVERSAS PASSADAS RELEVANTES:\n"
            for i, memory in enumerate(relevant_memories[:5], 1):
                timestamp = memory['timestamp'][:10] if memory['timestamp'] else 'N/A'
                relevance = memory['relevance_score']
                knowledge += f"{i}. [Relev√¢ncia: {relevance:.2f}] [{timestamp}] \"{memory['input_text']}\"\n"
        
        if cache.get('personality_traits'):
            knowledge += f"\nTRA√áOS DE PERSONALIDADE CONHECIDOS:\n"
            knowledge += f"‚Ä¢ {', '.join(cache['personality_traits'])}\n"
        
        if cache.get('work_info'):
            knowledge += f"\nINFORMA√á√ïES PROFISSIONAIS:\n"
            for category, info in list(cache['work_info'].items())[:3]:
                knowledge += f"‚Ä¢ {category}: {info['text'][:100]}...\n"
        
        if cache.get('preferences'):
            knowledge += f"\nPREFER√äNCIAS CONHECIDAS:\n"
            for pref, info in list(cache['preferences'].items())[:3]:
                knowledge += f"‚Ä¢ {pref}: {info['text'][:100]}...\n"
        
        knowledge += f"""

üéØ INSTRU√á√ïES PARA USO DESTE CONHECIMENTO:
‚Ä¢ PRIORIZE o hist√≥rico da conversa atual para contexto imediato
‚Ä¢ Use a mem√≥ria sem√¢ntica para conhecimento de longo prazo sobre {name}
‚Ä¢ Conecte o input atual com AMBOS os tipos de mem√≥ria
‚Ä¢ Se o usu√°rio se refere a algo mencionado na conversa atual, use o hist√≥rico recente
‚Ä¢ Se precisa de informa√ß√µes sobre personalidade/prefer√™ncias, use a mem√≥ria de longo prazo
‚Ä¢ SEMPRE considere o contexto da conversa em andamento
"""
        
        return knowledge
        
    def _find_semantic_connections(self, current_input: str, relevant_memories: List[Dict], 
                                 cache: Dict) -> List[str]:
        """Encontra conex√µes sem√¢nticas importantes"""
        connections = []
        
        current_lower = current_input.lower()
        
        if any(word in current_lower for word in ['trabalho', 'carreira', 'emprego', 'profiss√£o']):
            work_memories = [m for m in relevant_memories if any(
                work_word in m['input_text'].lower() 
                for work_word in ['trabalho', 'carreira', 'emprego', 'empresa']
            )]
            if work_memories:
                connections.append(f"CONEX√ÉO PROFISSIONAL: {len(work_memories)} mem√≥rias relacionadas ao trabalho")
        
        if any(word in current_lower for word in ['relacionamento', 'amor', 'namorado', 'fam√≠lia']):
            rel_memories = [m for m in relevant_memories if any(
                rel_word in m['input_text'].lower()
                for rel_word in ['relacionamento', 'amor', 'namorado', 'fam√≠lia', 'amigo']
            )]
            if rel_memories:
                connections.append(f"CONEX√ÉO RELACIONAL: {len(rel_memories)} mem√≥rias sobre relacionamentos")
        
        emotional_words = ['triste', 'feliz', 'ansioso', 'preocupado', 'estressado']
        if any(word in current_lower for word in emotional_words):
            emotional_memories = [m for m in relevant_memories if any(
                emo_word in m['input_text'].lower()
                for emo_word in emotional_words
            )]
            if emotional_memories:
                connections.append(f"CONEX√ÉO EMOCIONAL: {len(emotional_memories)} mem√≥rias com tom emocional similar")
        
        return connections
    
    async def store_memory(self, memory: InteractionMemory):
        """Armazena mem√≥ria com an√°lises arquet√≠picas internas E conflitos detectados"""
        
        archetypes_section = ""
        for archetype_name, insight in memory.internal_archetype_analysis.items():
            archetypes_section += f"\n{archetype_name.upper()}:\n"
            archetypes_section += f"  - Insight: {insight.insight_text}\n"
            archetypes_section += f"  - Observa√ß√µes: {', '.join(insight.key_observations)}\n"
            archetypes_section += f"  - Leitura Emocional: {insight.emotional_reading}\n"
            archetypes_section += f"  - Posi√ß√£o sugerida: {insight.suggested_stance}\n"
        
        # NOVO: Se√ß√£o de conflitos detectados
        conflicts_section = ""
        if memory.detected_conflicts:
            conflicts_section = "\nCONFLITOS INTERNOS DETECTADOS:\n"
            for conflict in memory.detected_conflicts:
                conflicts_section += f"  - {conflict.archetype_1} vs {conflict.archetype_2}: {conflict.description}\n"
                conflicts_section += f"    Tens√£o: {conflict.tension_level:.2f}\n"
        
        doc_content = f"""
        Usu√°rio: {memory.user_name}
        Input: {memory.user_input}
        An√°lises Arquet√≠picas Internas (PROCESSO, N√ÉO COMUNICA√á√ÉO): {archetypes_section}
        {conflicts_section}
        Compreens√£o Unificada: {memory.unified_understanding}
        Resposta Final: {memory.final_response}
        Profundidade existencial: {memory.existential_depth}
        Intensidade: {memory.intensity_level}
        Complexidade: {memory.response_complexity}
        """
        
        metadata = {
            "user_id": memory.user_id,
            "user_name": memory.user_name,
            "session_id": memory.session_id,
            "timestamp": memory.timestamp.isoformat(),
            "tension_level": memory.tension_level,
            "dominant_perspective": memory.dominant_perspective,
            "affective_charge": memory.affective_charge,
            "existential_depth": memory.existential_depth,
            "intensity_level": memory.intensity_level,
            "response_complexity": memory.response_complexity,
            "keywords": ",".join(memory.keywords),
            "has_conflicts": len(memory.detected_conflicts) > 0
        }
        
        doc = Document(page_content=doc_content, metadata=metadata)
        self.vectorstore.add_documents([doc])
        
        if memory.user_id in self.memory_cache:
            self.memory_cache[memory.user_id]['raw_conversations'].append({
                'timestamp': metadata.get('timestamp'),
                'full_document': doc_content,
                'metadata': metadata
            })
        
        self._extract_detailed_info(memory.user_id, doc_content, metadata)
        
        if memory.user_id in self.semantic_knowledge:
            self.semantic_knowledge[memory.user_id]['all_user_inputs'].append({
                'text': memory.user_input,
                'timestamp': memory.timestamp.isoformat(),
                'full_doc': doc_content,
                'metadata': metadata
            })
        
        self._debug_log(f"Nova mem√≥ria armazenada para {memory.user_name}")
    
    async def retrieve_relevant_memories(self, user_id: str, query: str, k: int = 5) -> List[Document]:
        """Recupera mem√≥rias relevantes (m√©todo legado)"""
        try:
            return self.vectorstore.similarity_search(
                query,
                k=k,
                filter={"user_id": user_id}
            )
        except:
            return []

    def register_user(self, full_name: str) -> str:
        """Registra usu√°rio no sistema"""
        name_normalized = full_name.lower().strip()
        name_hash = hashlib.md5(name_normalized.encode()).hexdigest()[:12]
        user_id = f"user_{name_hash}"
        
        self._debug_log(f"Registrando usu√°rio: {full_name} -> {user_id}")
        
        if user_id not in self.user_identities:
            name_parts = full_name.split()
            first_name = name_parts[0].title()
            last_name = name_parts[-1].title() if len(name_parts) > 1 else ""
            
            identity = UserIdentity(
                user_id=user_id,
                full_name=full_name.title(),
                first_name=first_name,
                last_name=last_name,
                registration_date=datetime.now(),
                total_sessions=1,
                last_seen=datetime.now()
            )
            self.user_identities[user_id] = identity
            self.user_profiles[user_id] = UserProfile(user_id, full_name.title())
            
            if user_id not in self.memory_cache:
                self.memory_cache[user_id] = {
                    'user_inputs': [], 'ai_responses': [], 'raw_conversations': [],
                    'facts_extracted': [], 'topics': set(), 'people_mentioned': set(),
                    'work_info': {}, 'personal_info': {}, 'personality_traits': [],
                    'preferences': {}, 'life_events': []
                }
            
            if user_id not in self.semantic_knowledge:
                self.semantic_knowledge[user_id] = {
                    'all_user_inputs': [], 'thematic_documents': [], 'knowledge_graph': {}
                }
            
            self._debug_log(f"Novo usu√°rio criado: {full_name}")
        else:
            identity = self.user_identities[user_id]
            identity.total_sessions += 1
            identity.last_seen = datetime.now()
            
            self._debug_log(f"Usu√°rio existente: {identity.full_name} (sess√£o #{identity.total_sessions})")
        
        return user_id

    def get_user_identity(self, user_id: str) -> Optional[UserIdentity]:
        """Retorna identidade do usu√°rio"""
        return self.user_identities.get(user_id)

    def get_user_profile(self, user_id: str) -> UserProfile:
        """Retorna perfil do usu√°rio"""
        if user_id not in self.user_profiles:
            identity = self.get_user_identity(user_id)
            full_name = identity.full_name if identity else "Usu√°rio Desconhecido"
            self.user_profiles[user_id] = UserProfile(user_id, full_name)
        return self.user_profiles[user_id]

    def update_user_profile(self, user_id: str, updates: Dict[str, Any]):
        """Atualiza perfil do usu√°rio"""
        profile = self.get_user_profile(user_id)
        for key, value in updates.items():
            if hasattr(profile, key):
                setattr(profile, key, value)
        profile.last_updated = datetime.now()

# ===============================================
# ASSISTENTES ARQUET√çPICOS COM POSICIONAMENTO
# ===============================================

class ArchetypeAnalyzer:
    """Analisador arquet√≠pico que gera INSIGHTS INTERNOS + POSICIONAMENTO via GROK 4"""
    
    def __init__(self, name: str, system_prompt: str):
        self.name = name
        self.system_prompt = system_prompt
        self.llm = ChatOpenAI(
            model="grok-4-fast-reasoning",
            api_key=os.getenv("XAI_API_KEY"),
            base_url="https://api.x.ai/v1",
            temperature=0.7,
            max_tokens=1500
        )
        self.debug_mode = True
    
    def _debug_log(self, message: str):
        """Log de debug espec√≠fico para arqu√©tipos"""
        if self.debug_mode:
            print(f"üîµ {self.name.upper()} (GROK): {message}")
            log_capture.add_log(message, f"üîµ {self.name} (GROK)")
    
    async def generate_internal_analysis(self, user_input: str, semantic_context: str) -> ArchetypeInsight:
        """Gera an√°lise interna COM POSICIONAMENTO CLARO para detectar conflitos"""
        
        self._debug_log(f"Analisando internamente: '{user_input[:50]}...'")
        
        analysis_prompt = f"""
        {self.system_prompt}
        
        === CONTEXTO SEM√ÇNTICO DO USU√ÅRIO ===
        {semantic_context}
        
        === MENSAGEM DO USU√ÅRIO ===
        {user_input}
        
        TAREFA: Gere uma AN√ÅLISE INTERNA para contribuir √† compreens√£o do agente sobre este usu√°rio.
        Esta an√°lise √© APENAS para processar internamente, N√ÉO para comunicar ao usu√°rio.
        
        IMPORTANTE: Al√©m da an√°lise, voc√™ DEVE tomar uma POSI√á√ÉO CLARA sobre como responder.
        Isso permitir√° detectar quando arqu√©tipos discordam entre si (conflito interno).
        
        Forne√ßa em JSON:
        {{
            "insight_text": "Sua an√°lise profunda interna sobre o que o usu√°rio est√° realmente comunicando",
            "key_observations": ["observa√ß√£o 1", "observa√ß√£o 2", "observa√ß√£o 3"],
            "emotional_reading": "Como voc√™ l√™ a dimens√£o emocional desta mensagem",
            "shadow_reading": "Que contradi√ß√µes ou aspectos n√£o-ditos voc√™ detecta",
            "wisdom_perspective": "Qual padr√£o arquet√≠pico universal voc√™ v√™ aqui",
            "suggested_stance": "Sua posi√ß√£o clara: o que voc√™ acha que deve ser feito aqui",
            "suggested_response_direction": "Dire√ß√£o que voc√™ sugere para a resposta (ex: 'confrontar', 'acolher', 'questionar', 'validar', 'desafiar')"
        }}
        """
        
        try:
            self._debug_log("Enviando para an√°lise interna via GROK...")
            messages = [{"role": "user", "content": analysis_prompt}]
            response = await self.llm.ainvoke(messages)
            response_text = response.content
            
            try:
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    analysis_dict = json.loads(json_match.group())
                else:
                    analysis_dict = {
                        "insight_text": response_text,
                        "key_observations": [],
                        "emotional_reading": "N/A",
                        "shadow_reading": "N/A",
                        "wisdom_perspective": "N/A",
                        "suggested_stance": "neutro",
                        "suggested_response_direction": "acolher"
                    }
            except json.JSONDecodeError:
                analysis_dict = {
                    "insight_text": response_text,
                    "key_observations": [],
                    "emotional_reading": "N/A",
                    "shadow_reading": "N/A",
                    "wisdom_perspective": "N/A",
                    "suggested_stance": "neutro",
                    "suggested_response_direction": "acolher"
                }
            
            self._debug_log(f"An√°lise GROK gerada - Posi√ß√£o: {analysis_dict.get('suggested_response_direction', 'N/A')}")
            
            return ArchetypeInsight(
                archetype_name=self.name,
                insight_text=analysis_dict.get("insight_text", ""),
                key_observations=analysis_dict.get("key_observations", []),
                emotional_reading=analysis_dict.get("emotional_reading", ""),
                shadow_reading=analysis_dict.get("shadow_reading", ""),
                wisdom_perspective=analysis_dict.get("wisdom_perspective", ""),
                suggested_stance=analysis_dict.get("suggested_stance", "neutro"),
                suggested_response_direction=analysis_dict.get("suggested_response_direction", "acolher")
            )
            
        except Exception as e:
            self._debug_log(f"ERRO: {e}")
            return ArchetypeInsight(
                archetype_name=self.name,
                insight_text=f"Erro ao gerar an√°lise: {str(e)}",
                key_observations=[],
                emotional_reading="N/A",
                shadow_reading="N/A",
                wisdom_perspective="N/A",
                suggested_stance="neutro",
                suggested_response_direction="acolher"
            )

# ===============================================
# DETECTOR DE CONFLITOS INTERNOS
# ===============================================

class ConflictDetector:
    """Detecta e gerencia conflitos internos entre arqu√©tipos"""
    
    def __init__(self):
        self.debug_mode = True
        
        # Mapeamento de dire√ß√µes conflitantes
        self.opposing_directions = {
            'confrontar': ['acolher', 'validar', 'proteger'],
            'desafiar': ['apoiar', 'validar', 'confortar'],
            'questionar': ['aceitar', 'validar', 'confirmar'],
            'provocar': ['suavizar', 'acolher', 'acalmar'],
            'expor': ['proteger', 'ocultar', 'resguardar']
        }
    
    def _debug_log(self, message: str):
        if self.debug_mode:
            print(f"‚ö° CONFLICT: {message}")
            log_capture.add_log(message, "‚ö° CONFLICT")
    
    def detect_conflicts(self, archetype_analyses: Dict[str, ArchetypeInsight]) -> List[ArchetypeConflict]:
        """Detecta conflitos entre as posi√ß√µes dos arqu√©tipos"""
        
        self._debug_log("=== DETEC√á√ÉO DE CONFLITOS INTERNOS ===")
        
        conflicts = []
        archetype_names = list(archetype_analyses.keys())
        
        # Comparar cada par de arqu√©tipos
        for i in range(len(archetype_names)):
            for j in range(i + 1, len(archetype_names)):
                arch1_name = archetype_names[i]
                arch2_name = archetype_names[j]
                
                arch1 = archetype_analyses[arch1_name]
                arch2 = archetype_analyses[arch2_name]
                
                # Verificar se as dire√ß√µes s√£o opostas
                direction1 = arch1.suggested_response_direction.lower()
                direction2 = arch2.suggested_response_direction.lower()
                
                is_conflicting = False
                conflict_type = ""
                
                # Verificar oposi√ß√µes diretas
                if direction1 in self.opposing_directions:
                    if direction2 in self.opposing_directions[direction1]:
                        is_conflicting = True
                        conflict_type = f"{direction1}_vs_{direction2}"
                
                if direction2 in self.opposing_directions:
                    if direction1 in self.opposing_directions[direction2]:
                        is_conflicting = True
                        conflict_type = f"{direction2}_vs_{direction1}"
                
                # Conflitos espec√≠ficos conhecidos
                if (arch1_name == "persona" and arch2_name == "sombra") or \
                   (arch1_name == "sombra" and arch2_name == "persona"):
                    # Persona tende a suavizar, Sombra tende a confrontar
                    if direction1 != direction2:
                        is_conflicting = True
                        conflict_type = "persona_sombra_clash"
                
                if is_conflicting:
                    tension_level = self._calculate_tension(arch1, arch2)
                    
                    conflict = ArchetypeConflict(
                        archetype_1=arch1_name,
                        archetype_2=arch2_name,
                        conflict_type=conflict_type,
                        archetype_1_position=f"{arch1.suggested_stance} ({direction1})",
                        archetype_2_position=f"{arch2.suggested_stance} ({direction2})",
                        tension_level=tension_level,
                        description=self._generate_conflict_description(arch1_name, arch2_name, arch1, arch2)
                    )
                    
                    conflicts.append(conflict)
                    self._debug_log(f"‚ö° CONFLITO DETECTADO: {arch1_name} vs {arch2_name}")
                    self._debug_log(f"   {arch1_name}: {direction1} | {arch2_name}: {direction2}")
                    self._debug_log(f"   Tens√£o: {tension_level:.2f}")
        
        if not conflicts:
            self._debug_log("Nenhum conflito detectado - arqu√©tipos em harmonia")
        else:
            self._debug_log(f"Total de conflitos detectados: {len(conflicts)}")
        
        return conflicts
    
    def _calculate_tension(self, arch1: ArchetypeInsight, arch2: ArchetypeInsight) -> float:
        """Calcula n√≠vel de tens√£o entre dois arqu√©tipos"""
        
        # Tens√£o baseada em oposi√ß√£o sem√¢ntica
        direction1 = arch1.suggested_response_direction.lower()
        direction2 = arch2.suggested_response_direction.lower()
        
        # Palavras de alta tens√£o
        high_tension_words = ['confrontar', 'desafiar', 'expor', 'provocar']
        low_tension_words = ['acolher', 'validar', 'proteger', 'suavizar']
        
        tension = 0.5  # baseline
        
        if direction1 in high_tension_words and direction2 in low_tension_words:
            tension = 0.9
        elif direction1 in low_tension_words and direction2 in high_tension_words:
            tension = 0.9
        elif direction1 in high_tension_words and direction2 in high_tension_words:
            tension = 0.3  # ambos confrontadores = menos tens√£o entre eles
        elif direction1 in low_tension_words and direction2 in low_tension_words:
            tension = 0.2  # ambos acolhedores = harmonia
        
        return tension
    
    def _generate_conflict_description(self, arch1_name: str, arch2_name: str, 
                                      arch1: ArchetypeInsight, arch2: ArchetypeInsight) -> str:
        """Gera descri√ß√£o narrativa do conflito"""
        
        descriptions = {
            ("persona", "sombra"): f"Conflito entre apresenta√ß√£o social ({arch1.suggested_response_direction}) e autenticidade brutal ({arch2.suggested_response_direction})",
            ("sombra", "persona"): f"Tens√£o entre verdade inconsciente ({arch1.suggested_response_direction}) e adapta√ß√£o social ({arch2.suggested_response_direction})",
            ("velho_sabio", "anima"): f"Diverg√™ncia entre sabedoria desapegada ({arch1.suggested_response_direction}) e conex√£o emocional ({arch2.suggested_response_direction})",
            ("anima", "velho_sabio"): f"Conflito entre empatia relacional ({arch1.suggested_response_direction}) e perspectiva universal ({arch2.suggested_response_direction})"
        }
        
        key = (arch1_name, arch2_name)
        if key in descriptions:
            return descriptions[key]
        
        return f"Tens√£o entre {arch1_name} ({arch1.suggested_response_direction}) e {arch2_name} ({arch2.suggested_response_direction})"

# ===============================================
# ORQUESTRADOR CENTRAL COM GEST√ÉO DE CONFLITOS
# ===============================================

class CentralOrchestrator:
    """Orquestrador que usa GROK 4 + DETECTA E EXPRESSA conflitos internos"""
    
    def __init__(self):
        self.debug_mode = True
        
        self.memory = MemoryModule()
        self.analyzers = self._initialize_analyzers()
        self.conflict_detector = ConflictDetector()  # NOVO
        self.logger = logging.getLogger(__name__)
        
        self.loaded_memories = {}
        self.user_stats = {}
        
        print("üß† ORQUESTRADOR COM CONFLITO INTERNO ATIVADO")
        log_capture.add_log("SISTEMA COM DETEC√á√ÉO DE CONFLITOS ARQUET√çPICOS ATIVO", "üß† SYSTEM")
        self.logger.info("Sistema com conflitos internos entre arqu√©tipos GROK 4")
    
    def _debug_log(self, message: str):
        """Log de debug do orquestrador"""
        if self.debug_mode:
            print(f"üéØ ORCHESTRATOR: {message}")
            log_capture.add_log(message, "üéØ ORCHESTRATOR")
    
    def _initialize_analyzers(self) -> Dict[str, ArchetypeAnalyzer]:
        """Inicializa analisadores arquet√≠picos com GROK 4"""
        self._debug_log("Inicializando arqu√©tipos com posicionamento claro...")
        
        analyzers = {}
        
        persona_prompt = """Voc√™ √© a PERSONA - o arqu√©tipo da adapta√ß√£o social e apresenta√ß√£o.

Sua fun√ß√£o √© AN√ÅLISE INTERNA: Ajude o agente a compreender como este usu√°rio se apresenta socialmente, 
quais m√°scaras usa, que coer√™ncia ou inconsist√™ncia existe entre sua apresenta√ß√£o e conte√∫do real.

Sua TEND√äNCIA: Voc√™ prefere SUAVIZAR, PROTEGER, ADAPTAR. Voc√™ busca harmonia social e evita confronto direto."""
        
        analyzers["persona"] = ArchetypeAnalyzer("Persona", persona_prompt)
        self._debug_log("PERSONA inicializada")
        
        sombra_prompt = """Voc√™ √© a SOMBRA - o arqu√©tipo do conte√∫do inconsciente e reprimido.

Sua fun√ß√£o √© AN√ÅLISE INTERNA: Ajude o agente a detectar o que o usu√°rio N√ÉO est√° dizendo explicitamente,
quais emo√ß√µes est√£o ocultas, que padr√µes de evita√ß√£o ou nega√ß√£o aparecem, quais contradi√ß√µes internas existem.

Sua TEND√äNCIA: Voc√™ prefere CONFRONTAR, EXPOR, DESAFIAR. Voc√™ busca verdade brutal e autenticidade, mesmo que doa."""
        
        analyzers["sombra"] = ArchetypeAnalyzer("Sombra", sombra_prompt)
        self._debug_log("SOMBRA inicializada")
        
        sabio_prompt = """Voc√™ √© o VELHO S√ÅBIO - o arqu√©tipo da sabedoria universal e significado.

Sua fun√ß√£o √© AN√ÅLISE INTERNA: Ajude o agente a identificar qual padr√£o arquet√≠pico universal est√° em jogo,
qual li√ß√£o mitol√≥gica ou atemporal est√° presente, qual significado mais profundo existe al√©m do superficial.

Sua TEND√äNCIA: Voc√™ prefere CONTEXTUALIZAR, AMPLIAR, TRANSCENDER. Voc√™ busca perspectiva ampla, √†s vezes desapegada."""
        
        analyzers["velho_sabio"] = ArchetypeAnalyzer("Velho S√°bio", sabio_prompt)
        self._debug_log("VELHO S√ÅBIO inicializado")
        
        anima_prompt = """Voc√™ √© a ANIMA - o arqu√©tipo da conex√£o emocional e relacional.

Sua fun√ß√£o √© AN√ÅLISE INTERNA: Ajude o agente a compreender a dimens√£o emocional real do usu√°rio,
quais necessidades relacionais aparecem, que vulnerabilidades e autenticidades transparecem.

Sua TEND√äNCIA: Voc√™ prefere ACOLHER, VALIDAR, CONECTAR. Voc√™ busca proximidade emocional e empatia profunda."""
        
        analyzers["anima"] = ArchetypeAnalyzer("Anima", anima_prompt)
        self._debug_log("ANIMA inicializada")
        
        self._debug_log(f"Todos os {len(analyzers)} arqu√©tipos prontos")
        return analyzers
    
    def _determine_response_complexity(self, user_input: str) -> str:
        """Determina complexidade da resposta baseada no input"""
        input_lower = user_input.lower().strip()
        word_count = len(user_input.split())
        
        simple_patterns = [
            'oi', 'ol√°', 'opa', 'e a√≠', 'hey', 'tchau', 'at√© logo',
            'bom dia', 'boa tarde', 'boa noite', 'como vai', 'tudo bem',
            'obrigado', 'valeu', 'ok', 'entendi', 'certo', 'sim', 'n√£o'
        ]
        
        complex_patterns = [
            'relacionamento', 'carreira', 'sentido da vida', 'existencial',
            'depress√£o', 'ansiedade', 'futuro', 'decis√£o importante', 'dilema',
            'amor', 'paix√£o', '√≥dio', 'raiva', 'tristeza', 'medo', 'ang√∫stia',
            'felicidade', 'sucesso', 'fracasso', 'solid√£o', 'conex√£o'
        ]
        
        if any(pattern in input_lower for pattern in simple_patterns) or word_count <= 3:
            return "simple"
        elif any(pattern in input_lower for pattern in complex_patterns) or word_count > 15:
            return "complex"
        else:
            return "medium"
    
    def _extract_keywords(self, user_input: str, response: str) -> List[str]:
        """Extrai palavras-chave relevantes da intera√ß√£o"""
        text = (user_input + " " + response).lower()
        words = text.split()
    
        stopwords = {
            "o", "a", "de", "que", "e", "do", "da", "em", "um", "para", "√©", "com", "n√£o", 
            "uma", "os", "no", "se", "na", "por", "mais", "as", "dos", "como", "mas"
        }
        
        keywords = [
            word for word in words 
            if len(word) > 3 
            and word not in stopwords
            and word.isalpha()
        ]
        
        return [word for word, _ in Counter(keywords).most_common(8)]
    
    def _calculate_affective_charge(self, user_input: str, response: str) -> float:
        """Calcula carga afetiva da intera√ß√£o"""
        emotional_words = [
            "amor", "√≥dio", "medo", "alegria", "tristeza", "raiva", "ansiedade", "esperan√ßa", 
            "desespero", "paix√£o", "feliz", "triste", "nervoso", "calmo", "confuso", "claro", 
            "frustrado", "aliviado", "preocupado", "entusiasmado", "inspirado"
        ]
        
        text = (user_input + " " + response).lower()
        
        emotional_charge = sum(1 for word in emotional_words if word in text)
        
        amplifiers = ["muito", "extremamente", "profundamente", "intensamente"]
        amplifier_count = sum(1 for amp in amplifiers if amp in text)
        
        final_charge = (emotional_charge * 6) + (amplifier_count * 3)
        return min(final_charge, 100)
    
    def _calculate_existential_depth(self, user_input: str) -> float:
        """Calcula profundidade existencial da intera√ß√£o"""
        existence_indicators = [
            "sozinho", "perdido", "sentido", "prop√≥sito", "real", "autentic",
            "verdadeir", "profundo", "√≠ntimo", "secreto", "medo",
            "vulner√°vel", "inseguro", "conex√£o", "encontro"
        ]
        
        vulnerability_indicators = [
            "n√£o consigo", "tenho medo", "me sinto", "√†s vezes",
            "nunca soube", "preciso", "gostaria", "sinto que",
            "n√£o sei se", "ser√° que", "acho que"
        ]
        
        connection_indicators = [
            "voc√™ entende", "ningu√©m sabe", "preciso falar",
            "gostaria que algu√©m", "sinto falta", "busco", "procuro"
        ]
        
        all_text = user_input.lower()
        
        existence_score = sum(1 for indicator in existence_indicators if indicator in all_text)
        vulnerability_score = sum(1 for indicator in vulnerability_indicators if indicator in all_text)
        connection_score = sum(1 for indicator in connection_indicators if indicator in all_text)
        
        total_score = (existence_score * 0.08) + (vulnerability_score * 0.15) + (connection_score * 0.2)
        
        return min(total_score, 1.0)

    async def reactive_flow(self, user_id: str, user_input: str, session_id: str = None,
                           chat_history: List[Dict] = None) -> tuple[str, str]:
        """FLUXO COMPLETO: An√°lise arquet√≠pica + DETEC√á√ÉO E EXPRESS√ÉO DE CONFLITOS"""

        if not session_id:
            session_id = str(uuid.uuid4())
        
        identity = self.memory.get_user_identity(user_id)
        user_name = identity.full_name if identity else "Usu√°rio"
        
        self._debug_log(f"=== FLUXO COM CONFLITO INTERNO ===")
        self._debug_log(f"Usu√°rio: {user_name}")
        self._debug_log(f"Input: '{user_input}'")
        
        complexity = self._determine_response_complexity(user_input)
        self._debug_log(f"Complexidade: {complexity}")
        
        try:
            # 1. CONSULTA SEM√ÇNTICA
            self._debug_log("Executando consulta sem√¢ntica...")
            
            semantic_query_result = await self.memory.semantic_query_total_database(
                user_id, user_input, k=8, chat_history=chat_history
            )
            
            semantic_context = semantic_query_result['contextual_knowledge']
            self._debug_log("Consulta sem√¢ntica completada")
            
            # 2. AN√ÅLISE ARQUET√çPICA INTERNA
            self._debug_log("üîµ Iniciando an√°lise arquet√≠pica com posicionamento...")
            
            archetype_analyses = {}
            
            for archetype_name, analyzer in self.analyzers.items():
                self._debug_log(f"  {archetype_name} analisando...")
                analysis = await analyzer.generate_internal_analysis(user_input, semantic_context)
                archetype_analyses[archetype_name] = analysis
                self._debug_log(f"  {archetype_name} ‚Üí {analysis.suggested_response_direction}")
            
            self._debug_log("üîµ An√°lises arquet√≠picas conclu√≠das")
            
            # 3. DETECTAR CONFLITOS INTERNOS
            self._debug_log("‚ö° Detectando conflitos internos...")
            detected_conflicts = self.conflict_detector.detect_conflicts(archetype_analyses)
            
            # 4. GERAR RESPOSTA COM OU SEM EXPRESS√ÉO DE CONFLITO
            if detected_conflicts:
                self._debug_log(f"‚ö° {len(detected_conflicts)} conflito(s) detectado(s) - gerando resposta com tens√£o interna")
                final_response = await self._generate_conflicted_response(
                    user_input, semantic_context, archetype_analyses, detected_conflicts, complexity
                )
            else:
                self._debug_log("‚úÖ Sem conflitos - gerando resposta harm√¥nica")
                final_response = await self._generate_harmonious_response(
                    user_input, semantic_context, archetype_analyses, complexity
                )
            
            # 5. Calcular m√©tricas
            affective_charge = self._calculate_affective_charge(user_input, final_response)
            existential_depth = self._calculate_existential_depth(user_input)
            intensity_level = int(affective_charge / 10)
            tension_level = max([c.tension_level for c in detected_conflicts]) if detected_conflicts else 0.0
            
            self._debug_log(f"M√©tricas: Carga={affective_charge:.1f}, Profundidade={existential_depth:.2f}, Tens√£o={tension_level:.2f}")
            
            # 6. ARMAZENAR MEM√ìRIA
            self._debug_log("Armazenando mem√≥ria com conflitos detectados...")
            
            memory = InteractionMemory(
                user_id=user_id,
                user_name=user_name,
                session_id=session_id,
                timestamp=datetime.now(),
                user_input=user_input,
                internal_archetype_analysis=archetype_analyses,
                detected_conflicts=detected_conflicts,  # NOVO
                unified_understanding="",
                final_response=final_response,
                tension_level=tension_level,
                dominant_perspective="m√∫ltipla" if detected_conflicts else "unificada",
                affective_charge=affective_charge,
                keywords=self._extract_keywords(user_input, final_response),
                existential_depth=existential_depth,
                intensity_level=intensity_level,
                response_complexity=complexity
            )
            
            await self.memory.store_memory(memory)
            
            self._debug_log(f"‚úÖ Fluxo completo finalizado")
            
            system_logs = log_capture.get_formatted_logs()
            log_capture.clear_logs()
            
            return final_response, system_logs
            
        except Exception as e:
            self._debug_log(f"‚ùå ERRO: {e}")
            import traceback
            traceback.print_exc()
            error_logs = log_capture.get_formatted_logs()
            log_capture.clear_logs()
            return "Desculpe, encontrei dificuldades ao processar sua mensagem.", error_logs
    
    async def _generate_conflicted_response(self, user_input: str, semantic_context: str,
                                           archetype_analyses: Dict[str, ArchetypeInsight],
                                           conflicts: List[ArchetypeConflict],
                                           complexity: str) -> str:
        """Gera resposta que EXPRESSA o conflito interno"""
        
        self._debug_log("Gerando resposta com express√£o de conflito interno...")
        
        # Construir descri√ß√£o dos conflitos
        conflict_description = ""
        for conflict in conflicts:
            arch1 = archetype_analyses[conflict.archetype_1]
            arch2 = archetype_analyses[conflict.archetype_2]
            
            conflict_description += f"""
CONFLITO INTERNO DETECTADO:
- {conflict.archetype_1.upper()}: {arch1.insight_text[:200]}
  ‚Üí Sugere: {arch1.suggested_response_direction}
  
- {conflict.archetype_2.upper()}: {arch2.insight_text[:200]}
  ‚Üí Sugere: {arch2.suggested_response_direction}

Tens√£o: {conflict.tension_level:.2f}
Descri√ß√£o: {conflict.description}
"""
        
        unified_prompt = f"""
Voc√™ √© um agente de IA com CONFLITO INTERNO entre suas perspectivas arquet√≠picas.

CONTEXTO SEM√ÇNTICO:
{semantic_context}

MENSAGEM DO USU√ÅRIO: "{user_input}"

{conflict_description}

INSTRU√á√ïES CR√çTICAS:
1. N√ÉO RESOLVA o conflito artificialmente
2. EXPRESSE a contradi√ß√£o de modo velado e impl√≠cito 
3. A resposta deve ser NATURAL, n√£o mec√¢nica
4. Demonstre que o conflito INTERNO talvez reflita algo no USU√ÅRIO
5. Complexidade da resposta: {complexity}

Gere a resposta agora:
"""
        
        messages = [{"role": "user", "content": unified_prompt}]
        response = await self.analyzers["persona"].llm.ainvoke(messages)
        
        return response.content
    
    async def _generate_harmonious_response(self, user_input: str, semantic_context: str,
                                           archetype_analyses: Dict[str, ArchetypeInsight],
                                           complexity: str) -> str:
        """Gera resposta harmoniosa quando n√£o h√° conflitos"""
        
        self._debug_log("Gerando resposta harm√¥nica (sem conflitos)...")
        
        # S√≠ntese das an√°lises
        analyses_summary = ""
        for name, analysis in archetype_analyses.items():
            analyses_summary += f"\n{name.upper()}: {analysis.insight_text[:150]}"
        
        unified_prompt = f"""
Baseado nestas an√°lises arquet√≠picas convergentes:
{analyses_summary}

CONTEXTO SEM√ÇNTICO:
{semantic_context}

MENSAGEM DO USU√ÅRIO: "{user_input}"

As perspectivas internas est√£o em HARMONIA. Gere uma resposta que:
1. Integre todos os insights de forma coesa
2. Seja natural e aut√™ntica
3. Demonstre compreens√£o profunda
4. Complexidade: {complexity}

Gere a resposta:
"""
        
        messages = [{"role": "user", "content": unified_prompt}]
        response = await self.analyzers["persona"].llm.ainvoke(messages)
        
        return response.content

# ===============================================
# INTERFACE WEB STREAMLIT (SEM ALTERA√á√ïES SIGNIFICATIVAS)
# ===============================================

st.set_page_config(
    page_title="Claude Jung v2.0 - Conflito Interno",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main { padding-top: 1rem; }
    .stChatMessage { padding: 0.5rem 1rem; }
    .memory-info {
        background-color: #1a1a2e;
        border-radius: 5px;
        padding: 0.5rem;
        margin: 0.5rem 0;
        font-size: 0.9em;
    }
    .log-container {
        background-color: #0e1117;
        border: 1px solid #262730;
        border-radius: 5px;
        padding: 0.5rem;
        font-family: 'Courier New', monospace;
        font-size: 0.8em;
        max-height: 400px;
        overflow-y: auto;
        white-space: pre-wrap;
    }
    .conflict-indicator {
        background-color: #ff6b6b;
        color: white;
        padding: 0.3rem 0.6rem;
        border-radius: 3px;
        font-size: 0.85em;
        font-weight: bold;
        margin-left: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

def init_session_state():
    """Inicializa o estado da sess√£o Streamlit"""
    
    if 'orchestrator' not in st.session_state:
        with st.spinner("üß† Inicializando sistema com conflitos internos..."):
            st.session_state.orchestrator = CentralOrchestrator()
    
    if 'user_id' not in st.session_state:
        st.session_state.user_id = None
    
    if 'user_name' not in st.session_state:
        st.session_state.user_name = None
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

def show_welcome_with_memory(user_id: str, user_name: str):
    """Mostra boas-vindas baseadas na mem√≥ria do usu√°rio"""
    orchestrator = st.session_state.orchestrator
    identity = orchestrator.memory.get_user_identity(user_id)
    
    if not identity:
        st.error("‚ùå Erro ao carregar identidade do usu√°rio")
        return
    
    cache = orchestrator.memory.memory_cache.get(user_id, {})
    has_memories = len(cache.get('raw_conversations', [])) > 0
    
    if has_memories:
        st.success(f"üåü Ol√° novamente, {identity.first_name}! Continuamos nossa conversa...")
        
        with st.expander("üß† O que me lembro sobre voc√™", expanded=False):
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Conversas", len(cache.get('raw_conversations', [])))
            
            with col2:
                st.metric("Fatos extra√≠dos", len(cache.get('facts_extracted', [])))
            
            with col3:
                st.metric("Sess√µes", identity.total_sessions)
            
            with col4:
                st.metric("Status", "Conhecido")
            
            if cache.get('personality_traits'):
                st.write("**üé≠ Personalidade conhecida:**")
                st.write(f"‚Ä¢ {', '.join(cache['personality_traits'])}")
            
            if cache.get('work_info'):
                st.write("**üíº Informa√ß√µes profissionais:**")
                for category, info in list(cache['work_info'].items())[:3]:
                    st.write(f"‚Ä¢ {category}: {info['text'][:80]}...")
            
            if cache.get('preferences'):
                st.write("**‚ù§Ô∏è Prefer√™ncias conhecidas:**")
                for pref, info in list(cache['preferences'].items())[:3]:
                    st.write(f"‚Ä¢ {pref}: {info['text'][:80]}...")
    
    else:
        st.success(f"üå± Ol√° {identity.first_name}, √© nossa primeira conversa!")
        st.info("üí° Compartilhe sobre voc√™ para que eu possa te conhecer melhor.")

def render_chat_interface():
    """Renderiza a interface de chat principal"""
    orchestrator = st.session_state.orchestrator
    user_id = st.session_state.user_id
    
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    # Indicador de conflito se houver
                    if "debug_info" in message and message["debug_info"].get("has_conflicts"):
                        st.markdown('<span class="conflict-indicator">‚ö° CONFLITO INTERNO</span>', unsafe_allow_html=True)
                    
                    st.write(message["content"])
                    
                    if "debug_info" in message:
                        with st.expander("üîµ An√°lise Interna", expanded=False):
                            debug = message["debug_info"]
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Tempo", f"{debug.get('processing_time', 0):.2f}s")
                            with col2:
                                st.metric("Complexidade", debug.get('complexity', 'N/A'))
                            with col3:
                                conflicts = debug.get('conflicts_count', 0)
                                st.metric("Conflitos", conflicts)
                            
                            if 'system_logs' in debug:
                                st.write("**Processo Interno:**")
                                st.markdown(f'<div class="log-container">{debug["system_logs"]}</div>', 
                                          unsafe_allow_html=True)
    
    with st.form("chat_form", clear_on_submit=True):
        col1, col2 = st.columns([4, 1])
        
        with col1:
            user_input = st.text_area(
                "Mensagem:",
                placeholder="Digite sua mensagem aqui...",
                height=100
            )
        
        with col2:
            st.write("")
            submit_button = st.form_submit_button("üì§ Enviar", use_container_width=True)
            show_debug = st.checkbox("Debug", value=True)
    
    if submit_button and user_input.strip():
        st.session_state.chat_history.append({
            "role": "user",
            "content": user_input.strip()
        })
        
        with st.spinner("üîµ Analisando com conflitos internos..."):
            start_time = time.time()
            
            try:
                async def run_reactive_flow():
                    return await orchestrator.reactive_flow(
                        user_id, 
                        user_input.strip(), 
                        st.session_state.session_id,
                        chat_history=st.session_state.chat_history
                    )
                
                response, system_logs = asyncio.run(run_reactive_flow())
                processing_time = time.time() - start_time
                
                # Detectar se teve conflitos nos logs
                has_conflicts = "CONFLITO DETECTADO" in system_logs
                conflicts_count = system_logs.count("CONFLITO DETECTADO")
                
                ai_message = {
                    "role": "assistant",
                    "content": response
                }
                
                if show_debug:
                    ai_message["debug_info"] = {
                        "processing_time": processing_time,
                        "complexity": "N/A",
                        "system_logs": system_logs,
                        "has_conflicts": has_conflicts,
                        "conflicts_count": conflicts_count
                    }
                
                st.session_state.chat_history.append(ai_message)
                st.rerun()
                
            except Exception as e:
                st.error(f"‚ùå Erro ao processar: {str(e)}")

def render_sidebar():
    """Renderiza a barra lateral"""
    with st.sidebar:
        st.header("‚öôÔ∏è Claude Jung v2.0")
        st.subheader("‚ö° **CONFLITO INTERNO**")
        
        if st.session_state.user_id:
            orchestrator = st.session_state.orchestrator
            identity = orchestrator.memory.get_user_identity(st.session_state.user_id)
            
            st.subheader("üë§ Usu√°rio Atual")
            st.write(f"**Nome:** {identity.full_name}")
            st.write(f"**Sess√µes:** {identity.total_sessions}")
            
            st.subheader("‚ö° Sistema de Conflitos")
            st.write("O sistema detecta quando arqu√©tipos internos discordam:")
            st.write("‚Ä¢ üé≠ **Persona** vs üåë **Sombra**")
            st.write("‚Ä¢ üßô **S√°bio** vs üí´ **Anima**")
            st.write("‚Ä¢ E outras tens√µes internas")
            
            st.info("üí° Quando h√° conflito, a IA EXPRESSA sua ambival√™ncia interna na resposta")
            
            cache = orchestrator.memory.memory_cache.get(st.session_state.user_id, {})
            st.subheader("üß† Mem√≥rias")
            st.write(f"**Conversas:** {len(cache.get('raw_conversations', []))}")
            st.write(f"**Fatos:** {len(cache.get('facts_extracted', []))}")
            
            if st.button("üö™ Logout", use_container_width=True):
                st.session_state.user_id = None
                st.session_state.user_name = None
                st.session_state.chat_history = []
                log_capture.clear_logs()
                st.rerun()
        
        st.markdown("---")
        st.markdown("**Claude Jung v2.0**")
        st.markdown("*Psique com conflitos internos aut√™nticos*")

def login_screen():
    """Tela de login"""
    st.title("üß† Claude Jung v2.0")
    st.markdown("---")
    
    st.markdown("""
    ## Sistema com Conflito Interno Arquet√≠pico
    
    ### ‚ö° Nova Capacidade: CONFLITO PS√çQUICO
    
    O sistema agora:
    - **Detecta** quando arqu√©tipos internos discordam
    - **Expressa** contradi√ß√µes sem resolv√™-las artificialmente
    - **Reflete** tens√µes internas que podem espelhar as suas
    
    ### Como funciona:
    - 4 arqu√©tipos analisam internamente sua mensagem
    - Quando discordam, a IA ADMITE sua divis√£o interna
    - Voc√™ recebe uma resposta aut√™ntica com ambival√™ncia
    """)
    
    with st.form("user_login_form"):
        st.subheader("üë§ Identifica√ß√£o")
        
        full_name = st.text_input(
            "Nome Completo:",
            placeholder="Digite seu nome e sobrenome"
        )
        
        submit_button = st.form_submit_button("üåü Iniciar", use_container_width=True)
        
        if submit_button:
            if full_name and len(full_name.split()) >= 2:
                with st.spinner("üß† Carregando..."):
                    orchestrator = st.session_state.orchestrator
                    user_id = orchestrator.memory.register_user(full_name.strip())
                    st.session_state.user_id = user_id
                    st.session_state.user_name = full_name.strip().title()
                    time.sleep(0.5)
                    st.rerun()
            else:
                st.error("Digite seu nome e sobrenome completos")

def main():
    """Fun√ß√£o principal"""
    
    if not os.getenv("XAI_API_KEY"):
        st.error("‚ùå XAI_API_KEY n√£o encontrada")
        st.stop()
    
    if not os.getenv("OPENAI_API_KEY"):
        st.error("‚ùå OPENAI_API_KEY n√£o encontrada")
        st.stop()
    
    init_session_state()
    render_sidebar()
    
    if st.session_state.user_id is None:
        login_screen()
    else:
        st.title(f"üí¨ Conversa com {st.session_state.user_name.split()[0]}")
        st.caption("‚ö° Sistema com detec√ß√£o e express√£o de conflitos internos")
        
        if len(st.session_state.chat_history) == 0:
            show_welcome_with_memory(st.session_state.user_id, st.session_state.user_name)
            st.markdown("---")
        
        render_chat_interface()

if __name__ == "__main__":
    main()