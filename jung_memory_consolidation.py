"""
jung_memory_consolidation.py - Sistema de Consolida√ß√£o de Mem√≥rias

Respons√°vel por:
- Agrupar mem√≥rias similares por per√≠odo
- Gerar resumos tem√°ticos com LLM
- Criar documentos "consolidated" no ChromaDB
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict
import json

logger = logging.getLogger(__name__)


class MemoryConsolidator:
    """
    Consolida mem√≥rias similares em resumos tem√°ticos
    """

    def __init__(self, db_manager):
        """
        Args:
            db_manager: HybridDatabaseManager instance
        """
        self.db = db_manager

    def consolidate_user_memories(self, user_id: str, lookback_days: int = 90):
        """
        Consolida mem√≥rias de um usu√°rio nos √∫ltimos N dias

        Args:
            user_id: ID do usu√°rio
            lookback_days: Per√≠odo de lookback (default: 90 dias)
        """
        logger.info(f"üì¶ Iniciando consolida√ß√£o de mem√≥rias para user_id={user_id} (lookback={lookback_days} dias)")

        # 1. Buscar todas as mem√≥rias do per√≠odo
        start_date = datetime.now() - timedelta(days=lookback_days)

        cursor = self.db.conn.cursor()
        cursor.execute("""
            SELECT id, user_input, ai_response, timestamp, keywords,
                   tension_level, affective_charge, existential_depth
            FROM conversations
            WHERE user_id = ?
            AND timestamp >= ?
            ORDER BY timestamp ASC
        """, (user_id, start_date.isoformat()))

        memories = [dict(row) for row in cursor.fetchall()]

        if len(memories) < 5:
            logger.info(f"   Menos de 5 mem√≥rias encontradas ({len(memories)}), consolida√ß√£o n√£o necess√°ria")
            return

        logger.info(f"   Encontradas {len(memories)} mem√≥rias para consolidar")

        # 2. Agrupar por t√≥pico usando keywords
        clusters = self._cluster_by_topic(memories)

        logger.info(f"   Identificados {len(clusters)} clusters tem√°ticos")

        # 3. Para cada cluster grande (‚â•5 mem√≥rias), gerar resumo
        for topic, cluster_memories in clusters.items():
            if len(cluster_memories) >= 5:
                logger.info(f"   Consolidando cluster '{topic}' ({len(cluster_memories)} mem√≥rias)")
                self._create_consolidated_memory(
                    user_id=user_id,
                    topic=topic,
                    memories=cluster_memories,
                    lookback_days=lookback_days
                )

    def _cluster_by_topic(self, memories: List[Dict]) -> Dict[str, List[Dict]]:
        """
        Agrupa mem√≥rias por t√≥pico baseado em keywords

        Args:
            memories: Lista de mem√≥rias

        Returns:
            Dict {topic: [mem√≥rias]}
        """
        clusters = {}

        for memory in memories:
            keywords = memory.get('keywords', '').split(',')

            # Detectar t√≥pico principal
            topic = self._identify_main_topic(keywords)

            if topic not in clusters:
                clusters[topic] = []

            clusters[topic].append(memory)

        return clusters

    def _identify_main_topic(self, keywords: List[str]) -> str:
        """
        Identifica t√≥pico principal baseado em keywords

        Args:
            keywords: Lista de keywords

        Returns:
            Nome do t√≥pico
        """
        if not keywords or not keywords[0]:
            return "geral"

        keywords_lower = [k.lower().strip() for k in keywords if k]

        topic_mapping = {
            "trabalho": ["trabalho", "emprego", "empresa", "carreira", "chefe", "colega"],
            "fam√≠lia": ["esposa", "marido", "filho", "filha", "pai", "mae", "familia"],
            "sa√∫de": ["saude", "doen√ßa", "ansiedade", "depressao", "insonia", "terapia"],
            "relacionamento": ["amigo", "namoro", "amor", "relacionamento"],
            "lazer": ["viagem", "hobby", "leitura"],
            "dinheiro": ["dinheiro", "financeiro", "salario", "conta", "divida"],
        }

        for topic, topic_keywords in topic_mapping.items():
            if any(kw in " ".join(keywords_lower) for kw in topic_keywords):
                return topic

        return "geral"

    def _create_consolidated_memory(self, user_id: str, topic: str,
                                    memories: List[Dict], lookback_days: int):
        """
        Cria mem√≥ria consolidada e salva no ChromaDB

        Args:
            user_id: ID do usu√°rio
            topic: T√≥pico do cluster
            memories: Mem√≥rias do cluster
            lookback_days: Per√≠odo de lookback
        """
        # Gerar resumo com LLM
        summary = self._generate_summary_with_llm(topic, memories)

        # IDs das conversas originais
        source_ids = [mem['id'] for mem in memories]

        # Calcular m√©tricas agregadas
        avg_tension = sum(m.get('tension_level', 0) for m in memories) / len(memories)
        avg_affective = sum(m.get('affective_charge', 0) for m in memories) / len(memories)
        avg_depth = sum(m.get('existential_depth', 0) for m in memories) / len(memories)

        # Per√≠odo da consolida√ß√£o
        timestamps = [datetime.fromisoformat(m['timestamp']) for m in memories]
        period_start = min(timestamps).strftime("%Y-%m-%d")
        period_end = max(timestamps).strftime("%Y-%m-%d")

        # Construir documento consolidado
        doc_content = f"""
=== MEM√ìRIA CONSOLIDADA ===
T√ìPICO: {topic.upper()}
PER√çODO: {period_start} a {period_end} ({len(memories)} conversas)

{summary}

M√âTRICAS DO PER√çODO:
- Tens√£o m√©dia: {avg_tension:.2f}
- Carga afetiva m√©dia: {avg_affective:.2f}
- Profundidade m√©dia: {avg_depth:.2f}
"""

        # Metadata
        metadata = {
            "user_id": user_id,
            "user_name": "",  # Will be populated from first memory
            "type": "consolidated",
            "topic": topic,
            "period_start": period_start,
            "period_end": period_end,
            "count": len(memories),
            "source_ids": json.dumps(source_ids),
            "avg_tension": round(avg_tension, 2),
            "avg_affective": round(avg_affective, 2),
            "avg_depth": round(avg_depth, 2),
            "timestamp": datetime.now().isoformat(),
            "recency_tier": "consolidated",  # Tier especial
            "emotional_intensity": round(avg_affective + avg_tension, 2),
            "has_conflicts": False,
            "keywords": topic,
            "topics": topic,
        }

        # Salvar no ChromaDB
        chroma_id = f"consolidated_{user_id}_{topic}_{period_end}"

        from langchain.schema import Document
        doc = Document(page_content=doc_content, metadata=metadata)

        try:
            # Tentar adicionar
            self.db.vectorstore.add_documents([doc], ids=[chroma_id])
            logger.info(f"‚úÖ Mem√≥ria consolidada criada: {chroma_id}")
        except Exception as e:
            # Se j√° existe, substituir
            if "already exists" in str(e).lower() or "duplicate" in str(e).lower():
                logger.info(f"   Substituindo mem√≥ria consolidada existente: {chroma_id}")
                try:
                    self.db.vectorstore.delete([chroma_id])
                    self.db.vectorstore.add_documents([doc], ids=[chroma_id])
                    logger.info(f"‚úÖ Mem√≥ria consolidada atualizada: {chroma_id}")
                except Exception as delete_error:
                    logger.error(f"‚ùå Erro ao substituir mem√≥ria consolidada: {delete_error}")
            else:
                logger.error(f"‚ùå Erro ao criar mem√≥ria consolidada: {e}")

    def _generate_summary_with_llm(self, topic: str, memories: List[Dict]) -> str:
        """
        Gera resumo tem√°tico das mem√≥rias usando LLM

        Args:
            topic: T√≥pico do cluster
            memories: Lista de mem√≥rias

        Returns:
            Resumo gerado
        """
        # Construir prompt com as mem√≥rias
        memories_text = "\n\n".join([
            f"[{mem['timestamp'][:10]}] Usu√°rio: {mem['user_input'][:200]}\nJung: {mem['ai_response'][:200]}"
            for mem in memories[:10]  # Limitar a 10 para n√£o estourar tokens
        ])

        prompt = f"""Voc√™ √© um sistema de consolida√ß√£o de mem√≥rias do Jung.

Analise as {len(memories)} conversas abaixo sobre o tema "{topic}" e gere um RESUMO CONSOLIDADO estruturado:

CONVERSAS:
{memories_text}

Gere um resumo seguindo este formato:

FATOS CONSOLIDADOS:
- [Liste 3-5 fatos principais mencionados repetidamente]

PADR√ïES EMOCIONAIS:
- [Descreva padr√µes emocionais recorrentes, gatilhos, sentimentos]

EVOLU√á√ÉO:
- [Descreva como o tema evoluiu ao longo do per√≠odo, se houve mudan√ßas]

Seja conciso mas informativo. M√°ximo 200 palavras."""

        try:
            if self.db.anthropic_client:
                response = self.db.anthropic_client.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=500,
                    messages=[{"role": "user", "content": prompt}]
                )
                summary = response.content[0].text.strip()
            elif self.db.xai_client:
                response = self.db.xai_client.chat.completions.create(
                    model="grok-beta",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=500
                )
                summary = response.choices[0].message.content.strip()
            else:
                # Fallback: resumo manual b√°sico
                summary = f"Consolida√ß√£o de {len(memories)} conversas sobre {topic}."

            return summary

        except Exception as e:
            logger.error(f"Erro ao gerar resumo com LLM: {e}")
            return f"Consolida√ß√£o de {len(memories)} conversas sobre {topic}."


def run_consolidation_job(db_manager):
    """
    Job para rodar consolida√ß√£o em todos os usu√°rios (s√≠ncrono)

    Args:
        db_manager: HybridDatabaseManager instance
    """
    logger.info("üîÑ Iniciando job de consolida√ß√£o de mem√≥rias")

    consolidator = MemoryConsolidator(db_manager)

    # Buscar todos os usu√°rios
    cursor = db_manager.conn.cursor()
    cursor.execute("SELECT DISTINCT user_id FROM conversations")
    user_ids = [row[0] for row in cursor.fetchall()]

    logger.info(f"   Consolidando mem√≥rias para {len(user_ids)} usu√°rios")

    for user_id in user_ids:
        try:
            consolidator.consolidate_user_memories(user_id, lookback_days=90)
        except Exception as e:
            logger.error(f"Erro ao consolidar mem√≥rias de {user_id}: {e}")

    logger.info("‚úÖ Job de consolida√ß√£o conclu√≠do")


async def run_consolidation_job_async(db_manager):
    """
    Vers√£o ass√≠ncrona do job de consolida√ß√£o (para APScheduler AsyncIO)

    Args:
        db_manager: HybridDatabaseManager instance
    """
    import asyncio
    # Executar a vers√£o s√≠ncrona em thread separada para n√£o bloquear event loop
    await asyncio.to_thread(run_consolidation_job, db_manager)
